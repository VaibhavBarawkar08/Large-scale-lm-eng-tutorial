{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a0bc167-6ec1-4f50-a150-b2e02d04a7db",
   "metadata": {},
   "source": [
    "## Additional Techniques\n",
    "\n",
    "So far, we have covered mostly topics related to **parallelism** and **memory**.  \n",
    "However, in addition to parallelization, there are many other techniques that are useful for **large-scale modeling**.\n",
    "\n",
    "In this session, we will briefly introduce some of those techniques.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40a33bd-6852-4430-81b8-9ab76cb72729",
   "metadata": {},
   "source": [
    "## 1. Kernel Fusion\n",
    "\n",
    "Kernel fusion is a technique used to improve computation speed by using custom CUDA-implemented code.\n",
    "\n",
    "### What is a Kernel?\n",
    "\n",
    "A kernel can be thought of as code that runs directly on a GPU device.\n",
    "CUDA provides syntax like the following:\n",
    "\n",
    "<br>\n",
    "\n",
    "```cpp\n",
    "__device__ void func(){...}\n",
    "\n",
    "__global__ void func(){...}\n",
    "\n",
    "__host__ void func(){...}\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "Functions with prefixes such as __global__ or __device__ run on the GPU,\n",
    "while functions with __host__ (or no prefix) run on the CPU.\n",
    "\n",
    "In general, code that includes prefixes like __global__ or __device__ and runs on the GPU is called a kernel.\n",
    "\n",
    "### What is Fusion??\n",
    "\n",
    "In practice, we build models by combining small operations provided by PyTorch.\n",
    "For example, we combine torch.matmul and torch.add to create an operation like: <br>\n",
    "#### y = torch.matmul(x, w) + b\n",
    "<br>\n",
    "Similarly, we combine operations such as torch.split and torch.permute to implement Transformer head split & merge.\n",
    "\n",
    "This approach is easy to implement and clean, but when analyzed internally, it is quite inefficient.\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "![](../images/kernel_fusion.png)\n",
    "\n",
    "<br>\n",
    "\n",
    "Let‚Äôs assume we execute an operation like:\n",
    "\n",
    "#### torch.matmul(x, w) * b\n",
    "\n",
    "\n",
    "What happens internally is as follows:\n",
    "\n",
    "1. First, PyTorch‚Äôs host code loads tensors `x` and `w` and launches the kernel (cuBLAS) corresponding to `torch.matmul`.\n",
    "2. The computed result is stored in memory.\n",
    "3. Next, to compute `* b`, the previously stored result of `matmul(x, w)` and tensor `b` are loaded again.\n",
    "4. The multiplication is performed, and the result is stored once more.\n",
    "\n",
    "This is clearly inefficient.\n",
    "\n",
    "Wouldn‚Äôt it be better if we could load `x`, `w`, and `b` **once**, perform all computations at once, and then store the result **only once**?\n",
    "\n",
    "Unfortunately, PyTorch needs to provide operations in small, composable units to maximize user flexibility. As a result, except for a few special cases, this kind of repeated load/store behavior is unavoidable.\n",
    "\n",
    "However, if we know how to write CUDA code, we don‚Äôt have to rely on PyTorch‚Äôs built-in kernels. We can directly implement a custom kernel that loads `x`, `w`, and `b` all at once, performs the computation, and stores the result in a single step.\n",
    "\n",
    "For this reason, many CUDA programmers implement kernels that load all necessary parameters, perform the computation, and store the result in one go‚Äîreducing unnecessary load/store operations and function-call overhead.\n",
    "\n",
    "\n",
    "\n",
    "### However‚Ä¶\n",
    "\n",
    "Including CUDA programming from scratch in this presentation would greatly exceed its scope, so CUDA programming details are omitted here.\n",
    "\n",
    "Instead, we recommend several **effective Transformer-related kernels** that have been released so far. Their documentation explains usage well, so you can refer to them to improve computational performance.  \n",
    "(If time allows in the future, it would be great to create CUDA-related notebooks as well üòä)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Training Kernel\n",
    "- [Apex Fused Kernel](https://github.com/NVIDIA/apex/tree/master/csrc): \n",
    "- [LightSeq Training Kernel](https://github.com/bytedance/lightseq/tree/master/lightseq/training)\n",
    "- [DeepSpeed Training Kernel](https://www.deepspeed.ai/tutorials/inference-tutorial/)\n",
    "\n",
    "#### Inference Kernel\n",
    "- [LightSeq Inference Kernel](https://github.com/bytedance/lightseq/tree/master/lightseq/inference)\n",
    "- [FastSeq NGram repeat blocking Kernel](https://github.com/microsoft/fastseq/tree/main/fastseq/clib/cuda)\n",
    "- [Faster Transformer Kernel](https://github.com/NVIDIA/FasterTransformer)\n",
    "- [DeepSpeed Inference Kernel](https://www.deepspeed.ai/tutorials/transformer_kernel/)\n",
    "- [Turbo Transformer Kernel](https://github.com/Tencent/TurboTransformers)\n",
    "- [Effective Transformer Kernel](https://github.com/bytedance/effective_transformer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a08c1b-2cf4-47e0-a5f9-0e6cac500850",
   "metadata": {},
   "source": [
    "## 2. Progressive Layer Dropping (PLD)\n",
    "\n",
    "Progressive Layer Dropping (PLD) is a technique that **randomly skips (drops) layers during training** to improve training speed and performance.  \n",
    "According to the authors of the paper, they observed the following two key insights from the BERT training process.\n",
    "\n",
    "<br>\n",
    "\n",
    "![](../images/pld_1.png)\n",
    "\n",
    "<br>\n",
    "\n",
    "- **In the early stage of training**, the differences in **L2-Norm** and **cosine similarity** between the input and output of layers are **large**, but **these differences become smaller in the later stages**.\n",
    "  - In other words, during early training, the model learns many new things and changes significantly, whereas in later training, it mainly performs fine-grained adjustments.\n",
    "\n",
    "<br>\n",
    "\n",
    "- In **Pre-LN models**, the **earlier layers** show relatively large differences between input and output, while the **later layers produce very similar outputs**.\n",
    "  - This suggests that as we go deeper into the network, later layers tend to make only very small corrections instead of significantly changing the representations.\n",
    "\n",
    "<br>\n",
    "\n",
    "Based on these observations, PLD improves training speed and performance by **dropping some layers during training**.  \n",
    "Using insights from experiments, layers that are considered more important‚Äî**early training stages and earlier layers**‚Äîare dropped less frequently, while layers considered less important‚Äî**later training stages and later layers**‚Äîare dropped more frequently. This strategy was shown to achieve good performance.\n",
    "\n",
    "<br>\n",
    "\n",
    "![](../images/pld_2.png)\n",
    "\n",
    "Dropping is controlled by a **gate function**:\n",
    "\n",
    "G ‚àà {0, 1}\n",
    "\n",
    "\n",
    "which determines whether a layer is dropped or not.\n",
    "\n",
    "If the gate function outputs `0`, the layer is skipped and only an **identity mapping** is passed to the next layer.\n",
    "\n",
    "<br>\n",
    "\n",
    "![](../images/pld_3.png)\n",
    "\n",
    "PLD is currently implemented in **DeepSpeed**, but it is **not yet compatible with ZeRO**, which makes it difficult to use in real large-scale model training at the moment.  \n",
    "Hopefully, PLD will be integrated with other DeepSpeed features in the near future üôÇ\n",
    "\n",
    "For more details, please refer to the paper:  \n",
    "https://arxiv.org/abs/2010.13369\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e31ab0-84ea-4974-a6f1-7d9f92954682",
   "metadata": {},
   "source": [
    "## 3. 1-Bit Compressive Optimizers\n",
    "\n",
    "- **1-bit Adam**\n",
    "- **1-bit LAMB (1-bit LAdam)**\n",
    "\n",
    "1-Bit Compressive Optimizers are techniques that **compress optimizer states such as momentum to 1-bit precision**.  \n",
    "Gradients and optimizer states are frequently exchanged between distributed devices and often become **communication bottlenecks** in large-scale training.\n",
    "\n",
    "For **SGD**, it has been common to compress the momentum term to 1-bit during communication and then apply **error compensation** afterward to improve communication efficiency.  \n",
    "However, for **Adam**, this approach is more difficult because the **variance term introduces non-linearity**, making naive compression ineffective.\n",
    "\n",
    "**1-bit Adam** solves this problem using a clever trick and achieves good performance even with heavy compression.\n",
    "\n",
    "<br>\n",
    "\n",
    "![](../images/one_bit_adam.png)\n",
    "\n",
    "<br>\n",
    "\n",
    "The key idea is as follows:\n",
    "\n",
    "- During the **initial training phase**, Adam operates **without compression**.\n",
    "- After reaching a certain point in training:\n",
    "  - The **variance term is fixed to a constant value** and communicated without precision requirements.\n",
    "  - Only the **momentum term is properly compressed to 1-bit** and communicated, similar to SGD.\n",
    "\n",
    "This approach achieves **the same convergence rate as standard (uncompressed) Adam**, while **reducing communication volume by up to 5√ó**.\n",
    "\n",
    "For more details, please refer to the paper:  \n",
    "https://arxiv.org/abs/2102.02888 üôÇ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa381ddb-6bbd-48a5-89c8-b2a24fcf8f2d",
   "metadata": {},
   "source": [
    "## 4. Curriculum Learning\n",
    "\n",
    "Curriculum Learning is inspired by **how humans learn**.  \n",
    "Humans typically start by learning **easy concepts** at a young age and gradually move on to **more difficult concepts** as they grow older.  \n",
    "However, modern neural network training usually does **not** follow this pattern.\n",
    "\n",
    "Curriculum Learning applies this human-like strategy to neural networks by:\n",
    "- Showing **easy samples early in training**\n",
    "- Gradually introducing **harder samples later in training**\n",
    "\n",
    "\n",
    "\n",
    "### How do we define ‚Äúeasy‚Äù and ‚Äúhard‚Äù samples?\n",
    "\n",
    "In the paper, the authors applied Curriculum Learning to **GPT pre-training** and used a very simple heuristic:\n",
    "\n",
    "- **Shorter samples ‚Üí easy samples**\n",
    "- **Longer samples ‚Üí hard samples**\n",
    "\n",
    "In **Megatron-LM**, all samples are concatenated together during pre-training.  \n",
    "To apply Curriculum Learning, the authors:\n",
    "- **Chunked the input into shorter sequences during early training**\n",
    "- Gradually **increased the input sequence length** as training progressed\n",
    "\n",
    "<br>\n",
    "\n",
    "![](../images/cl_1.png)\n",
    "\n",
    "<br>\n",
    "\n",
    "Despite its simplicity, this method showed **surprisingly strong performance**.  \n",
    "From the graph, we can see that models trained with Curriculum Learning achieve **lower loss** compared to those trained without it.\n",
    "\n",
    "Curriculum Learning is **already implemented in DeepSpeed**, so it is highly recommended to take advantage of this feature when training large-scale models.\n",
    "\n",
    "For more details, please refer to the paper:  \n",
    "https://arxiv.org/abs/2108.06084\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbefee6-4688-4dde-b9ef-29850b9e2fba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5eea640-14fe-4bbd-a457-15b41f57f49b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5ede92-df40-46df-8211-7c618fed6547",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
