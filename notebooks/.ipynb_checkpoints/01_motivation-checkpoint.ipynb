{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04554ccb-90d3-4b99-bdf7-135d852c4af3",
   "metadata": {},
   "source": [
    "# Motivation\n",
    "\n",
    "In this session, we explain the motivation behind preparing this material.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac2c9d5-b21b-43ed-a841-25ba89ea1993",
   "metadata": {},
   "source": [
    "## 1. Why Large Scale?\n",
    "\n",
    "Starting with GPT-3, which has been regarded as one of the greatest language models in human history, the size of deep learning language models has continued to grow rapidly. Models like BERT, which were considered large just a few years ago, now feel relatively small in comparison. This naturally raises an important question: **why have model sizes suddenly started increasing so dramatically in recent years?**\n",
    "\n",
    "![](../images/why_large_scale.png)\n",
    "\n",
    "<br>\n",
    "\n",
    "### 1) Is model architecture not that important?\n",
    "Until now, many researchers have paid significant attention to improving model architectures. However, recent research findings suggest that architectural changes alone have not led to performance improvements as dramatic as once expected. While architectural refinements have certainly improved language model performance to some extent, they have not resulted in game-changing breakthroughs.\n",
    "\n",
    "![](../images/arch_is_not_important.png)\n",
    "\n",
    "<br>\n",
    "\n",
    "### 2) In the end, data and model scale matter most ‚Äî and performance scales with them\n",
    "It has long been known that increasing the amount of data and the size of a model leads to better performance. However, when researchers pushed this idea to the extreme, models began to exhibit almost magical behavior. For example, language models were able to perform tasks such as translation, summarization, and classification **without fine-tuning**. Not only could they perform these tasks, but their performance was also comparable to that of fine-tuned models.\n",
    "\n",
    "Upon analyzing this phenomenon, researchers found that **model size had the greatest impact on performance**, followed by the size of the dataset. Considering that the Y-axis of the graph below is on a logarithmic scale, the effect of model size on performance is truly enormous.\n",
    "\n",
    "![](../images/scale_is_all_you_need.png)\n",
    "\n",
    "<br>\n",
    "\n",
    "### If this trend continues, then in a few years...?\n",
    "\n",
    "![](../images/GPT-X.png)\n",
    "\n",
    "<br><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca42433-17d6-49a0-b23c-f9d6b98ecb88",
   "metadata": {},
   "source": [
    "## 2. What Should We Prepare for the Era of Large-Scale Models?\n",
    "\n",
    "Large-scale Transformer models are **almost identical in architecture** to earlier models‚Äîthe main difference is that the **model size and data size have been massively increased**.  \n",
    "Because of this, some people think:\n",
    "\n",
    "> ‚ÄúIsn‚Äôt it basically the same as before, just bigger?‚Äù\n",
    "\n",
    "<br>\n",
    "\n",
    "![](../images/is_large_scale_easy.png)\n",
    "\n",
    "<br>\n",
    "\n",
    "### But the reality is‚Ä¶\n",
    "\n",
    "To **properly build and operate large-scale models**, a huge amount of **hardcore engineering** must be done in parallel, as shown below.\n",
    "\n",
    "<br>\n",
    "\n",
    "![](../images/hardcore.png)\n",
    "\n",
    "<br>\n",
    "\n",
    "In other words, **you must understand and be able to use these engineering techniques** to do modeling in the large-scale era.  \n",
    "The problem is that these techniques are **very difficult to use unless you study them professionally**.  \n",
    "\n",
    "For many general ML practitioners and data scientists:\n",
    "- The concepts can feel **very abstract and intimidating**\n",
    "- There is **very little material available in Korean**, making the barrier even higher\n",
    "\n",
    "\n",
    "### So, what‚Äôs the point?\n",
    "\n",
    "This is exactly **why I prepared this material and presentation**.  \n",
    "I hope this content can serve as a **practical guide** for those who are **learning large-scale engineering for the first time**. üôÇ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9208ede0-1273-4e36-8f77-818833edf22c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55dfeec-db9d-4d6c-b009-dd0c4c82b549",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
