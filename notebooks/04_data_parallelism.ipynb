{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49899d9c-f3b0-48ef-b54c-e0a4a22a1409",
   "metadata": {},
   "source": [
    "# Data Parallelism\n",
    "\n",
    "In this session, we will explore data parallelism techniques.\n",
    "\n",
    "## 1. `torch.nn.DataParallel`\n",
    "First, let‚Äôs take a look at how the familiar `torch.nn.DataParallel` works. `torch.nn.DataParallel` is a multi-threaded module that operates on a **single node with multiple GPUs**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be7f273-c370-4b0d-8c42-d0cc77ff0e34",
   "metadata": {},
   "source": [
    "### 1) Forward Pass\n",
    "\n",
    "1. **Scatter** the input mini-batch and send it to each device.\n",
    "2. **Broadcast** the model parameters from GPU-1 to GPUs 2, 3, and 4.\n",
    "3. Perform the **forward pass** on each device using the replicated model to compute the logits.\n",
    "4. **Gather** the computed logits and collect them on GPU-1.\n",
    "5. Compute the **loss** from the logits (with loss reduction).\n",
    "\n",
    "![](../images/dp_forward.png)\n",
    "\n",
    "<br>\n",
    "\n",
    "The corresponding code is shown below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853c0a42-d9d5-4283-92b6-9093dc51791f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "def data_parallel(module, inputs, labels, device_ids, output_device):\n",
    "    inputs = nn.parallel.scatter(inputs, device_ids)\n",
    "    # Scatter the input data to the devices specified by device_ids\n",
    "\n",
    "    replicas = nn.parallel.replicate(module, device_ids)\n",
    "    # Replicate the model across the devices in device_ids\n",
    "   \n",
    "    logit = nn.parallel.parallel_apply(replicas, inputs)\n",
    "    # Perform the forward pass on each device using the replicated models\n",
    "\n",
    "    logits = nn.parallel.gather(outputs, output_device)\n",
    "    # Gather the model logits to the output_device (a single device)\n",
    "    \n",
    "    return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a3f49c-a727-4d50-b0ec-75f85ea4cb5a",
   "metadata": {},
   "source": [
    "### 2) Backward Pass\n",
    "\n",
    "1. **Scatter** the computed loss to each device.\n",
    "2. Perform **backward propagation** on each device using the received loss to compute gradients.\n",
    "3. **Reduce** all computed gradients to GPU-1 by summing them.\n",
    "4. Update the model on GPU-1 using the aggregated gradients.\n",
    "\n",
    "![](../images/dp_backward.png)\n",
    "\n",
    "\n",
    "#### For those who may not be familiar...\n",
    "- `loss.backward()`: Computes gradients by differentiating the loss\n",
    "- `optimizer.step()`: Updates parameters using the computed gradients\n",
    "- The computation cost follows: `backward()` > `step()`\n",
    "\n",
    "![](../images/backward_step.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee180e72-9d39-4a95-9140-9362690769ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "src/ch4_data_parallelism/data_parallel.py\n",
    "\"\"\"\n",
    "\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "# 1. Create dataset\n",
    "datasets = load_dataset(\"multi_nli\").data[\"train\"]\n",
    "datasets = [\n",
    "    {\n",
    "        \"premise\": str(p),\n",
    "        \"hypothesis\": str(h),\n",
    "        \"labels\": l.as_py(),\n",
    "    }\n",
    "    for p, h, l in zip(datasets[2], datasets[5], datasets[9])\n",
    "]\n",
    "data_loader = DataLoader(datasets, batch_size=128, num_workers=4)\n",
    "\n",
    "# 2. Create model and tokenizer\n",
    "model_name = \"bert-base-cased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=3).cuda()\n",
    "\n",
    "# 3. Create the data parallel module\n",
    "# device_ids: list of devices to use / output_device: device to gather outputs\n",
    "model = nn.DataParallel(model, device_ids=[0, 1, 2, 3], output_device=0)\n",
    "\n",
    "# 4. Create optimizer and loss function\n",
    "optimizer = Adam(model.parameters(), lr=3e-5)\n",
    "loss_fn = nn.CrossEntropyLoss(reduction=\"mean\")\n",
    "\n",
    "# 5. Start training\n",
    "for i, data in enumerate(data_loader):\n",
    "    optimizer.zero_grad()\n",
    "    tokens = tokenizer(\n",
    "        data[\"premise\"],\n",
    "        data[\"hypothesis\"],\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "    logits = model(\n",
    "        input_ids=tokens.input_ids.cuda(),\n",
    "        attention_mask=tokens.attention_mask.cuda(),\n",
    "        return_dict=False,\n",
    "    )[0]\n",
    "\n",
    "    loss = loss_fn(logits, data[\"labels\"].cuda())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if i % 10 == 0:\n",
    "        print(f\"step:{i}, loss:{loss}\")\n",
    "\n",
    "    if i == 300:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24cd9469-8c67-4dba-b301-0bc953ea83d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ../src/ch4_data_parallelism/data_parallel.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66628752-a789-4d53-ad29-3e46605f9e3a",
   "metadata": {},
   "source": [
    "![](../images/dp_training.png)\n",
    "\n",
    "Training works well on multiple GPUs. However, there is a problem: since the logits are gathered on GPU 0, a **GPU memory imbalance** occurs. This issue can be alleviated to some extent by changing the approach to gather the **loss instead of the logits** on device 0. This is because the loss is a scalar and therefore much smaller in size compared to logits.\n",
    "\n",
    "This approach is equivalent to the `DataParallelCriterion` used in **PyTorch-Encoding**, which was introduced in a blog post by **Daangn Market**:\n",
    "- Blog: https://medium.com/daangn/pytorch-multi-gpu-%ED%95%99%EC%8A%B5-%EC%A0%9C%EB%8C%80%EB%A1%9C-%ED%95%98%EA%B8%B0-27270617936b  \n",
    "- Code: https://github.com/zhanghang1989/PyTorch-Encoding\n",
    "\n",
    "Although the blog explains it in a fairly complex way, the same functionality can be implemented much more easily by simply **overriding the `forward` function**.\n",
    "\n",
    "![](../images/dp_forward_2.png)\n",
    "\n",
    "<br>\n",
    "\n",
    "The key idea is to perform **loss computation and loss reduction inside the multi-threaded execution**. Since the model‚Äôs `forward` function already runs in multiple threads, placing the loss computation inside the `forward` function makes this very straightforward to implement.\n",
    "\n",
    "One interesting point is that, with this approach, **loss reduction happens twice**. First, within the multi-threaded execution, the loss is reduced from `batch_size // 4` to 4 (step 4 in the figure). Then, the four losses produced by each device are reduced again into a single loss (step 5 in the figure). Even so, this approach is much more efficient because the loss computation itself is parallelized and the memory burden on GPU 0 is significantly reduced.\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9860ea20-c7bf-45fb-a2e1-37b79c82a2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "src/ch4_data_parallelism/custom_data_parallel.py\n",
    "\"\"\"\n",
    "\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "# A standard model that outputs logits as the model output\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(768, 3)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        outputs = self.linear(inputs)\n",
    "        return outputs\n",
    "\n",
    "\n",
    "# A parallel model that outputs the loss during the forward pass\n",
    "class ParallelLossModel(Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, inputs, labels):\n",
    "        logits = super(ParallelLossModel, self).forward(inputs)\n",
    "        loss = nn.CrossEntropyLoss(reduction=\"mean\")(logits, labels)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42905029-f1f0-4d7a-8469-a9b32b9a385a",
   "metadata": {},
   "source": [
    "Fortunately, the Hugging Face Transformers models that we frequently use provide built-in support for computing the loss directly during the forward pass. Therefore, we can proceed using the Transformers functionality without going through the additional steps described above. The code below computes the loss directly by passing labels to the `labels` argument of a Transformers model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5079d0cf-5dc1-43da-957c-02f7fb68a9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "src/ch4_data_parallelism/efficient_data_parallel.py\n",
    "\"\"\"\n",
    "\n",
    "# 1 ~ 4ÍπåÏßÄ ÏÉùÎûµ...\n",
    "\n",
    "# 5. start training\n",
    "for i, data in enumerate(data_loader):\n",
    "    optimizer.zero_grad()\n",
    "    tokens = tokenizer(\n",
    "        data[\"premise\"],\n",
    "        data[\"hypothesis\"],\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "    loss = model(\n",
    "        input_ids=tokens.input_ids.cuda(),\n",
    "        attention_mask=tokens.attention_mask.cuda(),\n",
    "        labels=data[\"labels\"],\n",
    "    ).loss\n",
    "    \n",
    "    loss = loss.mean()\n",
    "    # (4,) -> (1,)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if i % 10 == 0:\n",
    "        print(f\"step:{i}, loss:{loss}\")\n",
    "\n",
    "    if i == 300:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1446f3d-ae2a-49e6-8b76-ec95e15a003d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ../src/ch4_data_parallelism/efficient_data_parallel.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38701d97-a4dc-4be4-aa2e-7dcb3cd2686e",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 2. Limitations of `torch.nn.DataParallel`\n",
    "\n",
    "### 1) Inefficient in Python because it is a multi-threaded module\n",
    "Due to the Global Interpreter Lock (GIL), Python does not allow multiple threads to run simultaneously within a single process. Therefore, to achieve true parallelism, the program must fundamentally be implemented as a **multi-process program**, allowing multiple processes to run concurrently.\n",
    "\n",
    "<br>\n",
    "\n",
    "### 2) The updated model must be replicated to other devices at every step\n",
    "In the current approach, gradients computed on each device are gathered onto a single device and used to update the model. As a result, the updated model must be broadcast and replicated to the other devices at every training step, which is quite expensive. However, if gradients were not gathered and each device instead performed its own `step()` locally, the model would not need to be replicated every time. So how can this be implemented?\n",
    "\n",
    "<br>\n",
    "\n",
    "### Solution? ‚ûù All-reduce!! üëç\n",
    "![](../images/allreduce.png)\n",
    "\n",
    "The answer is the **All-reduce** operation introduced earlier. If the gradients computed on each device are summed together and then evenly distributed to all devices, each device can perform its own `step()` independently. This eliminates the need to replicate the model from a single device at every step. Therefore, improving the existing approach requires leveraging All-reduce.\n",
    "\n",
    "<br>\n",
    "\n",
    "### However... ü§î\n",
    "All-reduce is considered a very expensive operation. Why is that? Let‚Äôs take a closer look at how All-reduce is implemented internally.\n",
    "\n",
    "<br>\n",
    "\n",
    "### Reduce + Broadcast implementation\n",
    "![](../images/allreduce_1.png)\n",
    "\n",
    "<br>\n",
    "\n",
    "### All-to-All implementation\n",
    "![](../images/allreduce_2.png)\n",
    "\n",
    "<br><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d8d982-c961-4e0a-9dd6-fc78650b95ff",
   "metadata": {},
   "source": [
    "## 3. `torch.nn.parallel.DistributedDataParallel` (DDP)\n",
    "\n",
    "### Ring All-reduce üíç\n",
    "Ring All-reduce is a new collective operation developed by Baidu researchers in 2017. Because it demonstrated significantly higher efficiency compared to previous approaches, it became a core component in the development of DDP.\n",
    "\n",
    "- https://github.com/baidu-research/baidu-allreduce\n",
    "\n",
    "![](../images/ring_allreduce.gif)\n",
    "\n",
    "<br>\n",
    "\n",
    "![](../images/ring_allreduce.png)\n",
    "\n",
    "<br>\n",
    "\n",
    "### What is DDP?\n",
    "DDP is a data parallelism module designed to address the limitations of the original DataParallel approach. It is a **multi-process module** that works in both **single-node and multi-node, multi-GPU** environments. By leveraging All-reduce, the concept of a master process is eliminated, which greatly simplifies the training workflow.\n",
    "\n",
    "![](../images/ddp.png)\n",
    "\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523247f9-ce0d-4bdc-8ef9-037f0b1681b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "src/ch4_data_parallelism/ddp.py\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "from torch.nn.parallel import DistributedDataParallel\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader, DistributedSampler\n",
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "# 1. Initialize the process group\n",
    "dist.init_process_group(\"nccl\")\n",
    "rank = dist.get_rank()\n",
    "world_size = dist.get_world_size()\n",
    "torch.cuda.set_device(rank)\n",
    "device = torch.cuda.current_device()\n",
    "\n",
    "# 2. Create dataset\n",
    "datasets = load_dataset(\"multi_nli\").data[\"train\"]\n",
    "datasets = [\n",
    "    {\n",
    "        \"premise\": str(p),\n",
    "        \"hypothesis\": str(h),\n",
    "        \"labels\": l.as_py(),\n",
    "    }\n",
    "    for p, h, l in zip(datasets[2], datasets[5], datasets[9])\n",
    "]\n",
    "\n",
    "# 3. Create DistributedSampler\n",
    "# DistributedSampler is a module used to split data and distribute it across different processes.\n",
    "sampler = DistributedSampler(\n",
    "    datasets,\n",
    "    num_replicas=world_size,\n",
    "    rank=rank,\n",
    "    shuffle=True,\n",
    ")\n",
    "data_loader = DataLoader(\n",
    "    datasets,\n",
    "    batch_size=32,\n",
    "    num_workers=4,\n",
    "    sampler=sampler,\n",
    "    shuffle=False,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "\n",
    "# 4. Create model and tokenizer\n",
    "model_name = \"bert-base-cased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=3).cuda()\n",
    "\n",
    "# 5. Create the Distributed Data Parallel module\n",
    "model = DistributedDataParallel(model, device_ids=[device], output_device=device)\n",
    "\n",
    "# 5. Create optimizer\n",
    "optimizer = Adam(model.parameters(), lr=3e-5)\n",
    "\n",
    "# 6. Start training\n",
    "for i, data in enumerate(data_loader):\n",
    "    optimizer.zero_grad()\n",
    "    tokens = tokenizer(\n",
    "        data[\"premise\"],\n",
    "        data[\"hypothesis\"],\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "    loss = model(\n",
    "        input_ids=tokens.input_ids.cuda(),\n",
    "        attention_mask=tokens.attention_mask.cuda(),\n",
    "        labels=data[\"labels\"],\n",
    "    ).loss\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if i % 10 == 0 and rank == 0:\n",
    "        print(f\"step:{i}, loss:{loss}\")\n",
    "\n",
    "    if i == 300:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06624fea-ab15-4657-8224-c588d530b815",
   "metadata": {},
   "source": [
    "Since this is a multi-process application, we launch it using `torch.distributed.launch`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf0ee28-104c-4b7c-8c84-05c892546f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m  torch.distributed.launch --nproc_per_node=4 ../src/ch4_data_parallelism/ddp.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d73502a-d546-422d-a678-fba237b7c912",
   "metadata": {},
   "source": [
    "### But wait‚Äîwhen is the best time to perform All-reduce?\n",
    "- Should All-reduce be performed together with the `backward()` operation?\n",
    "- Or should it be done after `backward()` finishes and before `step()` starts?\n",
    "\n",
    "![](../images/ddp_analysis_1.png)\n",
    "\n",
    "<br>\n",
    "\n",
    "### In conclusion, overlapping `backward()` and `all-reduce` is the most efficient approach.\n",
    "\n",
    "Ultimately, overlapping `backward()` and `all-reduce` is the most efficient strategy. Since `all-reduce` involves network communication, while `backward()` and `step()` are GPU computations, they can be executed simultaneously. By overlapping them, computation and communication are maximally overlapped, which significantly improves overall training efficiency.\n",
    "\n",
    "![](../images/ddp_analysis_2.png)\n",
    "\n",
    "<br>\n",
    "\n",
    "Analysis shows that when comparing `backward()` and `step()`, the `backward()` operation is much more computationally expensive.\n",
    "\n",
    "![](../images/ddp_analysis_3.png)\n",
    "\n",
    "<br>\n",
    "\n",
    "Naturally, the more expensive the operation you overlap, the shorter the total training time becomes. The analysis shows that performing `all-reduce` together with `backward()` is much faster than waiting until `backward()` finishes.\n",
    "\n",
    "![](../images/ddp_analysis_4.png)\n",
    "\n",
    "<br>\n",
    "\n",
    "### Common questions that may arise...\n",
    "- **Q1:** How can `all-reduce` be performed when not all gradients have been computed during `backward()`?\n",
    "  - **A1:** Since `backward()` proceeds sequentially from the later layers to the earlier ones, gradients can be communicated as soon as each layer‚Äôs gradients are computed.\n",
    "\n",
    "<br>\n",
    "\n",
    "- **Q2:** Then how often is `all-reduce` performed? Is it done for every layer?\n",
    "  - **A2:** No. **Gradient bucketing** is used. All-reduce is triggered when a bucket becomes full.\n",
    "\n",
    "<br>\n",
    "\n",
    "### Gradient Bucketing\n",
    "Gradient bucketing is a technique in which gradients are accumulated into buckets of a fixed size and sent to other processes once a bucket is full. During the `backward()` pass, gradients computed from the later layers are sequentially stored in a bucket. When the bucket reaches its capacity, an All-reduce operation is performed to distribute the summed gradients to each device.\n",
    "\n",
    "The diagram may be a bit confusing, but note that what is stored in the bucket is **not model parameters**, but the **gradients produced by each layer**. All buckets have a fixed size, which can be configured in megabytes using the `bucket_size_mb` argument.\n",
    "\n",
    "![](../images/ddp_analysis_5.png)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
