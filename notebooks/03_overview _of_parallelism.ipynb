{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25747507-0937-48b3-8c7f-1b37f18221f0",
   "metadata": {},
   "source": [
    "# Overview of Parallelism\n",
    "\n",
    "In this session, before diving into parallel processing, we will take an overall look at the various parallelism techniques.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008509cc-ac68-4b5d-be37-c5facfe1bbbf",
   "metadata": {},
   "source": [
    "## 1. Parallelism\n",
    "\n",
    "Parallelism refers to techniques that process multiple tasks simultaneously and is one of the most important concepts in large-scale modeling. In machine learning, it is mainly used to parallelize computation across multiple devices in order to improve speed and memory efficiency.\n",
    "\n",
    "<br><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e174a5a-86f4-46a2-9d90-c9c8f907bd05",
   "metadata": {},
   "source": [
    "## 2. Data Parallelism\n",
    "\n",
    "Data parallelism is a technique used to speed up training by processing data in parallel when the dataset is large. It works by replicating the model on every device and feeding different data to each device. As a result, the effective batch size can be increased proportionally to the number of devices. However, data parallelism is only possible when a single model can fully fit on one device.\n",
    "\n",
    "![](../images/data_parallelism.png)\n",
    "\n",
    "<br><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf24f92-5987-49f3-a5eb-3cfe8041d33d",
   "metadata": {},
   "source": [
    "## 3. Model Parallelism\n",
    "\n",
    "If a model is too large to fit entirely on a single device, its parameters must be split and placed across multiple devices. As a result, each device holds only a portion of the model’s parameters. This makes it possible to handle very large models using multiple smaller devices. Depending on the dimension along which the model is parallelized, model parallelism can be classified into **inter-layer** and **intra-layer** parallelism.\n",
    "\n",
    "![](../images/model_parallelism.png)\n",
    "\n",
    "### Inter-layer Model Parallelism\n",
    "Inter-layer model parallelism splits the model based on layers. For example, layers 1, 2, and 3 can be assigned to GPU 1, while layers 4 and 5 are assigned to GPU 2. A representative example of this approach is Google’s **GPipe**.\n",
    "\n",
    "![](../images/inter_layer.png)\n",
    "\n",
    "### Intra-layer Model Parallelism\n",
    "Intra-layer model parallelism splits tensors themselves regardless of layer boundaries. For instance, if a parameter tensor has a shape of `[256, 256]`, it can be split into `[128, 256]` or `[256, 128]`. A representative example of this approach is NVIDIA’s **Megatron-LM**.\n",
    "\n",
    "![](../images/intra_layer.png)\n",
    "\n",
    "### Pipeline Parallelism\n",
    "Pipeline parallelism is a model parallelism technique designed to address the drawbacks of inter-layer model parallelism. When using inter-layer model parallelism, there is an inherent execution order among GPUs. For example, if layers 1, 2, and 3 cannot be executed, then layers 4 and 5 cannot be executed either. As a result, GPU 2 must wait until GPU 1 finishes its computation.\n",
    "\n",
    "![](../images/pipeline_parallelism.png)\n",
    "\n",
    "<br>\n",
    "\n",
    "This is highly inefficient. Even though multiple GPUs are available, only one GPU is effectively utilized at a time. To solve this problem, pipeline parallelism overlaps computations in a pipelined manner, as illustrated below. (Sounds complicated, right? We’ll explain this in detail later.)\n",
    "\n",
    "<br>\n",
    "\n",
    "![](../images/pipeline_parallelism2.png)\n",
    "\n",
    "<br><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35981dfe-d733-4fc4-ad3b-71f7f6afe985",
   "metadata": {},
   "source": [
    "## 4. Multi-dimensional Parallelism\n",
    "\n",
    "The various parallelization techniques mentioned above can be applied simultaneously, and the dimensionality increases depending on how many types of parallelism are combined. As shown below, n-dimensional parallelism can be achieved in different ways.\n",
    "\n",
    "- e.g. 2D parallelism: Data parallelism + inter-layer parallelism  \n",
    "- e.g. 2D parallelism: Data parallelism + intra-layer parallelism  \n",
    "- e.g. 3D parallelism: Data parallelism + intra-layer parallelism + pipeline parallelism  \n",
    "\n",
    "![](../images/parallelism.png)\n",
    "\n",
    "This type of multi-dimensional parallelism is currently one of the most popular approaches in large-scale model training. In addition to the methods mentioned above, there are other techniques such as **ZeRO**, which will be explained in detail in a later chapter.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6719f680-0dcc-4b5a-b284-30a8cc1fc8b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4896d1b4-9739-498b-a985-ce06c1cba450",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c1ca6d-ab78-4519-8b26-6af26f76cde0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
