{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca4d49d4-0ce7-4e0b-94fd-05abf1a6fffc",
   "metadata": {},
   "source": [
    "# Distributed Programming\n",
    "\n",
    "Because large-scale models are extremely large, they must be split across multiple GPUs. The divided pieces of the model need to communicate with each other over a network to exchange values during computation. This approach‚Äîdistributing large computational resources across multiple computers or devices‚Äîis known as **distributed processing**.\n",
    "\n",
    "In this session, we will learn the fundamentals of distributed programming using PyTorch.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50c109f-971a-4c37-b37b-ce742e9b824b",
   "metadata": {},
   "source": [
    "## 1. Multi-processing with PyTorch\n",
    "\n",
    "Before diving into the distributed programming tutorial, we will first go through a tutorial on multi-processing applications implemented using PyTorch. Concepts such as threads and processes are typically covered in operating systems courses for computer science majors, so we will omit detailed explanations here. If you are not familiar with these concepts, we recommend searching online or reading an article such as:\n",
    "https://www.backblaze.com/blog/whats-the-diff-programs-processes-and-threads/\n",
    "\n",
    "### Basic terminology used in multi-process communication\n",
    "- **Node**: You can think of this as a computer. For example, three nodes mean three computers.\n",
    "- **Global Rank**: Originally refers to process priority, but in **machine learning it can be thought of as the GPU ID**.\n",
    "- **Local Rank**: Originally refers to process priority within a node, but in **machine learning it refers to the GPU ID within a node**.\n",
    "- **World Size**: Refers to the total number of processes.\n",
    "\n",
    "<br>\n",
    "\n",
    "![](../images/process_terms.jpeg)\n",
    "\n",
    "<br>\n",
    "\n",
    "### How to run a multi-process application\n",
    "There are two main ways to run a multi-process application implemented with PyTorch:\n",
    "\n",
    "1. The user‚Äôs code acts as the main process and spawns specific functions as subprocesses.\n",
    "2. The PyTorch launcher acts as the main process and spawns the entire user program as subprocesses.\n",
    "\n",
    "We will examine both approaches. The term *‚Äúspawn‚Äù* here refers to a process acting as a parent and launching multiple subprocesses simultaneously.\n",
    "\n",
    "<br>\n",
    "\n",
    "### 1) The user‚Äôs code acts as the main process and spawns specific functions as subprocesses\n",
    "\n",
    "In this approach, the user‚Äôs code becomes the main process and spawns a specific function as subprocesses.\n",
    "\n",
    "![](../images/multi_process_1.png)\n",
    "\n",
    "<br>\n",
    "\n",
    "In general, there are two ways to spawn subprocesses: `Spawn` and `Fork`.\n",
    "\n",
    "- **`Spawn`**\n",
    "  - Does not inherit resources from the main process; only the necessary resources are newly allocated to the subprocess.\n",
    "  - Slower but safer.\n",
    "- **`Fork`**\n",
    "  - Shares all resources of the main process with the subprocess when starting the process.\n",
    "  - Faster but more dangerous.\n",
    "\n",
    "p.s. In practice, there is also a `Forkserver` method, but it is less commonly used and relatively unfamiliar, so it is omitted here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68436890-b85b-4f99-a1d3-6655cd1a6ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "src/ch2_distributed_programming/multi_process_1.py\n",
    "\n",
    "Note:\n",
    "Jupyter Notebook has many limitations when running multiprocessing applications.\n",
    "Therefore, in most cases, only the code is included here, and execution should be done\n",
    "using the code inside the `src` directory.\n",
    "Please run the actual code from the `src` folder.\n",
    "\"\"\"\n",
    "\n",
    "import torch.multiprocessing as mp\n",
    "# Typically abbreviated as `mp`\n",
    "\n",
    "\n",
    "# Code executed concurrently in subprocesses\n",
    "def fn(rank, param1, param2):\n",
    "    print(f\"{param1} {param2} - rank: {rank}\")\n",
    "\n",
    "\n",
    "# Main process\n",
    "if __name__ == \"__main__\":\n",
    "    processes = []\n",
    "    # Set the start method\n",
    "    mp.set_start_method(\"spawn\")\n",
    "\n",
    "    for rank in range(4):\n",
    "        process = mp.Process(target=fn, args=(rank, \"A0\", \"B1\"))\n",
    "        # Create a subprocess\n",
    "        process.daemon = False\n",
    "        # Whether the process is a daemon (terminates when the main process exits)\n",
    "        process.start()\n",
    "        # Start the subprocess\n",
    "        processes.append(process)\n",
    "\n",
    "    for process in processes:\n",
    "        process.join()\n",
    "        # Join the subprocess (terminate after completion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb9885e-3b54-4858-9f40-a357cd02220b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ../src/ch2_distributed_programming/multi_process_1.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02cfd07-b959-4c08-8564-1c30b79d1e39",
   "metadata": {},
   "source": [
    "The `torch.multiprocessing.spawn` function makes this process significantly easier to implement.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0039fe2-b208-4dd6-8e42-025bea521ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "src/ch2_distributed_programming/multi_process_2.py\n",
    "\"\"\"\n",
    "\n",
    "import torch.multiprocessing as mp\n",
    "\n",
    "\n",
    "# Code executed concurrently in subprocesses\n",
    "def fn(rank, param1, param2):\n",
    "    # `rank` is provided automatically. `param1` and `param2` are passed when calling `spawn`.\n",
    "    print(f\"{param1} {param2} - rank: {rank}\")\n",
    "\n",
    "\n",
    "# Main process\n",
    "if __name__ == \"__main__\":\n",
    "    mp.spawn(\n",
    "        fn=fn,\n",
    "        args=(\"A0\", \"B1\"),\n",
    "        nprocs=4,  # Number of processes to create\n",
    "        join=True,  # Whether to join the processes\n",
    "        daemon=False,  # Whether the processes are daemons\n",
    "        start_method=\"spawn\",  # Set the start method\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce816363-fd1e-4ba0-92a5-b94ddf6ee424",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ../src/ch2_distributed_programming/multi_process_2.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2950ec1e-41b2-4887-beb6-0103e2bcc3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Note: torch/multiprocessing/spawn.py\n",
    "\n",
    "The `mp.spawn` function operates as shown below.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def start_processes(fn, args=(), nprocs=1, join=True, daemon=False, start_method='spawn'):\n",
    "    _python_version_check()\n",
    "    mp = multiprocessing.get_context(start_method)\n",
    "    error_queues = []\n",
    "    processes = []\n",
    "    for i in range(nprocs):\n",
    "        error_queue = mp.SimpleQueue()\n",
    "        process = mp.Process(\n",
    "            target=_wrap,\n",
    "            args=(fn, i, args, error_queue),\n",
    "            daemon=daemon,\n",
    "        )\n",
    "        process.start()\n",
    "        error_queues.append(error_queue)\n",
    "        processes.append(process)\n",
    "\n",
    "    context = ProcessContext(processes, error_queues)\n",
    "    if not join:\n",
    "        return context\n",
    "\n",
    "    # Loop on join until it returns True or raises an exception.\n",
    "    while not context.join():\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d88a3c-7593-4ccd-90c4-1b0ad08c860e",
   "metadata": {},
   "source": [
    "### 2) The PyTorch launcher acts as the parent process and spawns the entire user program as subprocesses\n",
    "\n",
    "This approach uses the multiprocessing launcher built into PyTorch to execute the entire user program as subprocesses, making it a very convenient method.\n",
    "\n",
    "![](../images/multi_process_2.png)\n",
    "\n",
    "<br>\n",
    "\n",
    "This method uses a command such as:\n",
    "`python -m torch.distributed.launch --nproc_per_node=n OOO.py`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c0059a-3623-407e-8535-14764e2e6d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "src/ch2_distributed_programming/multi_process_3.py\n",
    "\"\"\"\n",
    "\n",
    "# The entire code becomes a subprocess.\n",
    "import os\n",
    "\n",
    "# Variables such as RANK, LOCAL_RANK, and WORLD_SIZE are set automatically.\n",
    "print(f\"hello world, {os.environ['RANK']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf389cb-1fd1-471e-b09f-80b939a7b518",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m torch.distributed.launch --nproc_per_node=4 ../src/ch2_distributed_programming/multi_process_3.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f445e494-fb4c-4405-be48-d0841c33da39",
   "metadata": {},
   "source": [
    "## 2. Distributed Programming with PyTorch\n",
    "### Concept of Message Passing\n",
    "\n",
    "Message passing refers to a method in which multiple processes that do not share the same address space exchange data indirectly through messages. For example, if Process-1 is coded to send data with a specific tag to a message queue, and Process-2 is coded to receive that data, the two processes can exchange information without sharing any memory space. Most distributed communication techniques used in large-scale model development rely on this message passing approach.\n",
    "\n",
    "![](../images/message_passing.png)\n",
    "\n",
    "<br>\n",
    "\n",
    "### MPI (Message Passing Interface)\n",
    "MPI refers to a standard interface for message passing. MPI defines various operations used for message passing between processes (e.g., broadcast, reduce, scatter, gather, ...), and a well-known open-source implementation is OpenMPI.\n",
    "\n",
    "![](../images/open_mpi.png)\n",
    "\n",
    "<br>\n",
    "\n",
    "### NCCL & GLOO\n",
    "In practice, libraries such as NCCL or GLOO are used more often than OpenMPI.\n",
    "\n",
    "- **NCCL (NVIDIA Collective Communication Library)**\n",
    "  - A GPU-optimized message passing library developed by NVIDIA (pronounced ‚Äúnickel‚Äù).\n",
    "  - Known to deliver significantly higher performance on NVIDIA GPUs compared to other tools.\n",
    "- **GLOO (Facebook's Collective Communication Library)**\n",
    "  - A message passing library developed by Facebook.\n",
    "  - In `torch`, it is mainly recommended for CPU-based distributed processing.\n",
    "\n",
    "<br>\n",
    "\n",
    "### Backend Library Selection Guide\n",
    "Unless there is a specific reason to use OpenMPI, NCCL or GLOO is generally preferred: use **NCCL for GPU-based workloads** and **GLOO for CPU-based workloads**. For more detailed information, please refer to:\n",
    "https://pytorch.org/docs/stable/distributed.html\n",
    "\n",
    "The operations supported by each backend are shown below.\n",
    "\n",
    "![](../images/backends.png)\n",
    "\n",
    "<br>\n",
    "\n",
    "### The `torch.distributed` Package\n",
    "While directly using libraries such as `gloo`, `nccl`, or `openmpi` can be a valuable learning experience, it is not feasible to cover all of them due to time constraints. Instead, we will proceed using the `torch.distributed` package, which wraps these libraries. In practical applications, developers typically use high-level packages like `torch.distributed` rather than interacting directly with low-level libraries such as `nccl`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34472e1-2d33-4a15-801c-0c97eff29f5a",
   "metadata": {},
   "source": [
    "### Process Group\n",
    "\n",
    "Managing a large number of processes can be challenging, so process groups are used to simplify management. When `init_process_group` is called, a default process group (`default_pg`) that includes all processes is created. The `init_process_group` function, which initializes a process group, **must be executed in subprocesses**. If you want to create an additional group that includes only a specific subset of processes, you can call `new_group`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7e5385-279a-4083-adec-ed99b94444f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "src/ch2_distributed_programming/process_group_1.py\n",
    "\"\"\"\n",
    "\n",
    "import torch.distributed as dist\n",
    "# Typically abbreviated as `dist`\n",
    "\n",
    "dist.init_process_group(backend=\"nccl\", rank=0, world_size=1)\n",
    "# Initialize the process group\n",
    "# In this example, we use NCCL, which is the most commonly used backend.\n",
    "# You can also specify 'mpi' or 'gloo' instead of 'nccl' for the backend.\n",
    "\n",
    "process_group = dist.new_group([0])\n",
    "# Create a process group that includes process with rank 0\n",
    "\n",
    "print(process_group)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efbde70a-bb54-403d-b941-d3401af92567",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ../src/ch2_distributed_programming/process_group_1.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642a19f0-64fe-400b-8b23-7a73f92e5e17",
   "metadata": {},
   "source": [
    "When running the code above, an error occurs because required variables such as `MASTER_ADDR` and `MASTER_PORT` are not set. We will set these values and run the code again.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ddc589-bf57-4c37-a5dd-a330ae600626",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "src/ch2_distributed_programming/process_group_2.py\n",
    "\"\"\"\n",
    "\n",
    "import torch.distributed as dist\n",
    "import os\n",
    "\n",
    "\n",
    "# These values are typically registered and used as environment variables.\n",
    "os.environ[\"RANK\"] = \"0\"\n",
    "os.environ[\"LOCAL_RANK\"] = \"0\"\n",
    "os.environ[\"WORLD_SIZE\"] = \"1\"\n",
    "\n",
    "# Set the address required for communication.\n",
    "os.environ[\"MASTER_ADDR\"] = \"localhost\"  # Address to communicate with (usually localhost)\n",
    "os.environ[\"MASTER_PORT\"] = \"29500\"  # Port for communication (any available value is fine)\n",
    "\n",
    "dist.init_process_group(backend=\"nccl\", rank=0, world_size=1)\n",
    "# Initialize the process group\n",
    "\n",
    "process_group = dist.new_group([0])\n",
    "# Create a process group that includes the process with rank 0\n",
    "\n",
    "print(process_group)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe3204e-a475-4bf1-b517-c6722079774c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ../src/ch2_distributed_programming/process_group_2.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad04cba-3353-4d29-8d8f-39229810ccda",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "src/ch2_distributed_programming/process_group_3.py\n",
    "\"\"\"\n",
    "\n",
    "import torch.multiprocessing as mp\n",
    "import torch.distributed as dist\n",
    "import os\n",
    "\n",
    "\n",
    "# Code executed concurrently in subprocesses\n",
    "def fn(rank, world_size):\n",
    "    # `rank` is provided automatically. `world_size` is passed as an argument.\n",
    "    dist.init_process_group(backend=\"nccl\", rank=rank, world_size=world_size)\n",
    "    group = dist.new_group([_ for _ in range(world_size)])\n",
    "    print(f\"{group} - rank: {rank}\")\n",
    "\n",
    "\n",
    "# Main process\n",
    "if __name__ == \"__main__\":\n",
    "    os.environ[\"MASTER_ADDR\"] = \"localhost\"\n",
    "    os.environ[\"MASTER_PORT\"] = \"29500\"\n",
    "    os.environ[\"WORLD_SIZE\"] = \"4\"\n",
    "\n",
    "    mp.spawn(\n",
    "        fn=fn,\n",
    "        args=(4,),  # Pass world_size\n",
    "        nprocs=4,  # Number of processes to create\n",
    "        join=True,  # Whether to join the processes\n",
    "        daemon=False,  # Whether the processes are daemons\n",
    "        start_method=\"spawn\",  # Set the start method\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08eff31e-c801-4c16-8731-261f35284e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ../src/ch2_distributed_programming/process_group_3.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca887b93-d743-4434-a18d-74721229f722",
   "metadata": {},
   "source": [
    "When launching with `python -m torch.distributed.launch --nproc_per_node=n OOO.py`, the following approach is used. The `rank` and `world_size` can be retrieved via functions like `dist.get_rank()` and `dist.get_world_size()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb17ad3-143c-4564-8464-4538747dadb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "src/ch2_distributed_programming/process_group_4.py\n",
    "\"\"\"\n",
    "\n",
    "import torch.distributed as dist\n",
    "\n",
    "dist.init_process_group(backend=\"nccl\")\n",
    "# Initialize the process group\n",
    "\n",
    "group = dist.new_group([_ for _ in range(dist.get_world_size())])\n",
    "# Create a process group\n",
    "\n",
    "print(f\"{group} - rank: {dist.get_rank()}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5820f283-44a1-4c31-ab49-2356910454f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m torch.distributed.launch --nproc_per_node=4 ../src/ch2_distributed_programming/process_group_4.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99fe69ff-1e1a-4779-9074-ced671c520f2",
   "metadata": {},
   "source": [
    "### P2P Communication (Point-to-Point)\n",
    "\n",
    "![](../images/p2p.png)\n",
    "\n",
    "<br>\n",
    "\n",
    "P2P (Point-to-Point) communication refers to a communication pattern in which a specific process sends data directly to another process. This type of communication can be implemented using the `send` and `recv` functions provided by the `torch.distributed` package.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e0d1f6-6b35-4281-be33-403a16b2540b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "src/ch2_distributed_programming/p2p_communication.py\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "\n",
    "dist.init_process_group(\"gloo\")\n",
    "# Currently, NCCL does not support send/recv. (as of 2021/10/21)\n",
    "\n",
    "if dist.get_rank() == 0:\n",
    "    tensor = torch.randn(2, 2)\n",
    "    dist.send(tensor, dst=1)\n",
    "\n",
    "elif dist.get_rank() == 1:\n",
    "    tensor = torch.zeros(2, 2)\n",
    "    print(f\"rank 1 before: {tensor}\\n\")\n",
    "    dist.recv(tensor, src=0)\n",
    "    print(f\"rank 1 after: {tensor}\\n\")\n",
    "\n",
    "else:\n",
    "    raise RuntimeError(\"wrong rank\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472c1ca8-7b0d-47e8-8703-8d9cfc238396",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m torch.distributed.launch --nproc_per_node=2 ../src/ch2_distributed_programming/p2p_communication.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30dbf396-eded-4271-9c65-32aaf5322c6e",
   "metadata": {},
   "source": [
    "It is important to note that these operations perform **synchronous communication**. For asynchronous (non-blocking) communication, you can use `isend` and `irecv`. Since they operate asynchronously, you must call the `wait()` method and wait for the communication with the other process to complete before accessing the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083a0ef8-16df-448d-b119-0b45500f1854",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "src/ch2_distributed_programming/p2p_communication_non_blocking.py\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "\n",
    "dist.init_process_group(\"gloo\")\n",
    "# Currently, NCCL does not support send/recv. (as of 2021/10/21)\n",
    "\n",
    "if dist.get_rank() == 0:\n",
    "    tensor = torch.randn(2, 2)\n",
    "    request = dist.isend(tensor, dst=1)\n",
    "elif dist.get_rank() == 1:\n",
    "    tensor = torch.zeros(2, 2)\n",
    "    request = dist.irecv(tensor, src=0)\n",
    "else:\n",
    "    raise RuntimeError(\"wrong rank\")\n",
    "\n",
    "request.wait()\n",
    "\n",
    "print(f\"rank {dist.get_rank()}: {tensor}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71aa9d4-aef7-4b64-aac4-a3a2c6d96390",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m torch.distributed.launch --nproc_per_node=2 ../src/ch2_distributed_programming/p2p_communication_non_blocking.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245f07cf-aa5c-4607-b8f1-dca06b20af46",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Collective Communication\n",
    "\n",
    "Collective communication refers to communication in which multiple processes participate together. While there are various collective operations, the basic set consists of the following four operations: `broadcast`, `scatter`, `gather`, and `reduce`.\n",
    "\n",
    "![](../images/collective.png)\n",
    "\n",
    "In addition, we will cover a total of eight operations, including composite operations such as `all-reduce`, `all-gather`, and `reduce-scatter`, as well as the synchronization operation `barrier`. Furthermore, if you want to execute these operations in asynchronous mode, you can set the `async_op` parameter to `True` when performing each operation.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### 1) Broadcast\n",
    "\n",
    "Broadcast is an operation that copies data from a specific process to all processes within a group.\n",
    "\n",
    "![](../images/broadcast.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6929fd-f1c1-4bf6-a289-82821bc90a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "src/ch2_distributed_programming/broadcast.py\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "\n",
    "dist.init_process_group(\"nccl\")\n",
    "rank = dist.get_rank()\n",
    "torch.cuda.set_device(rank)\n",
    "# By setting the device, you can later access the device corresponding to the rank.\n",
    "\n",
    "if rank == 0:\n",
    "    tensor = torch.randn(2, 2).to(torch.cuda.current_device())\n",
    "else:\n",
    "    tensor = torch.zeros(2, 2).to(torch.cuda.current_device())\n",
    "\n",
    "print(f\"before rank {rank}: {tensor}\\n\")\n",
    "dist.broadcast(tensor, src=0)\n",
    "print(f\"after rank {rank}: {tensor}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0cccd3a-af04-47e9-b4a7-ebe32ecf5dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m torch.distributed.launch --nproc_per_node=4 ../src/ch2_distributed_programming/broadcast.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98064be8-1845-497d-b8c7-cc16833ff12b",
   "metadata": {},
   "source": [
    "When P2P operations such as `send` and `recv` are not supported, `broadcast` is sometimes used as an alternative for point-to-point communication. For example, when `src = 0` and `dst = 1`, creating a group with `new_group([0, 1])` and performing a `broadcast` operation is equivalent to P2P communication from rank 0 to rank 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be967cc4-d6ad-456a-b4cc-9ac68be4c062",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Note: deepspeed/deepspeed/runtime/pipe/p2p.py\n",
    "\"\"\"\n",
    "\n",
    "def send(tensor, dest_stage, async_op=False):\n",
    "    global _groups\n",
    "    assert async_op == False, \"Doesnt support async_op true\"\n",
    "    src_stage = _grid.get_stage_id()\n",
    "    _is_valid_send_recv(src_stage, dest_stage)\n",
    "\n",
    "    dest_rank = _grid.stage_to_global(stage_id=dest_stage)\n",
    "    if async_op:\n",
    "        global _async\n",
    "        op = dist.isend(tensor, dest_rank)\n",
    "        _async.append(op)\n",
    "    else:\n",
    "\n",
    "        if can_send_recv():\n",
    "            return dist.send(tensor, dest_rank)\n",
    "        else:\n",
    "            group = _get_send_recv_group(src_stage, dest_stage)\n",
    "            src_rank = _grid.stage_to_global(stage_id=src_stage)\n",
    "            return dist.broadcast(tensor, src_rank, group=group, async_op=async_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9a43c1-5618-4439-ac6f-718d98f3eb3f",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### 2) Reduce\n",
    "Reduce is an operation that applies a specified computation to the data held by each process and collects the result on a single device. Common operations include sum, max, and min.\n",
    "\n",
    "![](../images/reduce.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe87b729-af95-4f33-9c5e-09a39d1abd7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "src/ch2_distributed_programming/reduce_sum.py\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "\n",
    "dist.init_process_group(\"nccl\")\n",
    "rank = dist.get_rank()\n",
    "torch.cuda.set_device(rank)\n",
    "\n",
    "tensor = torch.ones(2, 2).to(torch.cuda.current_device()) * rank\n",
    "# rank==0 => [[0, 0], [0, 0]]\n",
    "# rank==1 => [[1, 1], [1, 1]]\n",
    "# rank==2 => [[2, 2], [2, 2]]\n",
    "# rank==3 => [[3, 3], [3, 3]]\n",
    "\n",
    "dist.reduce(tensor, op=torch.distributed.ReduceOp.SUM, dst=0)\n",
    "\n",
    "if rank == 0:\n",
    "    print(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817b8b49-f9e2-4bc3-8213-b146524446c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m torch.distributed.launch --nproc_per_node=4 ../src/ch2_distributed_programming/reduce_sum.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9eb311-c935-4129-888d-76fccab23e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "src/ch2_distributed_programming/reduce_max.py\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "\n",
    "dist.init_process_group(\"nccl\")\n",
    "rank = dist.get_rank()\n",
    "torch.cuda.set_device(rank)\n",
    "\n",
    "tensor = torch.ones(2, 2).to(torch.cuda.current_device()) * rank\n",
    "# rank==0 => [[0, 0], [0, 0]]\n",
    "# rank==1 => [[1, 1], [1, 1]]\n",
    "# rank==2 => [[2, 2], [2, 2]]\n",
    "# rank==3 => [[3, 3], [3, 3]]\n",
    "\n",
    "dist.reduce(tensor, op=torch.distributed.ReduceOp.MAX, dst=0)\n",
    "\n",
    "if rank == 0:\n",
    "    print(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473f6617-7adc-43f9-9d56-827204a579ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m torch.distributed.launch --nproc_per_node=4 ../src/ch2_distributed_programming/reduce_max.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4fa2106-582b-48aa-be74-a907e331bd3e",
   "metadata": {},
   "source": [
    "#### 3) Scatter\n",
    "Scatter is an operation that splits multiple elements and distributes them across each device.\n",
    "\n",
    "\n",
    "![](../images/scatter.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce76841f-441f-4276-b7e7-4dd0ee21aa0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "src/ch2_distributed_programming/scatter.py\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "\n",
    "dist.init_process_group(\"gloo\")\n",
    "rank = dist.get_rank()\n",
    "torch.cuda.set_device(rank)\n",
    "\n",
    "\n",
    "output = torch.zeros(1)\n",
    "print(f\"before rank {rank}: {output}\\n\")\n",
    "\n",
    "if rank == 0:\n",
    "    inputs = torch.tensor([10.0, 20.0, 30.0, 40.0])\n",
    "    inputs = torch.split(inputs, dim=0, split_size_or_sections=1)\n",
    "    # (tensor([10]), tensor([20]), tensor([30]), tensor([40]))\n",
    "    dist.scatter(output, scatter_list=list(inputs), src=0)\n",
    "else:\n",
    "    dist.scatter(output, src=0)\n",
    "\n",
    "print(f\"after rank {rank}: {output}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a673efa-7c08-4956-88f4-179fc9807d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m torch.distributed.launch --nproc_per_node=4 ../src/ch2_distributed_programming/scatter.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699f28e3-2515-47d7-868f-2b924cf1a4d4",
   "metadata": {},
   "source": [
    "Since NCCL does not support the `scatter` operation, the following approach is used to perform a scatter operation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2929f8ce-0a58-4755-b8f6-eced43cb65cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "src/ch2_distributed_programming/scatter_nccl.py\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "\n",
    "dist.init_process_group(\"nccl\")\n",
    "rank = dist.get_rank()\n",
    "torch.cuda.set_device(rank)\n",
    "\n",
    "inputs = torch.tensor([10.0, 20.0, 30.0, 40.0])\n",
    "inputs = torch.split(tensor=inputs, dim=-1, split_size_or_sections=1)\n",
    "output = inputs[rank].contiguous().to(torch.cuda.current_device())\n",
    "print(f\"after rank {rank}: {output}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0a4a82-a6c0-4905-8916-1fec3b5f18ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m torch.distributed.launch --nproc_per_node=4 ../src/ch2_distributed_programming/scatter_nccl.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26aa3d48-cf94-4e86-abf0-8b5647368c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Note: megatron-lm/megatron/mpu/mappings.py\n",
    "\"\"\"\n",
    "\n",
    "def _split(input_):\n",
    "    \"\"\"Split the tensor along its last dimension and keep the\n",
    "    corresponding slice.\"\"\"\n",
    "\n",
    "    world_size = get_tensor_model_parallel_world_size()\n",
    "    # Bypass the function if we are using only 1 GPU.\n",
    "    if world_size==1:\n",
    "        return input_\n",
    "\n",
    "    # Split along last dimension.\n",
    "    input_list = split_tensor_along_last_dim(input_, world_size)\n",
    "\n",
    "    # Note: torch.split does not create contiguous tensors by default.\n",
    "    rank = get_tensor_model_parallel_rank()\n",
    "    output = input_list[rank].contiguous()\n",
    "\n",
    "    return output\n",
    "\n",
    "class _ScatterToModelParallelRegion(torch.autograd.Function):\n",
    "    \"\"\"Split the input and keep only the corresponding chuck to the rank.\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def symbolic(graph, input_):\n",
    "        return _split(input_)\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, input_):\n",
    "        return _split(input_)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        return _gather(grad_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4b0b93-8c67-41ba-9ae4-a653d4222eb5",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### 4) Gather\n",
    "Gather is an operation that collects tensors from multiple devices and combines them into a single tensor.\n",
    "\n",
    "![](../images/gather.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130e6b67-7bec-4f1a-ad44-c837e0709b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "src/ch2_distributed_programming/gather.py\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "\n",
    "dist.init_process_group(\"gloo\")\n",
    "# NCCL does not support gather.\n",
    "rank = dist.get_rank()\n",
    "torch.cuda.set_device(rank)\n",
    "\n",
    "input = torch.ones(1) * rank\n",
    "# rank == 0 => [0]\n",
    "# rank == 1 => [1]\n",
    "# rank == 2 => [2]\n",
    "# rank == 3 => [3]\n",
    "\n",
    "if rank == 0:\n",
    "    outputs_list = [torch.zeros(1), torch.zeros(1), torch.zeros(1), torch.zeros(1)]\n",
    "    dist.gather(input, gather_list=outputs_list, dst=0)\n",
    "    print(outputs_list)\n",
    "else:\n",
    "    dist.gather(input, dst=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e8bf72-5a7b-4011-86d7-db4fe72895ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m torch.distributed.launch --nproc_per_node=4 ../src/ch2_distributed_programming/gather.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a1422a-b676-4b98-a1c8-c0f304b9cdb3",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### 5) All-reduce\n",
    "Operations prefixed with **All-** perform the specified operation and then broadcast the result to all devices. As shown in the figure below, All-reduce first performs a reduce operation and then copies the computed result to every device.\n",
    "\n",
    "![](../images/allreduce.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f08277-90da-4a41-a948-64be2507fe3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "src/ch2_distributed_programming/allreduce_sum.py\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "\n",
    "dist.init_process_group(\"nccl\")\n",
    "rank = dist.get_rank()\n",
    "torch.cuda.set_device(rank)\n",
    "\n",
    "tensor = torch.ones(2, 2).to(torch.cuda.current_device()) * rank\n",
    "# rank==0 => [[0, 0], [0, 0]]\n",
    "# rank==1 => [[1, 1], [1, 1]]\n",
    "# rank==2 => [[2, 2], [2, 2]]\n",
    "# rank==3 => [[3, 3], [3, 3]]\n",
    "\n",
    "dist.all_reduce(tensor, op=torch.distributed.ReduceOp.SUM)\n",
    "\n",
    "print(f\"rank {rank}: {tensor}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d3b418-1386-4ea6-9663-9fb0731c32bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m torch.distributed.launch --nproc_per_node=4 ../src/ch2_distributed_programming/allreduce_sum.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ababb0-f8d7-4407-b9a2-994b55872c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "src/allreduce_max.py\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "\n",
    "dist.init_process_group(\"nccl\")\n",
    "rank = dist.get_rank()\n",
    "torch.cuda.set_device(rank)\n",
    "\n",
    "tensor = torch.ones(2, 2).to(torch.cuda.current_device()) * rank\n",
    "# rank==0 => [[0, 0], [0, 0]]\n",
    "# rank==1 => [[1, 1], [1, 1]]\n",
    "# rank==2 => [[2, 2], [2, 2]]\n",
    "# rank==3 => [[3, 3], [3, 3]]\n",
    "\n",
    "dist.all_reduce(tensor, op=torch.distributed.ReduceOp.MAX)\n",
    "\n",
    "print(f\"rank {rank}: {tensor}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9341373-b691-4fd6-a737-69c1e7dc01d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m torch.distributed.launch --nproc_per_node=4 ../src/ch2_distributed_programming/allreduce_max.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b88caf-8622-493e-8fec-f2bb1d0f13d2",
   "metadata": {},
   "source": [
    "#### 6) All-gather\n",
    "All-gather performs a gather operation and then copies the collected result to all devices.\n",
    "\n",
    "![](../images/allgather.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5041031-e5d0-49c1-ab90-37212bce8662",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "src/ch2_distributed_programming/allgather.py\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "\n",
    "dist.init_process_group(\"nccl\")\n",
    "rank = dist.get_rank()\n",
    "torch.cuda.set_device(rank)\n",
    "\n",
    "input = torch.ones(1).to(torch.cuda.current_device()) * rank\n",
    "# rank==0 => [0]\n",
    "# rank==1 => [1]\n",
    "# rank==2 => [2]\n",
    "# rank==3 => [3]\n",
    "\n",
    "outputs_list = [\n",
    "    torch.zeros(1, device=torch.device(torch.cuda.current_device())),\n",
    "    torch.zeros(1, device=torch.device(torch.cuda.current_device())),\n",
    "    torch.zeros(1, device=torch.device(torch.cuda.current_device())),\n",
    "    torch.zeros(1, device=torch.device(torch.cuda.current_device())),\n",
    "]\n",
    "\n",
    "dist.all_gather(tensor_list=outputs_list, tensor=input)\n",
    "print(outputs_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9897fdb-d7c5-42ab-9ee4-e065b0d94e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m torch.distributed.launch --nproc_per_node=4 ../src/ch2_distributed_programming/allgather.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613a020d-5ae3-4603-bef0-9d6bd4bc5aa7",
   "metadata": {},
   "source": [
    "#### 7) Reduce-scatter\n",
    "Reduce-scatter performs a reduce operation and then splits the result, returning a portion of the output to each device.\n",
    "\n",
    "![](../images/reduce_scatter.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a848cad-d156-4070-aed0-e8405aa7a290",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "src/ch2_distributed_programming/reduce_scatter.py\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "\n",
    "dist.init_process_group(\"nccl\")\n",
    "rank = dist.get_rank()\n",
    "torch.cuda.set_device(rank)\n",
    "\n",
    "input_list = torch.tensor([1, 10, 100, 1000]).to(torch.cuda.current_device()) * rank\n",
    "input_list = torch.split(input_list, dim=0, split_size_or_sections=1)\n",
    "# rank==0 => [0, 00, 000, 0000]\n",
    "# rank==1 => [1, 10, 100, 1000]\n",
    "# rank==2 => [2, 20, 200, 2000]\n",
    "# rank==3 => [3, 30, 300, 3000]\n",
    "\n",
    "output = torch.tensor([0], device=torch.device(torch.cuda.current_device()),)\n",
    "\n",
    "dist.reduce_scatter(\n",
    "    output=output,\n",
    "    input_list=list(input_list),\n",
    "    op=torch.distributed.ReduceOp.SUM,\n",
    ")\n",
    "\n",
    "print(f\"rank {rank}: {output}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d52a5a-34a9-43c1-a7e8-2d7ee10dc381",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m torch.distributed.launch --nproc_per_node=4 ../src/ch2_distributed_programming/reduce_scatter.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45cd218-71a6-40f4-9d3d-fcc760036547",
   "metadata": {},
   "source": [
    "#### 8) Barrier\n",
    "A barrier is used to synchronize processes. Processes that reach the barrier first will wait until all processes have reached the same point.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0c6c68-c3e1-4771-88d5-3bf8df43fa2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "src/ch2_distributed_programming/barrier.py\n",
    "\"\"\"\n",
    "import time\n",
    "import torch.distributed as dist\n",
    "\n",
    "dist.init_process_group(\"nccl\")\n",
    "rank = dist.get_rank()\n",
    "\n",
    "if rank == 0:\n",
    "    seconds = 0\n",
    "    while seconds <= 3:\n",
    "        time.sleep(1)\n",
    "        seconds += 1\n",
    "        print(f\"rank 0 - seconds: {seconds}\\n\")\n",
    "\n",
    "print(f\"rank {rank}: no-barrier\\n\")\n",
    "dist.barrier()\n",
    "print(f\"rank {rank}: barrier\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8c1b2a-8b15-4a95-9fab-bf951a60f07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m torch.distributed.launch --nproc_per_node=4 ../src/ch2_distributed_programming/barrier.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e443337-1935-4abc-9c75-a0f29fc5965a",
   "metadata": {},
   "source": [
    "### That‚Äôs a lot, right...? üòÖ\n",
    "You only need to remember the four basic operations below‚Äîmost others can be inferred from them.\n",
    "\n",
    "![](../images/collective.png)\n",
    "\n",
    "Based on these four operations, keep the following points in mind:\n",
    "\n",
    "- `all-reduce` and `all-gather` can be thought of as performing the corresponding operation first, followed by a `broadcast`.\n",
    "- `reduce-scatter` literally means taking the result of a `reduce` operation and then **scattering (splitting)** it.\n",
    "- `barrier` works exactly like its name suggests‚Äîa wall. Processes that arrive early are blocked, like hitting a wall, until all processes reach the same point.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8dae7e-2550-46c5-8c6e-5fb5354f8f16",
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
