{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bac696c2-f688-4491-a883-c67a3ad5f18d",
   "metadata": {},
   "source": [
    "# Tensor Parallelism\n",
    "In this session, we will explore tensor parallelism.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93aefb04-c823-4367-8a73-90ca76f7a964",
   "metadata": {},
   "source": [
    "## 1. Intra-layer Model Parallelism\n",
    "Tensor parallelism is an intra-layer model parallelism approach that **splits the model at the tensor level within a layer**. Inter-layer model parallelism is relatively intuitive, but for those encountering intra-layer parallelism for the first time, it may not be immediately clear how this is even possible.\n",
    "\n",
    "![](../images/intra_layer.png)\n",
    "\n",
    "The inner product operations that we commonly use have a property where splitting the matrices involved, performing the computations in parallel, and then summing or concatenating the results does not change the final output. Tensor parallelism leverages this property of inner product operations to parallelize models.\n",
    "\n",
    "The terminology can be a bit confusing here: **intra-layer parallelism** is a broader concept that refers to all forms of parallelism that occur within a layer, while **tensor parallelism** is one specific method for implementing intra-layer parallelism.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b745a6-acdb-4350-a263-8e9420820150",
   "metadata": {},
   "source": [
    "## 2. Megatron-LM\n",
    "Megatron-LM is an intra-layer model parallelism implementation released by NVIDIA, and it is currently one of the most important projects in large-scale model development.\n",
    "\n",
    "<img src=\"../images/megatron_lm.jpeg\" width=540>\n",
    "\n",
    "### Column & Row Parallelism\n",
    "Below are illustrations of column parallelism and row parallelism used in Megatron-LM.\n",
    "\n",
    "- **Column parallelism** splits the model parameters (A) **vertically** into parts (A1, A2).\n",
    "- **Row parallelism** splits the model parameters (A) **horizontally** into parts (A1, A2).\n",
    "\n",
    "![](../images/intra_layer_2.png)\n",
    "\n",
    "Let’s verify the results by coding them directly. First, consider the matrix multiplication result of tensor X and tensor A, as shown below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757aa846-7c04-4991-9425-bb9b875cc98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "src/non_parallelism.py\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "\n",
    "X = torch.tensor(\n",
    "    [\n",
    "        [0, 1, 2, 3],\n",
    "        [4, 5, 6, 7],\n",
    "    ]\n",
    ")\n",
    "\n",
    "A = torch.tensor(\n",
    "    [\n",
    "        [10, 14],\n",
    "        [11, 15],\n",
    "        [12, 16],\n",
    "        [13, 17],        \n",
    "    ]\n",
    ")\n",
    "\n",
    "Y = X @ A\n",
    "\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a274133b-a2f9-4033-aa8c-8d0a11f069d2",
   "metadata": {},
   "source": [
    "Column parallelism works by splitting the model parameters (A) **vertically**, performing the computation, and then **concatenating** the results. As shown in the figure, we replicate tensor X, split tensor A vertically, perform the computation, and then concatenate the outputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2455e78-4647-49dd-bc9e-1f2b7fecc762",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "src/column_parallelism.py\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "\n",
    "X = torch.tensor(\n",
    "    [\n",
    "        [0, 1, 2, 3],\n",
    "        [4, 5, 6, 7],\n",
    "    ]\n",
    ")\n",
    "\n",
    "A1 = torch.tensor(\n",
    "    [\n",
    "        [10],\n",
    "        [11],\n",
    "        [12],\n",
    "        [13],        \n",
    "    ]\n",
    ")\n",
    "\n",
    "A2 = torch.tensor(\n",
    "    [\n",
    "        [14],\n",
    "        [15],\n",
    "        [16],\n",
    "        [17],        \n",
    "    ]\n",
    ")\n",
    "\n",
    "Y1 = X @ A1\n",
    "Y2 = X @ A2\n",
    "\n",
    "print(Y1)\n",
    "print(Y2)\n",
    "\n",
    "Y = torch.cat([Y1, Y2], dim=1)\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c1784b-6154-4f42-8331-35f006a6fa15",
   "metadata": {},
   "source": [
    "We can verify that the computation results are identical before and after parallelization.\n",
    "\n",
    "Next, let’s look at **row parallelism**. Row parallelism works by splitting the model parameters (A) **horizontally** and then **summing** the computation results. As illustrated, we split both X and Y, perform the computations, and then add the results together.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95435494-eb06-41c9-a90a-8ed47401ddff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "src/row_parallelism.py\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "\n",
    "X1 = torch.tensor(\n",
    "    [\n",
    "        [0, 1],\n",
    "        [4, 5],\n",
    "    ]\n",
    ")\n",
    "\n",
    "X2 = torch.tensor(\n",
    "    [\n",
    "        [2, 3],\n",
    "        [6, 7],\n",
    "    ]\n",
    ")\n",
    "\n",
    "A1 = torch.tensor(\n",
    "    [\n",
    "        [10, 14],\n",
    "        [11, 15],      \n",
    "    ]\n",
    ")\n",
    "\n",
    "A2 = torch.tensor(\n",
    "    [\n",
    "        [12, 16],\n",
    "        [13, 17],        \n",
    "    ]\n",
    ")\n",
    "\n",
    "Y1 = X1 @ A1\n",
    "Y2 = X2 @ A2\n",
    "\n",
    "print(Y1)\n",
    "print(Y2)\n",
    "\n",
    "Y = Y1 + Y2\n",
    "\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52de3a80-a2ab-4c68-ab86-c81a38c0eecd",
   "metadata": {},
   "source": [
    "We can confirm that the computation results are identical.\n",
    "\n",
    "<br>\n",
    "\n",
    "### Column Parallelism: $(D, D) \\rightarrow (D, \\frac{D}{n}) \\times n$\n",
    "\n",
    "As seen in the previous example, **column parallelism** works by **replicating the input tensor (X)**, splitting the model parameters (A) **vertically** into parts (A1, A2), performing the inner product, and then concatenating the results.\n",
    "\n",
    "<br>\n",
    "\n",
    "![](../images/column_parallel.png)\n",
    "\n",
    "<br>\n",
    "\n",
    "In Megatron-LM, the **partitioned parameters (A1, A2) are placed on different devices to parallelize the model**. As a result, the matrix multiplication is performed simultaneously across multiple GPUs, which requires distributed programming to coordinate the computation. For column parallelism, **Broadcast** and **All-gather** operations are used.\n",
    "\n",
    "- **Broadcast** is used to send the same input to different GPUs.\n",
    "- **All-gather** is used to collect the results of the matrix multiplications.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da79ebf5-9d61-4047-bed6-16c65b7d5bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Note: ColumnParallelLinear in megatron-lm/megatron/mpu/layers.py\n",
    "\"\"\"\n",
    "\n",
    "def forward(self, input_):\n",
    "    bias = self.bias if not self.skip_bias_add else None\n",
    "\n",
    "    # Set up backprop all-reduce.\n",
    "    input_parallel = copy_to_tensor_model_parallel_region(input_)\n",
    "\n",
    "    # Matrix multiply.\n",
    "    output_parallel = F.linear(input_parallel, self.weight, bias)\n",
    "\n",
    "    if self.gather_output:\n",
    "        output = gather_from_tensor_model_parallel_region(output_parallel)\n",
    "    else:\n",
    "        output = output_parallel\n",
    "    \n",
    "    output_bias = self.bias if self.skip_bias_add else None\n",
    "    return output, output_bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6017d439-0561-4055-b6fe-cca7f33cd5c7",
   "metadata": {},
   "source": [
    "### Row Parallelism: $(D, D) \\rightarrow (\\frac{D}{n}, D) \\times n$\n",
    "\n",
    "Row parallelism works by **splitting the input tensor (X)** and splitting the model parameters (A) **horizontally** into parts (A1, A2), performing the inner product, and then **summing** the results.\n",
    "\n",
    "<br>\n",
    "\n",
    "![](../images/row_parallelism.png)\n",
    "\n",
    "<br>\n",
    "\n",
    "Similarly, executing row parallelism across multiple GPUs requires distributed programming. For row parallelism, **Scatter** and **All-reduce** operations are used.\n",
    "\n",
    "- **Scatter** is used to split and distribute the input across different GPUs.\n",
    "- **All-reduce** is used to sum the results of the matrix multiplications.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7460b68-6202-4e14-9694-28e7488b8f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Note: RowParallelLinear in megatron-lm/megatron/mpu/layers.py\n",
    "\"\"\"\n",
    "\n",
    "def forward(self, input_):\n",
    "    # Set up backprop all-reduce.\n",
    "    if self.input_is_parallel:\n",
    "        input_parallel = input_\n",
    "    else:\n",
    "        input_parallel = scatter_to_tensor_model_parallel_region(input_)\n",
    "    \n",
    "    # Matrix multiply.\n",
    "    output_parallel = F.linear(input_parallel, self.weight)\n",
    "    \n",
    "    # All-reduce across all the partitions.\n",
    "    output_ = reduce_from_tensor_model_parallel_region(output_parallel)\n",
    "    \n",
    "    if not self.skip_bias_add:\n",
    "        output = output_ + self.bias if self.bias is not None else output_\n",
    "        output_bias = None\n",
    "    else:\n",
    "        output = output_\n",
    "        output_bias = self.bias\n",
    "    return output, output_bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0628eb7e-4b3c-41ce-b730-bfb9a4965892",
   "metadata": {},
   "source": [
    "### Transformer Block\n",
    "\n",
    "Now that we understand column and row parallelism, let’s take a closer look at how a Transformer can be parallelized. A typical Transformer block that we are familiar with is structured as shown below. In Megatron-LM, layers with very small parameter sizes—such as **LayerNorm**—have their parameters replicated across all devices. The other layers (Attention and MLP), excluding LayerNorm, are parallelized using column and row parallelism as described above.\n",
    "\n",
    "![](../images/megatron_block.png)\n",
    "\n",
    "<br>\n",
    "\n",
    "### MLP Layer\n",
    "\n",
    "Let’s start by examining the MLP layer. The MLP layer consists of the following sequence:  \n",
    "`Linear1` → `GeLU` → `Linear2` → `Dropout`.\n",
    "\n",
    "<br>\n",
    "\n",
    "![](../images/megatron_mlp.png)\n",
    "\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198c2ccc-cd35-4edf-aff0-055885282afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Note: transformers/models/gpt_neo/modeling_gpt_neo.py\n",
    "\"\"\"\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class GPTNeoMLP(nn.Module):\n",
    "    def __init__(self, intermediate_size, config):  # in MLP: intermediate_size= 4 * hidden_size\n",
    "        super().__init__()\n",
    "        embed_dim = config.hidden_size\n",
    "        self.c_fc = nn.Linear(embed_dim, intermediate_size)\n",
    "        self.c_proj = nn.Linear(intermediate_size, embed_dim)\n",
    "        self.act = ACT2FN[config.activation_function]\n",
    "        self.dropout = nn.Dropout(config.resid_dropout)\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        hidden_states = self.c_fc(hidden_states)\n",
    "        hidden_states = self.act(hidden_states)\n",
    "        hidden_states = self.c_proj(hidden_states)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7bdf17-0591-42d8-a91f-27bf36d3f262",
   "metadata": {},
   "source": [
    "Here, the **first Linear layer uses column parallelism**, and the **second Linear layer uses row parallelism**.\n",
    "\n",
    "<br>\n",
    "\n",
    "![](../images/megatron_mlp_2.png)\n",
    "\n",
    "<br>\n",
    "\n",
    "There are two reasons why column–row parallelism is applied in this order in the MLP layer.\n",
    "\n",
    "- The first reason is that it allows us to **skip the `All-gather` and `Scatter` operations**.\n",
    "\n",
    "<br>\n",
    "\n",
    "![](../images/megatron_mlp_3.png)\n",
    "\n",
    "<br>\n",
    "\n",
    "The computation in the green region on the left is the inner product between the input data X and the weight matrix W that has been parallelized across devices. Then, in the red region, these results are `All-gather`ed and concatenated, and subsequently `Scatter`ed again. An interesting observation here is that since the concatenated tensor is split again, the result is identical to the state before concatenation. Therefore, the values in the green region on the right are the same as those in the green region on the left.\n",
    "\n",
    "As a result, the red region (`All-gather` → `Scatter`) can be omitted, leading to a significant performance gain.\n",
    "\n",
    "This is a unique phenomenon that occurs **only when parallelization is applied in a column–row order**. If other combinations such as column–column, row–column, or row–row parallelism are used, the communication between the two Linear layers cannot be skipped.\n",
    "\n",
    "<br>\n",
    "\n",
    "![](../images/megatron_mlp_4.png)\n",
    "\n",
    "<br>\n",
    "\n",
    "The technique for skipping `All-gather` and `Scatter` is implemented in Megatron-LM via the parameters `input_is_parallel` and `gather_output`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1387a6b4-105a-4b9c-803c-aa3a32cdf3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Reference: ColumnParallelLinear in megatron-lm/megatron/mpu/layers.py\n",
    "\"\"\"\n",
    "\n",
    "def forward(self, input_):\n",
    "    bias = self.bias if not self.skip_bias_add else None\n",
    "\n",
    "    # Set up backprop all-reduce.\n",
    "    input_parallel = copy_to_tensor_model_parallel_region(input_)\n",
    "\n",
    "    # Matrix multiply.\n",
    "    output_parallel = F.linear(input_parallel, self.weight, bias)\n",
    "\n",
    "    # Set gather_output to False to return the output in a parallelized form.\n",
    "    if self.gather_output:\n",
    "        output = gather_from_tensor_model_parallel_region(output_parallel)\n",
    "    else:\n",
    "        output = output_parallel\n",
    "\n",
    "    output_bias = self.bias if self.skip_bias_add else None\n",
    "    return output, output_bias\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Reference: RowParallelLinear in megatron-lm/megatron/mpu/layers.py\n",
    "\"\"\"\n",
    "\n",
    "def forward(self, input_):\n",
    "    # Set up backprop all-reduce.\n",
    "\n",
    "    # Set input_is_parallel to True to receive the input in a parallelized form.\n",
    "    if self.input_is_parallel:\n",
    "        input_parallel = input_\n",
    "    else:\n",
    "        input_parallel = scatter_to_tensor_model_parallel_region(input_)\n",
    "    \n",
    "    # Matrix multiply.\n",
    "    output_parallel = F.linear(input_parallel, self.weight)\n",
    "    \n",
    "    # All-reduce across all partitions.\n",
    "    output_ = reduce_from_tensor_model_parallel_region(output_parallel)\n",
    "    \n",
    "    if not self.skip_bias_add:\n",
    "        output = output_ + self.bias if self.bias is not None else output_\n",
    "        output_bias = None\n",
    "    else:\n",
    "        output = output_\n",
    "        output_bias = self.bias\n",
    "    return output, output_bias\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637ffc9d-2dfb-4688-9df6-153771c7d6cf",
   "metadata": {},
   "source": [
    "- The second reason for using **column–row parallelism** is that, in order to skip `Scatter` and `All-gather`, the **GeLU operation must be executed in a parallelized form**.\n",
    "\n",
    "<br>\n",
    "\n",
    "![](../images/megatron_mlp_5.png)\n",
    "\n",
    "<br>\n",
    "\n",
    "The figure above shows a case where the GeLU operation is inserted between the two Linear layers **without skipping** `Scatter` and `All-gather`. If we implement the model in a way that skips these two operations, then the GeLU operation must be executed independently on each device, as shown below.\n",
    "\n",
    "<br>\n",
    "\n",
    "![](../images/megatron_mlp_6.png)\n",
    "\n",
    "<br>\n",
    "\n",
    "However, for GeLU to be parallelized across different devices in this way, the output of the GeLU computed in parallel must be identical to the output of the GeLU computed in a non-parallelized manner. In other words, the following equations must hold. (The symbol $\\circledcirc$ denotes concatenation.)\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "\\text{Row Parallelism: } \\mathrm{GeLU}(XW_1 + XW_2) = \\mathrm{GeLU}(XW_1) + \\mathrm{GeLU}(XW_2)\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "\\text{Column Parallelism: } \\mathrm{GeLU}(XW_1 \\circledcirc XW_2) = \\mathrm{GeLU}(XW_1) \\circledcirc \\mathrm{GeLU}(XW_2)\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "The problem is that these equations **only hold for column parallelism**, and **do not hold for row parallelism**.\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "\\text{Row Parallelism: } \\mathrm{GeLU}(XW_1 + XW_2) \\neq \\mathrm{GeLU}(XW_1) + \\mathrm{GeLU}(XW_2)\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "Let’s verify this by implementing it in code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db772161-a485-448e-94ee-8a8cf0575678",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "src/megatron_mlp_gelu.py\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "from torch.nn.functional import gelu\n",
    "\n",
    "\n",
    "w = torch.randn(6, 6)\n",
    "x = torch.randn(6, 6)\n",
    "\n",
    "\n",
    "class RowParallelLinear(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RowParallelLinear, self).__init__()\n",
    "        chunked = torch.chunk(w, 2, dim=0)\n",
    "\n",
    "        # row parallelized parameters\n",
    "        self.w1 = chunked[0]  # [3, 6]\n",
    "        self.w2 = chunked[1]  # [3, 6]\n",
    "\n",
    "    def forward(self, x):\n",
    "        # GeLU(X1A1 + X2A2) != GeLU(X1A1) + GeLU(X2A2)\n",
    "        x1, x2 = torch.chunk(x, 2, dim=1)\n",
    "\n",
    "        # parallel output\n",
    "        y1 = gelu(x1 @ self.w1) + gelu(x2 @ self.w2)\n",
    "\n",
    "        # non-parallel output\n",
    "        y2 = gelu(x1 @ self.w1 + x2 @ self.w2)\n",
    "\n",
    "        return torch.all(y1 == y2)\n",
    "\n",
    "\n",
    "class ColumnParallelLinear(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ColumnParallelLinear, self).__init__()\n",
    "        chunked = torch.chunk(w, 2, dim=1)\n",
    "\n",
    "        # column parallelized parameters\n",
    "        self.w1 = chunked[0]  # [6, 3]\n",
    "        self.w2 = chunked[1]  # [6, 3]\n",
    "\n",
    "    def forward(self, x):\n",
    "        # GeLU(X1A1 cat X2A2) == GeLU(X1A1) cat GeLU(X2A2)\n",
    "\n",
    "        # parallel output\n",
    "        y1 = torch.cat([gelu(x @ self.w1), gelu(x @ self.w2)], dim=1)\n",
    "\n",
    "        # non-parallel output\n",
    "        y2 = gelu(torch.cat([(x @ self.w1), (x @ self.w2)], dim=1))\n",
    "\n",
    "        return torch.all(y1 == y2)\n",
    "\n",
    "\n",
    "# Row Parallelism\n",
    "print(\"Is GeLU in RowParallelLinear same with non-parallel = \", end=\"\")\n",
    "print(RowParallelLinear()(x).item())\n",
    "\n",
    "# Column Parallelism\n",
    "print(\"Is GeLU in ColumnParallelLinear same with non-parallel = \", end=\"\")\n",
    "print(ColumnParallelLinear()(x).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5447040f-0b28-4eb5-8ddb-46dd2a84f819",
   "metadata": {},
   "source": [
    "Therefore, in order to parallelize the GeLU operation, the Linear layer preceding GeLU must be parallelized in the **column direction**. As a result, applying parallelism in a **column–row order** is the most efficient approach.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699f3ecb-6b88-4149-b01a-b93a82dc4165",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Multi-head Attention Layer\n",
    "\n",
    "Next, let’s examine the Multi-head Attention layer. The Multi-head Attention layer follows the sequence:  \n",
    "`Linear1` → `Split heads` → `Scaled Dot-Product Attention` → `Concat (Merge) heads` → `Linear2` → `Dropout`.\n",
    "\n",
    "![](../images/multi_head_attention.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa5a954-984e-4c8a-8228-0eaed62e1dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Reference: transformers/models/gpt_neo/modeling_gpt_neo.py\n",
    "\"\"\"\n",
    "\n",
    "class GPTNeoSelfAttention(nn.Module):\n",
    "    def __init__(self, config, attention_type):\n",
    "        super().__init__()\n",
    "        self.attn_dropout = nn.Dropout(config.attention_dropout)\n",
    "        self.resid_dropout = nn.Dropout(config.resid_dropout)\n",
    "\n",
    "        self.embed_dim = config.hidden_size\n",
    "        self.num_heads = config.num_heads\n",
    "        self.head_dim = self.embed_dim // self.num_heads\n",
    "        if self.head_dim * self.num_heads != self.embed_dim:\n",
    "            raise ValueError(\n",
    "                f\"embed_dim must be divisible by num_heads (got `embed_dim`: {self.embed_dim} and `num_heads`: {self.num_heads}).\"\n",
    "            )\n",
    "\n",
    "        self.k_proj = nn.Linear(self.embed_dim, self.embed_dim, bias=False)\n",
    "        self.v_proj = nn.Linear(self.embed_dim, self.embed_dim, bias=False)\n",
    "        self.q_proj = nn.Linear(self.embed_dim, self.embed_dim, bias=False)\n",
    "        self.out_proj = nn.Linear(self.embed_dim, self.embed_dim, bias=True)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states,\n",
    "        attention_mask=None,\n",
    "        layer_past=None,\n",
    "        head_mask=None,\n",
    "        use_cache=False,\n",
    "        output_attentions=False,\n",
    "    ):\n",
    "        # 1. linear projection\n",
    "        query = self.q_proj(hidden_states)\n",
    "        key = self.k_proj(hidden_states)\n",
    "        value = self.v_proj(hidden_states)\n",
    "        \n",
    "        # 2. split heads\n",
    "        query = self._split_heads(query, self.num_heads, self.head_dim)\n",
    "        key = self._split_heads(key, self.num_heads, self.head_dim)\n",
    "        value = self._split_heads(value, self.num_heads, self.head_dim)\n",
    "\n",
    "        # 3. scale dot product attention\n",
    "        attn_output, attn_weights = self._attn(query, key, value, attention_mask, head_mask)\n",
    "\n",
    "        # 4. concat (merge) heads\n",
    "        attn_output = self._merge_heads(attn_output, self.num_heads, self.head_dim)\n",
    "        \n",
    "        # 5. linear projection\n",
    "        attn_output = self.out_proj(attn_output)\n",
    "        \n",
    "        # 6. dropout\n",
    "        attn_output = self.resid_dropout(attn_output)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8895b063-2636-45b3-a877-00383e3f1bf1",
   "metadata": {},
   "source": [
    "![](../images/megatron_attention.jpeg)\n",
    "\n",
    "<br>\n",
    "\n",
    "Megatron-LM parallelizes the **Q, K, V linear projections** and the **output projection** in the Attention layer. Similarly, the Q, K, V linear projection layers are handled using **column parallelism**, while the output projection layer is handled using **row parallelism**, forming a **column–row pattern**. \n",
    "\n",
    "By doing so, the Attention layer—just like the MLP layer—can **skip the `Scatter` and `All-gather` operations**, resulting in more efficient execution.\n",
    "\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32dfc171-81d4-4112-8c7b-bcaa20cc0a99",
   "metadata": {},
   "source": [
    "### Vocab Parallel Embedding\n",
    "\n",
    "Megatron-LM also parallelizes the word embedding layer. A unique aspect is that the parallelization is done along the **vocabulary size dimension**. For example, if we have a word embedding matrix with a vocabulary size of 50,000, the matrix has a shape of `(50,000, embedding_dim)`. Megatron-LM parallelizes this matrix along the vocabulary dimension. This distinctive parallelization technique is called **Vocab Parallel Embedding**.\n",
    "\n",
    "![](../images/vpe_1.png)\n",
    "\n",
    "<br>\n",
    "\n",
    "The figure above shows word embedding without parallelization. When a sequence of length 6 is given as input, an input tensor of shape `[6, embedding_dim]` is produced.\n",
    "\n",
    "<br>\n",
    "\n",
    "![](../images/vpe_2.png)\n",
    "\n",
    "The figure above illustrates how vocab parallel embedding works. The original embedding matrix is split into two halves: one embedding matrix responsible for tokens from index 0 to 24,999, and another responsible for tokens from index 25,000 to 50,000. When input data is provided, **tokens that fall outside the range covered by a given matrix are masked out**. Then, **the vectors corresponding to the masked positions are initialized to all zeros**, and finally, the two matrices are **summed together**. This results in a complete input tensor that contains the correct embedding vectors for all words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b860a3-60ea-441b-b8e7-964cc9878c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Reference: VocabParallelEmbedding in megatron-lm/megatron/mpu/layers.py\n",
    "\"\"\"\n",
    "\n",
    "def forward(self, input_):\n",
    "    if self.tensor_model_parallel_size > 1:\n",
    "        # Build the mask.\n",
    "        input_mask = (input_ < self.vocab_start_index) | \\\n",
    "                     (input_ >= self.vocab_end_index)\n",
    "\n",
    "        # Mask the input.\n",
    "        masked_input = input_.clone() - self.vocab_start_index\n",
    "        masked_input[input_mask] = 0\n",
    "\n",
    "    else:\n",
    "        masked_input = input_\n",
    "        # Get the embeddings.\n",
    "    \n",
    "    output_parallel = F.embedding(masked_input, self.weight,\n",
    "                                  self.padding_idx, self.max_norm,\n",
    "                                  self.norm_type, self.scale_grad_by_freq,\n",
    "                                  self.sparse)\n",
    "\n",
    "    # Mask the output embedding.\n",
    "    if self.tensor_model_parallel_size > 1:\n",
    "        output_parallel[input_mask, :] = 0.0\n",
    "    \n",
    "    # Reduce across all the model parallel GPUs.\n",
    "    output = reduce_from_tensor_model_parallel_region(output_parallel)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a258efed-7992-46f2-993c-9c51acf7e1a7",
   "metadata": {},
   "source": [
    "However, a problem arises here. Tensor parallelism requires the vocabulary size to be divisible by the number of GPUs used for parallelization. Since 52,527 is not an even number, it cannot be evenly split across two GPUs. To address this, unused tokens are added to the word embedding matrix to make the vocabulary size divisible. This adjusted size is called the **padded vocab size**.\n",
    "\n",
    "In Megatron-LM, the vocabulary size can be adjusted using the `make-vocab-size-divisible-by` argument, which ensures that the vocabulary size becomes a multiple of the specified value.\n",
    "\n",
    "In conclusion, by applying **vocab parallel embedding**, Megatron-LM can further improve memory efficiency.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945d4f38-a1dc-4734-8b93-1b7126e43103",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Vocab Parallel Cross Entropy\n",
    "\n",
    "Tasks such as **causal language modeling** in GPT-2 or **masked language modeling** in BERT generate natural language tokens as the final output. Therefore, after passing through the final Transformer layer, the model’s output shape expands to `(batch_size, sequence_length, vocab_size)`. (This does not apply to tasks such as classification or tagging.)\n",
    "\n",
    "<br>\n",
    "\n",
    "![](../images/lm_head.png)\n",
    "\n",
    "<br>\n",
    "\n",
    "In this case, if **input and output embeddings are tied** (weight tying), the word embedding matrix is reused instead of initializing new parameters for the Linear layer used in the Language Modeling Head (hereafter referred to as the **LM Head**). In most publicly released models such as BERT, GPT-2, and GPT-Neo, the output embeddings (LM Head) are tied to the input embeddings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9fad8d-65f3-45ca-9acd-4987593062ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Reference: transformers/models/gpt_neo/modeling_gpt_neo.py\n",
    "\"\"\"\n",
    "\n",
    "class GPTNeoForCausalLM(GPTNeoPreTrainedModel):\n",
    "    _keys_to_ignore_on_load_missing = [\n",
    "        r\"h\\.\\d+\\.attn\\.masked_bias\",\n",
    "        r\"lm_head\\.weight\",\n",
    "        r\"h\\.\\d+\\.attn\\.attention\\.bias\",\n",
    "    ]\n",
    "    _keys_to_ignore_on_save = [r\"lm_head.weight\"]\n",
    "    # 3. Therefore, the `lm_head.weight` parameter is not loaded or saved.\n",
    "    # There is no need to store or load the same tensor twice.\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.transformer = GPTNeoModel(config)\n",
    "        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
    "        # 1. At first glance, it appears as if the nn.Linear layer parameters\n",
    "        #    are newly allocated and used.\n",
    "\n",
    "        self.init_weights()\n",
    "        # 2. However, when this method is called, the input and output embeddings\n",
    "        #    (lm head) are tied.\n",
    "        #    At this point, the word embedding matrix weights are copied to the\n",
    "        #    nn.Linear layer’s weights.\n",
    "        #    This copy is not a deep copy, but a shallow copy\n",
    "        #    (the values are shared, not just the reference).\n",
    "        #    Therefore, `lm_head.weight` is a single tensor that resides in the\n",
    "        #    same memory address space as the word embedding.\n",
    "\n",
    "    def get_output_embeddings(self):\n",
    "        return self.lm_head\n",
    "\n",
    "    def set_output_embeddings(self, new_embeddings):\n",
    "        self.lm_head = new_embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d3d52c-3487-416f-9584-3c237d5c1fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Reference: transformers/modeling_utils.py\n",
    "\"\"\"\n",
    "\n",
    "def init_weights(self):\n",
    "    \"\"\"\n",
    "    If needed prunes and maybe initializes weights.\n",
    "    \"\"\"\n",
    "    # Prune heads if needed\n",
    "    if self.config.pruned_heads:\n",
    "        self.prune_heads(self.config.pruned_heads)\n",
    "\n",
    "    if _init_weights:\n",
    "        # Initialize weights\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "        # For models that support weight tying, calling this method\n",
    "        # automatically ties the input embeddings and the output embeddings\n",
    "        # (= lm head).\n",
    "        self.tie_weights()\n",
    "\n",
    "\n",
    "def tie_weights(self):\n",
    "    \"\"\"\n",
    "    Tie the weights between the input embeddings and the output embeddings.\n",
    "    If the :obj:`torchscript` flag is set in the configuration, can't handle parameter sharing so we are cloning\n",
    "    the weights instead.\n",
    "    \"\"\"\n",
    "    output_embeddings = self.get_output_embeddings()\n",
    "    if output_embeddings is not None and self.config.tie_word_embeddings:\n",
    "        self._tie_or_clone_weights(output_embeddings, self.get_input_embeddings())\n",
    "        # When this method is called, the output embeddings (lm head)\n",
    "        # are tied to the input embeddings.\n",
    "\n",
    "    if self.config.is_encoder_decoder and self.config.tie_encoder_decoder:\n",
    "        if hasattr(self, self.base_model_prefix):\n",
    "            self = getattr(self, self.base_model_prefix)\n",
    "        self._tie_encoder_decoder_weights(self.encoder, self.decoder, self.base_model_prefix)\n",
    "\n",
    "    for module in self.modules():\n",
    "        if hasattr(module, \"_tie_weights\"):\n",
    "            module._tie_weights()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893a5d15-0a71-4d13-985a-9e5175adc846",
   "metadata": {},
   "source": [
    "However, a problem arises here. In general, when computing the loss between the logits produced by the LM Head and the target data, the following process takes place.\n",
    "\n",
    "<br>\n",
    "\n",
    "![](../images/vpce_1.png)\n",
    "\n",
    "<br>\n",
    "\n",
    "However, since Megatron-LM uses **vocab parallel embedding**, the embedding layer is split across multiple devices. As a result, when weight tying is applied, the **output embeddings (LM Head) are also split across multiple devices**. Consequently, the size of the logits produced by the model corresponds to a partitioned vocabulary size.\n",
    "\n",
    "<br>\n",
    "\n",
    "![](../images/vpce_2.png)\n",
    "\n",
    "<br>\n",
    "\n",
    "As shown in the figure above, if the vocabulary size is 50,000, the output logits would normally have the shape `(batch_size, length, 50,000)`. But if the vocabulary is split across two devices, each device produces logits of shape `(batch_size, length, 25,000)`. The logits on each device contain different values. **These are referred to as Parallel LM Logits.**  \n",
    "\n",
    "In this situation, how should we compute the loss against the target sentence? The target data contains token indices from 0 to 49,999, whereas each device’s logits cover only half of that range.\n",
    "\n",
    "<br>\n",
    "\n",
    "![](../images/vpce_3.png)\n",
    "\n",
    "<br>\n",
    "\n",
    "In this case, we must use a special loss function called **vocab parallel cross entropy**, rather than the standard cross entropy. The computation of vocab parallel cross entropy proceeds as illustrated above. From the computed logits, only the portion of the vocabulary covered by the current device is retained, and the rest is masked out when computing the loss. Then, the losses computed on each device are **All-reduced** to obtain the final loss value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5db76b-730b-4a4d-8988-d079523a40b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Reference: _VocabParallelCrossEntropy in megatron-lm/megatron/mpu/cross_entropy.py\n",
    "\"\"\"\n",
    "\n",
    "@staticmethod\n",
    "def forward(ctx, vocab_parallel_logits, target):\n",
    "    # Compute the maximum value along the vocab dimension across all GPUs.\n",
    "    logits_max = torch.max(vocab_parallel_logits, dim=-1)[0]\n",
    "    torch.distributed.all_reduce(logits_max,\n",
    "                                 op=torch.distributed.ReduceOp.MAX,\n",
    "                                 group=get_tensor_model_parallel_group())\n",
    "\n",
    "    # Subtract the maximum value.\n",
    "    vocab_parallel_logits.sub_(logits_max.unsqueeze(dim=-1))\n",
    "\n",
    "    # Get the partitioned vocab indices.\n",
    "    get_vocab_range = VocabUtility.vocab_range_from_per_partition_vocab_size\n",
    "    partition_vocab_size = vocab_parallel_logits.size()[-1]\n",
    "    rank = get_tensor_model_parallel_rank()\n",
    "    world_size = get_tensor_model_parallel_world_size()\n",
    "    vocab_start_index, vocab_end_index = get_vocab_range(\n",
    "        partition_vocab_size, rank, world_size)\n",
    "\n",
    "    # Create a mask for valid vocab IDs (1 means the token should be masked).\n",
    "    target_mask = (target < vocab_start_index) | (target >= vocab_end_index)\n",
    "    masked_target = target.clone() - vocab_start_index\n",
    "    masked_target[target_mask] = 0\n",
    "\n",
    "    # Extract predicted logits = logits[target].\n",
    "    # For simplicity, convert logits into a 2-D tensor of shape\n",
    "    # [*, partition_vocab_size] and target into a 1-D tensor of shape [*].\n",
    "    logits_2d = vocab_parallel_logits.view(-1, partition_vocab_size)\n",
    "    masked_target_1d = masked_target.view(-1)\n",
    "    arange_1d = torch.arange(start=0, end=logits_2d.size()[0],\n",
    "                                 device=logits_2d.device)\n",
    "    predicted_logits_1d = logits_2d[arange_1d, masked_target_1d]\n",
    "    predicted_logits_1d = predicted_logits_1d.clone().contiguous()\n",
    "    predicted_logits = predicted_logits_1d.view_as(target)\n",
    "    predicted_logits[target_mask] = 0.0\n",
    "    \n",
    "    # All-reduce is required to collect chunks from other GPUs.\n",
    "    torch.distributed.all_reduce(predicted_logits,\n",
    "                                 op=torch.distributed.ReduceOp.SUM,\n",
    "                                 group=get_tensor_model_parallel_group())\n",
    "\n",
    "    # Compute the sum of exp(logits) along the vocab dimension across all GPUs.\n",
    "    exp_logits = vocab_parallel_logits\n",
    "    torch.exp(vocab_parallel_logits, out=exp_logits)\n",
    "    sum_exp_logits = exp_logits.sum(dim=-1)\n",
    "    torch.distributed.all_reduce(sum_exp_logits,\n",
    "                                 op=torch.distributed.ReduceOp.SUM,\n",
    "                                 group=get_tensor_model_parallel_group())\n",
    "\n",
    "    # Loss = log(sum(exp(logits))) - predicted_logit.\n",
    "    loss = torch.log(sum_exp_logits) - predicted_logits\n",
    "\n",
    "    # Store softmax, target mask, and masked target for the backward pass.\n",
    "    exp_logits.div_(sum_exp_logits.unsqueeze(dim=-1))\n",
    "    ctx.save_for_backward(exp_logits, target_mask, masked_target_1d)\n",
    "\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66cff4d1-a7c7-473c-a023-6435226d30a6",
   "metadata": {},
   "source": [
    "### Training a Model with Megatron-LM\n",
    "\n",
    "Let’s try training a model using Megatron-LM. Unlike Hugging Face’s `transformers`, Megatron-LM is not a framework that is typically used at the code level. Instead, it provides a well-structured codebase that is used to build and train models. Therefore, we will proceed by first cloning the repository.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad62291b-8a70-4de3-ab70-42580f095600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If git and wget are not installed, install them using the following command.\n",
    "!apt update && apt install git wget -y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d97430f-f46b-41fc-9878-d28d0a6c017f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the Megatron-LM repository.\n",
    "!git clone https://github.com/NVIDIA/Megatron-LM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13dd6073-4f71-4972-9923-b51fa454c189",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd Megatron-LM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a07ad8d-1410-4c70-ad68-e44bb1b71d16",
   "metadata": {},
   "source": [
    "Next, let’s install a few required packages. Megatron-LM includes functionality that uses `nltk` to split data into sentences during preprocessing. Although we won’t be using this feature here, an error will occur if `nltk` is not installed, so we will install it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9093dffb-800f-4a1f-b8c6-4de15a774ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0afb6b-aec9-4476-80b9-c4d2f78b9d38",
   "metadata": {},
   "source": [
    "Megatron-LM also uses the `pybind11` and `apex` packages. Let’s install them as well.  \n",
    "(The CUDA compilation can take quite a while, so please be patient.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094278f9-d3ea-4960-8a80-f6997ee0d7d8",
   "metadata": {},
   "source": [
    "Megatron-LM also uses the `pybind11` and `apex` packages. We will install them as well.  \n",
    "(The CUDA compilation takes quite a long time, so please be patient.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26317448-90c4-496b-8269-b534cf9682be",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pybind11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef9453c-1af8-4b09-900e-baa290bcd8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/NVIDIA/apex\n",
    "%cd apex\n",
    "!pip install -v --disable-pip-version-check --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" ./\n",
    "%cd "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44bf95d2-d421-4df3-acb6-45ac39f71ab9",
   "metadata": {},
   "source": [
    "Next, let’s create the dataset. When pre-training a model with Megatron-LM, you only need to create a simple `jsonl` file where each line follows a structure like `{\"text\": \"sample text\"}`. For fine-tuning, however, the dataset must be constructed according to the specific task.\n",
    "\n",
    "Since this tutorial focuses only on **pre-training**, we will not cover fine-tuning here. If you need fine-tuning, please refer to the Megatron-LM GitHub repository.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f96db81-4d6d-4128-bd4f-0910f3a85675",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "src/megatron_datasets.py\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import os\n",
    "from datasets import load_dataset\n",
    "\n",
    "train_samples, min_length = 10000, 512\n",
    "filename = \"megatron_datasets.jsonl\"\n",
    "curr_num_datasets = 0\n",
    "\n",
    "if os.path.exists(filename):\n",
    "    os.remove(filename)\n",
    "\n",
    "datasets = load_dataset(\"wikitext\", \"wikitext-103-raw-v1\")\n",
    "datasets = datasets.data[\"train\"][\"text\"]\n",
    "dataset_fp_write = open(filename, mode=\"w\", encoding=\"utf-8\")\n",
    "\n",
    "for sample in datasets:\n",
    "    sample = sample.as_py()\n",
    "\n",
    "    if len(sample) >= min_length:\n",
    "        line = json.dumps(\n",
    "            {\"text\": sample},\n",
    "            ensure_ascii=False,\n",
    "        )\n",
    "\n",
    "        dataset_fp_write.write(line + \"\\n\")\n",
    "        curr_num_datasets += 1\n",
    "\n",
    "        # Since this is a tutorial, we will create only a small amount of data.\n",
    "        if curr_num_datasets >= train_samples:\n",
    "            break\n",
    "\n",
    "dataset_fp_read = open(filename, mode=\"r\", encoding=\"utf-8\")\n",
    "dataset_read = dataset_fp_read.read().splitlines()[:3]\n",
    "\n",
    "# Check the structure of the data.\n",
    "for sample in dataset_read:\n",
    "    print(sample, end=\"\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a32455-75a7-4fc5-af79-76321733e66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd Large-scale-lm-eng-tutorial/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab48ef1-2407-4c54-980d-22f6f6c8e494",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python src/ch6_tensor_parallelism/megatron_datasets.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e960b6ec-bc41-4e78-845c-2a963e77c1e0",
   "metadata": {},
   "source": [
    "Download the vocabulary to be used for tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb43c21-d556-4d07-8a61-872ba4949cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://huggingface.co/gpt2/raw/main/vocab.json\n",
    "!wget https://huggingface.co/gpt2/raw/main/merges.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e2f135-aa0a-45a8-b567-2270ac16c606",
   "metadata": {},
   "outputs": [],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6ae1c3-51bc-4bb5-8fc3-bce5e9e42f9c",
   "metadata": {},
   "source": [
    "Now we preprocess the dataset. The preprocessing performed here includes both tokenization and binarization. Megatron-LM’s preprocessing code is copied from Fairseq’s indexed dataset implementation. Fairseq provides three main preprocessing modes—lazy, cached, and mmap. Before proceeding, we will briefly explain these preprocessing methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e94562-7a24-4ec7-b8fb-f681800ba8ab",
   "metadata": {},
   "source": [
    "<b>1) Lazy\n",
    "\n",
    "The lazy mode loads the required data from disk into memory at every step. In other words, whenever the __getitem__() method of the Dataset class is called, the data is loaded into memory by accessing the specified file path. However, because disk I/O is performed through the file buffer at every step, this approach can be relatively slow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c084a4-56da-49f3-ab0e-d1efd8e4c9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Reference: fairseq/fairseq/data/indexed_dataset.py\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "from typing import Union\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def __getitem__(self, idx: Union[int, slice]) -> np.ndarray:\n",
    "    if not self.data_file:\n",
    "        # Load file buffer\n",
    "        self.read_data(self.path)\n",
    "\n",
    "    if isinstance(idx, int):\n",
    "        # Validate index\n",
    "        self.check_index(idx)\n",
    "\n",
    "        # Compute the size of the tensor to load\n",
    "        tensor_size = self.sizes[self.dim_offsets[idx] : self.dim_offsets[idx + 1]]\n",
    "\n",
    "        # Allocate empty memory space to store the tensor\n",
    "        array = np.empty(tensor_size, dtype=self.dtype)\n",
    "\n",
    "        # Set the disk address to read from based on the offset\n",
    "        self.data_file.seek(self.data_offsets[idx] * self.element_size)\n",
    "\n",
    "        # Load data from disk into memory (file I/O)\n",
    "        self.data_file.readinto(array)\n",
    "        return array\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef8e171-fc61-42b2-99b8-d4a3f2037f8f",
   "metadata": {},
   "source": [
    "#### 2) Cached\n",
    "`cached` prefetches all data before training and keeps it in memory. Since disk access is not required for data loading during training, it is generally faster than the other methods. However, because memory capacity is limited, it is difficult to use this method when the dataset size is very large.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23026a70-c220-4675-8c26-fb651ef9995b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Reference: fairseq/fairseq/data/indexed_dataset.py\n",
    "Comments were added manually by me.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "from typing import List\n",
    "\n",
    "\n",
    "def prefetch(self, indices: List[int]) -> None:\n",
    "    if all(i in self.cache_index for i in indices):\n",
    "        # If all data has already been cached, exit the method\n",
    "        return\n",
    "\n",
    "    if not self.data_file:\n",
    "        # If the file buffer is not loaded, load the file buffer\n",
    "        self.read_data(self.path)\n",
    "\n",
    "    # Sort indices to calculate the total contiguous memory size\n",
    "    indices = sorted(set(indices))\n",
    "\n",
    "    total_size = 0\n",
    "    for i in indices:\n",
    "        total_size += self.data_offsets[i + 1] - self.data_offsets[i]\n",
    "\n",
    "    # Allocate the total memory space to be used as cache\n",
    "    self.cache = np.empty(\n",
    "        total_size,\n",
    "        dtype=self.dtype,\n",
    "    )\n",
    "\n",
    "    self.cache_index.clear()\n",
    "    ptx = 0\n",
    "\n",
    "    for i in indices:\n",
    "        # Store the starting position of each array\n",
    "        self.cache_index[i] = ptx\n",
    "\n",
    "        # Compute the data size from offsets and assign memory space for the current sample\n",
    "        size = self.data_offsets[i + 1] - self.data_offsets[i]\n",
    "        array = self.cache[ptx : ptx + size]\n",
    "\n",
    "        # Set the disk address to read from based on the offset\n",
    "        self.data_file.seek(self.data_offsets[i] * self.element_size)\n",
    "\n",
    "        # Write the current sample into the allocated memory\n",
    "        self.data_file.readinto(array)\n",
    "        ptx += size\n",
    "\n",
    "    if self.data_file:\n",
    "        # All data has been loaded from the file buffer, so close the buffer and release the reference\n",
    "        self.data_file.close()\n",
    "        self.data_file = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478311c9-71d8-4800-a4a8-f7785e66e479",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Reference: fairseq/fairseq/data/indexed_dataset.py\n",
    "Comments were added manually by me.\n",
    "\"\"\"\n",
    "\n",
    "def __getitem__(self, idx: Union[int, tuple]) -> Union[np.ndarray, List]:\n",
    "    if isinstance(idx, int):\n",
    "        # Validate index\n",
    "        self.check_index(idx)\n",
    "\n",
    "        # Compute tensor size\n",
    "        tensor_size = self.sizes[self.dim_offsets[idx] : self.dim_offsets[idx + 1]]\n",
    "\n",
    "        # Allocate memory space\n",
    "        array = np.empty(tensor_size, dtype=self.dtype)\n",
    "\n",
    "        # Load prefetched data (no file I/O occurs)\n",
    "        ptx = self.cache_index[idx]\n",
    "\n",
    "        # Copy prefetched data from cache into the allocated memory space\n",
    "        np.copyto(array, self.cache[ptx : ptx + array.size])\n",
    "        return array\n",
    "\n",
    "    elif isinstance(idx, slice):\n",
    "        return [self[i] for i in range(*idx.indices(len(self)))]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1836507e-4011-4ef7-a3d0-787f12a79592",
   "metadata": {},
   "source": [
    "#### 3) Mmap\n",
    "`mmap`, like `lazy`, loads only the required amount of data into memory at each step, but it uses a Memory Map instead of a File Buffer. Unlike a File Buffer, a Memory Map maps the file address into the virtual memory allocated to the current process, allowing the data to be accessed as if it already resides in memory. It does not perform direct disk I/O, loads data in page-sized units (4KB), and since all operations effectively occur in memory, it generally provides faster processing speed compared to a File Buffer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b351048-342e-4f8f-bdf0-cb46bbe9697f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Reference: fairseq/fairseq/data/indexed_dataset.py\n",
    "Comments were added manually by me.\n",
    "\"\"\"\n",
    "\n",
    "def __init__(self, path: str):\n",
    "    with open(path, \"rb\") as stream:\n",
    "        # 1. Load magic string\n",
    "        # The magic string is used to identify the format of the stored data structure.\n",
    "        # Whether it is lazy, mmap, etc... (cached uses the same value as lazy)\n",
    "        magic_test = stream.read(9)\n",
    "        assert magic_test == self._HDR_MAGIC, (\n",
    "            \"Index file doesn't match expected format. \"\n",
    "            \"Please check your configuration file.\"\n",
    "        )\n",
    "        \n",
    "        # 2. Load version (little endian unsigned long long)\n",
    "        # Looking at the code, the version is always written as 1, so this seems to be a variable with little practical meaning.\n",
    "        # b'\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00'\n",
    "        version = struct.unpack(\"<Q\", stream.read(8))\n",
    "        assert (1,) == version\n",
    "\n",
    "        # 3. Load data type (little endian unsigned char)\n",
    "        (dtype_code,) = struct.unpack(\"<B\", stream.read(1))\n",
    "        self._dtype = _code_to_dtype[dtype_code]\n",
    "        self._dtype_size = self._dtype().itemsize\n",
    "\n",
    "        # 4. Load total dataset length (little endian unsigned long long)\n",
    "        self._len = struct.unpack(\"<Q\", stream.read(8))[0]\n",
    "\n",
    "        # 5. Load total number of samples (little endian unsigned long long)\n",
    "        self._doc_count = struct.unpack(\"<Q\", stream.read(8))[0]\n",
    "        offset = stream.tell()\n",
    "\n",
    "    # 6. Perform cache warmup\n",
    "    _warmup_mmap_file(path)\n",
    "\n",
    "    # 7. Create memory-mapped array\n",
    "    self._bin_buffer_mmap = np.memmap(path, mode=\"r\", order=\"C\")\n",
    "    self._bin_buffer = memoryview(self._bin_buffer_mmap)\n",
    "\n",
    "    # 8. Load sample sizes into a memory-mapped array\n",
    "    self._sizes = np.frombuffer(\n",
    "        self._bin_buffer, dtype=np.int32, count=self._len, offset=offset\n",
    "    )\n",
    "\n",
    "    # 9. Load data pointers (offsets) into a memory-mapped array\n",
    "    self._pointers = np.frombuffer(\n",
    "        self._bin_buffer,\n",
    "        dtype=np.int64,\n",
    "        count=self._len,\n",
    "        offset=offset + self._sizes.nbytes,\n",
    "    )\n",
    "\n",
    "    # 10. Load document indices into a memory-mapped array\n",
    "    self._doc_idx = np.frombuffer(\n",
    "        self._bin_buffer,\n",
    "        dtype=np.int64,\n",
    "        count=self._doc_count,\n",
    "        offset=offset + self._sizes.nbytes + self._pointers.nbytes,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b157a2-4d5b-4782-911e-bff62b4a44ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Reference: fairseq/fairseq/data/indexed_dataset.py\n",
    "Comments were added manually by me.\n",
    "\"\"\"\n",
    "\n",
    "from typing import Union\n",
    "import numpy as np\n",
    "\n",
    "def __getitem__(self, idx: Union[int, slice]) -> np.ndarray:\n",
    "    if not self.data_file:\n",
    "        # If the index file is not loaded, load it\n",
    "        self.read_data(self.path)\n",
    "\n",
    "    if isinstance(idx, int):\n",
    "        # Validate index\n",
    "        self.check_index(idx)\n",
    "\n",
    "        # Compute tensor size\n",
    "        tensor_size = self.sizes[self.dim_offsets[idx] : self.dim_offsets[idx + 1]]\n",
    "\n",
    "        # Allocate memory space\n",
    "        array = np.empty(tensor_size, dtype=self.dtype)\n",
    "\n",
    "        # Set the virtual memory address of the data to read based on the offset\n",
    "        self.data_file.seek(self.data_offsets[idx] * self.element_size)\n",
    "\n",
    "        # Load data into memory\n",
    "        self.data_file.readinto(array)\n",
    "        return array\n",
    "\n",
    "    elif isinstance(idx, slice):\n",
    "        start, stop, step = idx.indices(len(self))\n",
    "        if step != 1:\n",
    "            # When using slicing, it must be contiguous\n",
    "            raise ValueError(\"Slices into indexed_dataset must be contiguous\")\n",
    "\n",
    "        # Compute the list of tensor sizes and their total sum\n",
    "        sizes = self.sizes[self.dim_offsets[start] : self.dim_offsets[stop]]\n",
    "        total_size = sum(sizes)\n",
    "\n",
    "        # Allocate the required amount of memory\n",
    "        array = np.empty(total_size, dtype=self.dtype)\n",
    "\n",
    "        # Set the virtual memory address of the data to read based on the offset\n",
    "        self.data_file.seek(self.data_offsets[start] * self.element_size)\n",
    "        self.data_file.readinto(array)\n",
    "\n",
    "        # Split into multiple samples based on tensor sizes\n",
    "        offsets = list(accumulate(sizes))\n",
    "        sentences = np.split(array, offsets[:-1])\n",
    "        return sentences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c849b5-bc01-4d65-9992-c225b68dd513",
   "metadata": {},
   "source": [
    "Now we perform dataset preprocessing. I will use the `mmap` method for preprocessing. \n",
    "\n",
    "At this point, you may notice an option called `append-eod`. Megatron-LM concatenates all data during pre-training in order to avoid creating padding. For example, if there are samples such as `{\"text\": \"I am a boy.\"}` and `{\"text\": \"You are so lucky\"}`, during pre-training all samples are concatenated like `input = \"I am a boy. You are so lucky ...\"` and then the data is split according to the user-defined length (e.g. 2048) for training. \n",
    "\n",
    "However, if all samples are concatenated into a single string, the boundaries between samples can be lost, which may cause issues. When the `append-eod` option is enabled, an `end of document` token is inserted between samples to distinguish them. In the case of GPT2, the `eod` token is set to the `eos` token.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e274e43-ffdf-439c-b7d8-66afc7a35499",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python tools/preprocess_data.py \\\n",
    "       --input megatron_datasets.jsonl \\\n",
    "       --output-prefix my-gpt2 \\\n",
    "       --vocab vocab.json \\\n",
    "       --dataset-impl mmap \\\n",
    "       --tokenizer-type GPT2BPETokenizer \\\n",
    "       --merge-file merges.txt \\\n",
    "       --append-eod"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81c4828-76c1-403a-b80b-2af17fa83bd2",
   "metadata": {},
   "source": [
    "Dataset preprocessing has been completed. Let’s take a look at the data.\n",
    "\n",
    "- my-gpt2_text_document.bin\n",
    "- my-gpt2_text_document.idx\n",
    "\n",
    "Files like these have been created. The `idx` file stores metadata such as data offsets and locations, while the `bin` file contains the actual tokenized data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc10bcf8-26df-434d-b9cb-89cfb6674447",
   "metadata": {},
   "outputs": [],
   "source": [
    "%ls\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c450a1-1fe1-4051-9d08-708098829ce2",
   "metadata": {},
   "source": [
    "Now we will start model training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb129a51-6ea7-4320-987d-3d7e1a2021fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For now, we will use only Tensor parallelism.\n",
    "# Data parallelism and Pipeline parallelism will be used in the Multi-dimensional Parallelism session. :)\n",
    "# We will train for only 1000 steps. Please set a larger number for actual training.\n",
    "\n",
    "\n",
    "!python -m torch.distributed.launch \\\n",
    "                  --nproc_per_node \"4\" \\\n",
    "                  --nnodes \"1\" \\\n",
    "                  --node_rank \"0\" \\\n",
    "                  --master_addr \"localhost\" \\\n",
    "                  --master_port \"6000\" \\\n",
    "                  ./pretrain_gpt.py \\\n",
    "                  --num-layers \"24\" \\\n",
    "                  --hidden-size \"1024\" \\\n",
    "                  --num-attention-heads \"16\" \\\n",
    "                  --seq-length \"1024\" \\\n",
    "                  --max-position-embeddings \"1024\" \\\n",
    "                  --micro-batch-size \"4\" \\\n",
    "                  --global-batch-size \"8\" \\\n",
    "                  --lr \"0.00015\" \\\n",
    "                  --train-iters \"1000\" \\\n",
    "                  --lr-decay-iters \"300\" \\\n",
    "                  --lr-decay-style cosine \\\n",
    "                  --vocab-file \"vocab.json\" \\\n",
    "                  --merge-file \"merges.txt\" \\\n",
    "                  --lr-warmup-fraction \".01\" \\\n",
    "                  --fp16 \\\n",
    "                  --log-interval \"10\" \\\n",
    "                  --save-interval \"500\" \\\n",
    "                  --eval-interval \"100\" \\\n",
    "                  --eval-iters 10 \\\n",
    "                  --activations-checkpoint-method \"uniform\" \\\n",
    "                  --save \"checkpoints/gpt2_345m\" \\\n",
    "                  --load \"checkpoints/gpt2_345m\" \\\n",
    "                  --data-path \"my-gpt2_text_document\" \\\n",
    "                  --tensor-model-parallel-size \"4\" \\\n",
    "                  --pipeline-model-parallel-size \"1\" \\\n",
    "                  --DDP-impl \"torch\"\n",
    "\n",
    "# Megatron-LM provides many more options in addition to the ones set above.\n",
    "# It is difficult to explain all options, so please refer to the link below.\n",
    "# https://github.com/NVIDIA/Megatron-LM/blob/main/megatron/arguments.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a15e980-0e5f-40cc-82aa-a3a57a1d6cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd Large-scale-lm-eng-tutorial/notebooks/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea81470-fa23-4b71-b6b6-0bdbae6546da",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 3. Parallelformers\n",
    "\n",
    "<img src=\"../images/parallelformers.png\" width=360>\n",
    "\n",
    "So far, we have trained models using Megatron-LM. Although Megatron-LM provides excellent Tensor Parallelism capabilities, it could not parallelize models that were trained using the Hugging Face `transformers` library, which we commonly use. To address this limitation, TUNiB released an open-source project called `parallelformers` in 2021. `parallelformers` is a tool that allows Tensor Parallelism to be applied to inference for most models trained with Hugging Face `transformers` using just one or two lines of code.\n",
    "\n",
    "Let’s install `parallelformers`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72de6efe-dd8a-4f7d-b8e8-069176dd5bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install parallelformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4edc312-d31f-428f-9afd-920a1bc961d4",
   "metadata": {},
   "source": [
    "`parallelformers` can parallelize an existing model using the `parallelize` function as shown in the code below, and it also provides several additional options such as `num_gpus` and `fp16`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27224514-d8a7-4670-a60c-c3bba6645859",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from parallelformers import parallelize\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/gpt-neo-2.7B\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neo-2.7B\")\n",
    "    parallelize(model, num_gpus=4, fp16=True, verbose=\"simple\")\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        \"Parallelformers is\",\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        num_beams=5,\n",
    "        no_repeat_ngram_size=4,\n",
    "        max_length=15,\n",
    "    )\n",
    "\n",
    "    print(f\"\\nOutput: {tokenizer.batch_decode(outputs)[0]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f98519-f38f-4220-8ebf-61c12b09b3ea",
   "metadata": {},
   "source": [
    "Note: `parallelformers` uses shared memory for inter-process data communication. Therefore, **when using it in environments that allow only limited resources, such as docker, you must increase the shared memory size.**\n",
    "\n",
    "You can increase the shared memory size using the `docker run ... --shm_size=?gb` option, or remove the shared memory limitation using the `docker run ... --ipc=host` option. It has been confirmed that almost all issues occurring in docker are caused by shared memory limitations, and using larger models requires allocating a larger amount of shared memory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec086a5e-9ca4-4670-95c9-a01a6c1fd9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ../src/parallelformers_inference.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8515b7f-80f7-4989-b0bf-e368ea9e0add",
   "metadata": {},
   "source": [
    "### How Parallelformers Works\n",
    "\n",
    "<br>\n",
    "\n",
    "![](../images/tensor_replace.png)\n",
    "\n",
    "<br>\n",
    "\n",
    "Then how is `parallelformers` able to perform Tensor parallelism without modifying the model code? The answer lies in the `Tensor Replacement` mechanism. `parallelformers` first extracts all parameters from the original model, then splits the tensors in the same way as Megatron-LM, and replaces the original parameters in the model with the partitioned tensors. By doing so, it can achieve parallelization without changing the model structure. Using this approach, approximately 70 different models were able to be parallelized. Although several additional mechanisms were introduced, they are not directly related to tensor parallelism and are therefore omitted here. If you would like more detailed information, please refer to the following links.\n",
    "\n",
    "- Korean: https://tunib.notion.site/TECH-2021-07-26-Parallelformers-_-0dcceeaddc5247429745ba36c6549fe5\n",
    "- English: https://tunib.notion.site/TECH-2021-07-26-Parallelformers-Journey-to-deploying-big-models_TUNiB-32b19a599c38497abaad2a98727f6dc8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a96187b-0de9-46ee-a40c-f838934e1394",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
