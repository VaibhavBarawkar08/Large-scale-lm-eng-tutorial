{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dce4e59e-c562-4a99-85ac-4ee6ff372c23",
   "metadata": {},
   "source": [
    "# Zero Redundancy Optimization (ZeRO)\n",
    "\n",
    "In this session, we will learn about ZeRO, Microsoft’s neural network training optimization solution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c478505-9655-4fcb-bf6a-c56b5b0645d5",
   "metadata": {},
   "source": [
    "## 1. Mixed Precision\n",
    "\n",
    "As modern GPUs support computation with lower precision, most modern neural network training uses a mixed precision approach that combines FP16 (half) and FP32 (single). On a V100 GPU, while FP32 delivers around 14 TFLOPS, FP16 can achieve training speeds of up to 100 TFLOPS. In addition, using FP16 reduces the model size, which provides advantages not only during training but also during deployment.\n",
    "\n",
    "<br>\n",
    "\n",
    "![](../images/mixed_precision_1.png)\n",
    "\n",
    "<br>\n",
    "\n",
    "### But why Mixed?\n",
    "This raises a question: why not train the model using only FP16? Why is it necessary to use both FP32 and FP16 together? In short, training with only FP16 causes the loss to diverge severely, making training almost impossible. If gradients are kept in FP16, most fractional values are truncated, which prevents precise learning. Therefore, the goal is to combine the high speed of FP16 with the high accuracy of FP32 to take advantage of both approaches.\n",
    "\n",
    "![](../images/ddp_analysis_3.png)\n",
    "\n",
    "The computationally expensive Forward and Backward passes are performed using the FP16 model, and the computed gradients are copied to the higher-precision FP32 model to update the weights. This leads to another question: how should FP16 gradients be applied to FP32? Experimental results showed that when backpropagating a loss computed in FP16, some values with very small magnitudes (shown on the left in the figure) became zero during computation.\n",
    "\n",
    "![](../images/mixed_precision_4.png)\n",
    "\n",
    "<br>\n",
    "\n",
    "### Loss Scaling\n",
    "How can this problem be resolved? The idea is very simple: multiply the loss gradients by a very large value to shift their distribution to the right. This technique is called loss scaling. By multiplying the FP16 loss by a large value, values that might otherwise disappear when applied to FP32 can be preserved.\n",
    "\n",
    "![](../images/mixed_precision_5.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e53754b-c5ac-4bb5-9a45-fc404dda9cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Reference: apex/apex/amp/opt.py\n",
    "\"\"\"\n",
    "\n",
    "import contextlib\n",
    "\n",
    "@contextlib.contextmanager\n",
    "def scale_loss(self, loss):\n",
    "    if not self._amp_handle.is_active():\n",
    "        yield loss\n",
    "        return\n",
    "\n",
    "    # When there are multiple losses per-optimizer, we need\n",
    "    # to save out current grad accumulation, since we won't be\n",
    "    # able to unscale this particulare loss once the grads are\n",
    "    # all mixed together.\n",
    "    cached_grads = []\n",
    "    if self._loss_idx > 0:\n",
    "        for p in master_params(self._optimizer):\n",
    "            if p.grad is not None:\n",
    "                cached_grads.append(p.grad.data.detach().clone())\n",
    "            else:\n",
    "                cached_grads.append(None)\n",
    "        self._optimizer.zero_grad()\n",
    "\n",
    "    loss_scale = self._cur_loss_scaler().loss_scale()\n",
    "    yield loss * loss_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c51063-bb2b-456a-bd5f-ffa0fa8e104f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Reference: apex/tests/L0/run_amp/test_fused_sgd.py\n",
    "\"\"\"\n",
    "\n",
    "with amp.scale_loss(loss0, optimizer, loss_id=loss_ids[0]) as scaled_loss:\n",
    "    scaled_loss.backward()\n",
    "    if i == inject_inf and which_backward == 0:\n",
    "        if inject_inf_loc == \"fp32\":\n",
    "            model0.weight0.grad[0] = float('inf')\n",
    "        elif inject_inf_loc == \"fp16\":\n",
    "            model0.weight1.grad[0] = float('inf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406220e4-f6d7-4a40-ab63-f81cfc47bc11",
   "metadata": {},
   "source": [
    "In practice, as shown in the figure below, multiplying the loss by a large value allowed training to proceed without divergence. The gray graph represents performance without scaling, while the green graph shows performance with scaling applied. Surprisingly, the performance is almost identical to FP32.\n",
    "\n",
    "![](../images/mixed_precision_2.png)\n",
    "\n",
    "For this reason, mixed precision using both FP16 and FP32 has become almost essential in modern neural network training. Until bfloat16 (Google TPU), which provides FP32-level coverage with FP16-level storage requirements, is supported on a wider variety of GPUs and becomes more widely adopted, FP16 + FP32 mixed precision training will remain a core technique in neural network training.\n",
    "\n",
    "<br>\n",
    "\n",
    "### How Mixed Precision Works\n",
    "\n",
    "The following figure illustrates how mixed precision operates. Let’s take a closer look at the process using code and equations.\n",
    "\n",
    "<br>\n",
    "\n",
    "![](../images/mixed_precision_33.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368bd097-9b20-4daf-a31f-3f26c0db24fb",
   "metadata": {},
   "source": [
    "### 0) Create the model and optimizer\n",
    "\n",
    "We define a neural network with two layers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0e9031-fc53-40cb-971a-c20e8519d7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.w1 = nn.Linear(512, 512, bias=False)\n",
    "        self.w2 = nn.Linear(512, 1, bias=False)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        z1 = self.w1(x)\n",
    "        z2 = self.w2(z1)\n",
    "        return z2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cabc70f-9424-4e3a-a52e-81a0a60545a2",
   "metadata": {},
   "source": [
    "We create the neural network to be trained and the optimizer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd34ad19-2743-467c-a156-53c70bbbebf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import SGD\n",
    "\n",
    "fp32_model= Net().to(\"cuda\")\n",
    "optimizer = SGD(fp32_model.parameters(), lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb87240-c035-42cb-b07e-ae0eb1c2edb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "f\"GPU = {torch.cuda.memory_allocated(0) / (1024 ** 2)} GiB\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6decba-633f-4018-b6e3-0ec258511c85",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 1)  Float2Half\n",
    "\n",
    "This process simply truncates parameters such as `0.524796132` to values like `0.5247`.\n",
    "\n",
    "As you can see, the memory footprint is roughly half the size of an FP32 model. (1.0 GB + 0.5 GB)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f640e9a9-dd8c-44c4-a9a8-c67d31425787",
   "metadata": {},
   "outputs": [],
   "source": [
    "fp16_model = Net().half().to(\"cuda\")\n",
    "fp16_model.load_state_dict(fp32_model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93131aa-84e8-4c56-af18-fa9fcba62be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "f\"GPU = {torch.cuda.memory_allocated(0) / (1024 ** 2)} GiB\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bcfb3d9-100e-480b-99e1-247c8edca0b2",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 2) Forward\n",
    "\n",
    "We perform the forward pass using the model copied to fp16.\n",
    "\n",
    "$z_1 = w_1 \\cdot x \\; $ (FWD: layer1)\n",
    "\n",
    "$z_2 = w_2 \\cdot z_1 \\; $ (FWD: layer2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a8f1aa-51f0-4bf4-aabb-0935c19408b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# example input sizes\n",
    "batch_size, hidden_size = 4, 512\n",
    "\n",
    "# create dummy data (bsz=4, hid=256)\n",
    "x = torch.randn(batch_size,hidden_size, dtype=torch.half, device=\"cuda\") \n",
    "\n",
    "# do forward\n",
    "z2 = fp16_model(x)\n",
    "\n",
    "# check dtypr of output logits\n",
    "f\"logits type = {z2.dtype}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810376cb-5b6d-409b-b971-e3e13c8b5bcf",
   "metadata": {},
   "source": [
    "We compute the loss using the output values calculated in FP16.\n",
    "\n",
    "$L = \\frac{(y - z_2)^2}{2} \\; $ (Loss computation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79784534-0dce-44fa-af06-c7a3a1d73790",
   "metadata": {},
   "outputs": [],
   "source": [
    "# craete dummy data (bsz=4)\n",
    "y = torch.tensor([[1.9], [9.5], [0.9], [1.2]], dtype=torch.half, device=\"cuda\")\n",
    "\n",
    "# compute mean square error loss\n",
    "L = torch.nn.functional.mse_loss(z2, y)\n",
    "\n",
    "# check dtype of loss\n",
    "f\"loss type = {L.dtype}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc9d285-1d26-49dd-b2a8-1808748381ba",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 3) Backward \n",
    "\n",
    "Now we need to update the model parameters using the Gradient Descent rule:\n",
    "\n",
    "$w_n := w_n - lr \\cdot \\frac{dL}{dw_n}$\n",
    "\n",
    "Therefore, we must compute the gradients $\\frac{dL}{dw_1}$ and $\\frac{dL}{dw_2}$.  \n",
    "They are approximately as follows (the desired results can be obtained using the chain rule):\n",
    "\n",
    "$\\frac{dL}{dw_2} = \\frac{dL}{dz_2} \\cdot \\frac{dz_2}{dw_2}$\n",
    "\n",
    "$\\frac{dL}{dw_1} = \\frac{dL}{dz_2} \\cdot \\frac{dz_2}{dz_1} \\cdot \\frac{dz_1}{dw_1}$\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "More concretely, they are:\n",
    "\n",
    "$\\frac{dL}{dz_2} =  y - z_2 \\; $ (BWD-activation: layer2)\n",
    "\n",
    "$\\frac{dz_2}{dw_2} = z_1 \\;$ (BWD-weight: layer2)\n",
    "\n",
    "$\\frac{dz_2}{dz_1} = w_2 \\;$ (BWD-activation: layer1)\n",
    "\n",
    "$\\frac{dz_1}{dw_1} = x \\; $ (BWD-weight: layer1)\n",
    "\n",
    "<br>\n",
    "\n",
    "$\\frac{dL}{dw_2} = (y - z_2) \\cdot z_1$\n",
    "\n",
    "$\\frac{dL}{dw_1} = (y - z_2) \\cdot w_2 \\cdot x$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd73b2f-28a4-4781-9fce-b5a855eb2e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss scaling\n",
    "L *= 1024\n",
    "\n",
    "# do backward\n",
    "L.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2c4080-7e55-4dcf-9905-7925ea4aeb07",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 4) Update Weight\n",
    "\n",
    "Finally, to update the parameters, we call `optimizer.step()`.\n",
    "\n",
    "$w_1 := w_1 - lr \\cdot \\frac{dL}{dw_1} \\; $ (Weight Update)\n",
    "\n",
    "$w_2 := w_2 - lr \\cdot \\frac{dL}{dw_2} \\; $ (Weight Update)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7a99a0-56c8-4f0e-990b-b99da7360983",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'before: {fp32_model.w1.weight}\\n')\n",
    "optimizer.step()\n",
    "print(f'after: {fp32_model.w1.weight}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c471ce-3804-469f-9363-0f92e65f72b6",
   "metadata": {},
   "source": [
    "If you think about it, the FP32 model has never performed forward or backward passes.  \n",
    "Therefore, it does not have gradient tensors. As a result, even if we call `optimizer.step()`, the values do not change.  \n",
    "So, before calling `optimizer.step()`, we must copy the gradients from the FP16 model that went through `backward()`.\n",
    "\n",
    "For reference, in PyTorch, all parameters (`nn.Parameter`) that have `requires_grad=True` set contain an attribute called `grad`.  \n",
    "When `backward()` is called on a tensor produced by the model, PyTorch traverses the computation graph backward, computes derivatives, and stores the results in the `grad` attribute.  \n",
    "\n",
    "Since `grad` has the same size as the corresponding tensor, if a model occupies 10GB of memory, its gradients will also require about 10GB.  \n",
    "This is one of the reasons why training requires much more memory than inference.  \n",
    "\n",
    "Therefore, for tensors that are not used for training, you should always set `requires_grad=False` to prevent unnecessary memory consumption.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a51c8b1-3341-438a-9273-7afc1d962a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy gradient to FP32 model\n",
    "fp32_model.w1.weight.grad = fp16_model.w1.weight.grad.float()\n",
    "fp32_model.w2.weight.grad = fp16_model.w2.weight.grad.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e186c746-4d19-4b8f-99f4-29e84d123693",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'before: {fp32_model.w1.weight}\\n')\n",
    "optimizer.step()\n",
    "print(f'after: {fp32_model.w1.weight}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac647122-8f05-4637-84d5-cfce8da77737",
   "metadata": {},
   "source": [
    "### Performing Mixed Precision Training in PyTorch\n",
    "\n",
    "In PyTorch, you can easily perform mixed precision training as follows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269f8f0f-ef3c-4704-994e-9bc4ae798a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference: https://pytorch.org/blog/accelerating-training-on-nvidia-gpus-with-pytorch-automatic-mixed-precision/\n",
    "import torch\n",
    "# Creates once at the beginning of training\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "for data, label in data_iter:\n",
    "   optimizer.zero_grad()\n",
    "   # Casts operations to mixed precision\n",
    "   with torch.cuda.amp.autocast():\n",
    "      loss = model(data)\n",
    "\n",
    "   # Scales the loss, and calls backward()\n",
    "   # to create scaled gradients\n",
    "   scaler.scale(loss).backward()\n",
    "\n",
    "   # Unscales gradients and calls\n",
    "   # or skips optimizer.step()\n",
    "   scaler.step(optimizer)\n",
    "\n",
    "   # Updates the scale for next iteration\n",
    "   scaler.update()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5830e77-6eea-4bfe-8f28-085ee4adad1d",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Dynamic Loss Scaling\n",
    "\n",
    "Loss Scaling makes mixed precision training very effective. However, it is very difficult to know what scale value is optimal. To address this problem, several open-source projects propose a technique called **Dynamic Loss Scaling**. This is implemented in tools such as NVIDIA’s `amp` and Microsoft’s `deepspeed`.\n",
    "\n",
    "The idea of Dynamic Loss Scaling is very simple. **The goal is to keep the scale value as large as possible without causing gradient overflow.** Increasing gradient values is generally beneficial, but if they become too large, overflow occurs. Therefore, the scale value is increased to the maximum level that does not cause overflow.\n",
    "\n",
    "As a result, training starts with a very large scale value. In the case of `deepspeed`, the default value is set to $2^{32}$. Backpropagation is performed with this scaled loss, and if gradient overflow is detected, the scale value is reduced by half. This process is repeated multiple times to find the largest possible scale value that does not cause overflow—this is exactly what Dynamic Loss Scaling does.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1ccb64-938e-40fc-bf77-91f2b0d4e43b",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### AMP (Apex Mixed Precision)\n",
    "\n",
    "`apex` is a library developed by NVIDIA and is one of the most well-known mixed precision libraries. Nowadays, mixed precision functionality is built directly into `torch`, and tools such as DeepSpeed and PyTorch Lightning are widely used, so `apex` is not used as frequently as before. However, it is still a commonly used library. Its usage is very simple, as shown below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a088de-e4f7-4789-9f8f-4a36b8704c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from apex import amp\n",
    "\n",
    "\n",
    "# Declare model and optimizer as usual, with default (FP32) precision\n",
    "model = torch.nn.Linear(D_in, D_out).cuda()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Allow Amp to perform casts as required by the opt_level\n",
    "model, optimizer = amp.initialize(model, optimizer, opt_level=\"O1\")\n",
    "\n",
    "# loss.backward() becomes:\n",
    "with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "    scaled_loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b200d131-6e11-4c97-94b5-dd6d6da62e53",
   "metadata": {},
   "source": [
    "Looking at the code above, you can see a parameter called `opt_level`.  \n",
    "`apex` provides a feature to configure different mixed precision levels, and understanding these levels can be very useful if you ever need to use `apex` in the future.  \n",
    "(For reference, these are the letter **O** followed by the numbers **0, 1, 2, 3**.)\n",
    "\n",
    "![](../images/apex.png)\n",
    "\n",
    "- `O0`: FP32 training  \n",
    "- `O1`: Operations that are well supported by Tensor Cores run in FP16 / the rest run in FP32  \n",
    "- `O2`: All parameters are set to FP16 except normalization weights  \n",
    "- `O3`: Pure FP16 training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1dd5871-19b7-4e96-ba6b-d52b7963f636",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 2. Zero Redundancy Optimization\n",
    "\n",
    "By using FP16 and FP32 together, training speed improves significantly, but a drawback appears: **memory usage**.  \n",
    "Because the FP32 master weights, FP16 parameters, and gradients are all kept on the GPU at the same time, more memory is required compared to before.\n",
    "\n",
    "![](../images/zero_1.png)\n",
    "\n",
    "And even if the model parameters themselves exist in FP16, optimization is performed in FP32.  \n",
    "Therefore, tensors required by adaptive optimizers such as AdaGrad and Adam—such as **variance** and **momentum**—still need to be stored in FP32.\n",
    "\n",
    "![](../images/adam.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b4a96a-5e33-4000-98bb-7d3c2726992c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Reference: pytorch/torch/optim/adam.py\n",
    "\"\"\"\n",
    "\n",
    "@torch.no_grad()\n",
    "def step(self, closure=None):\n",
    "    \"\"\"Performs a single optimization step.\n",
    "\n",
    "    Args:\n",
    "        closure (callable, optional): A closure that reevaluates the model\n",
    "            and returns the loss.\n",
    "    \"\"\"\n",
    "    loss = None\n",
    "    if closure is not None:\n",
    "        with torch.enable_grad():\n",
    "            loss = closure()\n",
    "\n",
    "    for group in self.param_groups:\n",
    "        params_with_grad = []\n",
    "        grads = []\n",
    "        exp_avgs = []\n",
    "        exp_avg_sqs = []\n",
    "        max_exp_avg_sqs = []\n",
    "        state_steps = []\n",
    "        beta1, beta2 = group['betas']\n",
    "\n",
    "        for p in group['params']:\n",
    "            if p.grad is not None:\n",
    "                params_with_grad.append(p)\n",
    "                if p.grad.is_sparse:\n",
    "                    raise RuntimeError(\n",
    "                        'Adam does not support sparse gradients, please consider SparseAdam instead'\n",
    "                    )\n",
    "                grads.append(p.grad)\n",
    "\n",
    "                state = self.state[p]\n",
    "                # Lazy state initialization\n",
    "                # For every parameter, tensors `exp_avg` and `exp_avg_sq`\n",
    "                # of the same size are maintained.\n",
    "                # Because of this, when using Adam-based optimizers,\n",
    "                # GPU memory equivalent to **two additional model copies**\n",
    "                # is required.\n",
    "                if len(state) == 0:\n",
    "                    state['step'] = 0\n",
    "                    # Exponential moving average of gradient values\n",
    "                    state['exp_avg'] = torch.zeros_like(\n",
    "                        p, memory_format=torch.preserve_format\n",
    "                    )\n",
    "                    # Exponential moving average of squared gradient values\n",
    "                    state['exp_avg_sq'] = torch.zeros_like(\n",
    "                        p, memory_format=torch.preserve_format\n",
    "                    )\n",
    "                    if group['amsgrad']:\n",
    "                        # Maintains the maximum of all exponential moving\n",
    "                        # averages of squared gradient values\n",
    "                        state['max_exp_avg_sq'] = torch.zeros_like(\n",
    "                            p, memory_format=torch.preserve_format\n",
    "                        )\n",
    "\n",
    "                exp_avgs.append(state['exp_avg'])\n",
    "                exp_avg_sqs.append(state['exp_avg_sq'])\n",
    "\n",
    "                if group['amsgrad']:\n",
    "                    max_exp_avg_sqs.append(state['max_exp_avg_sq'])\n",
    "\n",
    "                # Update the step count for each parameter\n",
    "                state['step'] += 1\n",
    "                # Record the step after update\n",
    "                state_steps.append(state['step'])\n",
    "\n",
    "        F.adam(\n",
    "            params_with_grad,\n",
    "            grads,\n",
    "            exp_avgs,\n",
    "            exp_avg_sqs,\n",
    "            max_exp_avg_sqs,\n",
    "            state_steps,\n",
    "            amsgrad=group['amsgrad'],\n",
    "            beta1=beta1,\n",
    "            beta2=beta2,\n",
    "            lr=group['lr'],\n",
    "            weight_decay=group['weight_decay'],\n",
    "            eps=group['eps'],\n",
    "        )\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39242079-e6ad-4dda-930b-51791299f31b",
   "metadata": {},
   "source": [
    "Up to this point, we have examined the different kinds of tensors that are allocated in memory when training a model, such as **FP16 parameters**, **FP16 gradients**, **FP32 parameters**, **FP32 gradients**, **momentum**, **variance**, and so on.\n",
    "What is surprising is that the *actual model parameters themselves occupy only a small portion* of the total memory. During training, an **enormous number of additional tensors** are allocated in GPU memory beyond the model itself.\n",
    "\n",
    "<br>\n",
    "\n",
    "![](../images/memory.png)\n",
    "\n",
    "<br>\n",
    "\n",
    "In addition, **Data tensors** and **Activation tensors** are also allocated in memory.\n",
    "\n",
    "* **Data tensors** refer to tensors in token form *before* they are fed into the model.\n",
    "* **Activation tensors** refer to tensors such as hidden states that are produced during the Forward & Backward passes.\n",
    "\n",
    "Furthermore, when distributed training is used, extra memory is required for **bucket buffers that temporarily hold tensors during communication**. We already discussed buckets earlier in the Data Parallelism session, for example in the context of *Gradient Bucketing*.\n",
    "\n",
    "Therefore, it is not enough to parallelize only the **model** and the **data**. We also need to carefully manage **optimizer states** (variance, momentum), as well as **Data & Activation memory**, and even **communication-related buffers**.\n",
    "\n",
    "<br>\n",
    "\n",
    "**Zero Redundancy Optimization (ZeRO)** is a collection of **memory optimization techniques** designed to manage all of these components very efficiently. Broadly speaking, ZeRO consists of solutions such as **ZeRO-DP (ZeRO Data Parallelism)** and **ZeRO-R (ZeRO Residual States)**.\n",
    "\n",
    "From here on, let’s go through them step by step.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd0fbee-c37a-4385-8074-a027f4cc3123",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 3. ZeRO Data Parallelism\n",
    "\n",
    "First, if we examine memory usage, the left side of the figure above (FP16, FP32, model & optimizer & gradient) occupies the largest space. Therefore, these need to be partitioned and managed efficiently. ZeRO-DP helps with this by enabling these tensors to be partitioned across devices together with Data Parallelism.\n",
    "\n",
    "![](../images/zero_2.png)\n",
    "\n",
    "ZeRO-DP is provided in four stages, and it can be selectively applied through the `DeepSpeed` library.\n",
    "\n",
    "- **Stage 0**:\n",
    "  - No Partitioning\n",
    "  - ZeRO-DP is not applied.\n",
    "- **Stage 1**:\n",
    "  - Optimizer States Partitioning\n",
    "  - Optimizer states (momentum, variance) tensors are partitioned across multiple GPUs.\n",
    "  - ~4× reduction in memory usage\n",
    "  - Similar amount of communication cost as before\n",
    "- **Stage 2**:\n",
    "  - Stage 1 + Gradient partitioning\n",
    "  - Gradient tensors are partitioned across multiple GPUs.\n",
    "  - ~2× further reduction in memory usage\n",
    "  - Similar amount of communication cost as before\n",
    "- **Stage 3**:\n",
    "  - Parameter partitioning\n",
    "  - Parameter (model) tensors are partitioned across multiple GPUs.\n",
    "  - Memory usage decreases linearly depending on the partitioning level\n",
    "  - ~1.5× more communication cost than before\n",
    "  \n",
    "Because the operation of ZeRO-DP is quite complex, let’s watch a video to understand it.\n",
    "\n",
    "https://www.microsoft.com/en-us/research/uploads/prod/2020/02/Turing-Animation.mp4?_=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9908ce88-092d-4e06-8ebe-8a0b0bef0276",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "HTML(\"\"\"\n",
    "<div align=\"middle\">\n",
    "<video width=\"80%\" controls>\n",
    "      <source src=\"../images/zero_video.mp4\" type=\"video/mp4\">\n",
    "</video></div>\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33d0b82-ad0e-4953-b649-e1800ee5439d",
   "metadata": {},
   "source": [
    "![](../images/zero_3.png)\n",
    "\n",
    "In conclusion, by applying ZeRO-DP, you can train much larger models on smaller GPUs than before. Let’s try it out right away. First, we create a configuration file. I enabled the learning rate scheduler, fp16, and zero optimization (stage 1). In addition to these, the DeepSpeed configuration provides a wide variety of options. You can find more options at https://www.deepspeed.ai/docs/config-json.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ebcd6d-abbb-4785-bcc2-d2b025edad2d",
   "metadata": {},
   "source": [
    "```\n",
    "{\n",
    "  \"train_batch_size\": 16,\n",
    "  \"gradient_accumulation_steps\": 1,\n",
    "  \"scheduler\": {\n",
    "    \"type\": \"WarmupDecayLR\",\n",
    "    \"params\": {\n",
    "      \"total_num_steps\": 300,\n",
    "      \"warmup_min_lr\": 0,\n",
    "      \"warmup_max_lr\": 3e-5,\n",
    "      \"warmup_num_steps\": 30\n",
    "    }\n",
    "  },\n",
    "  \"fp16\": {\n",
    "    \"enabled\": true,\n",
    "    \"initial_scale_power\": 32,\n",
    "    \"loss_scale_window\": 1000,\n",
    "    \"hysteresis\": 2,\n",
    "    \"min_loss_scale\": 1\n",
    "  },\n",
    "  \"zero_optimization\": {\n",
    "    \"stage\": 1\n",
    "  },\n",
    "  \"zero_allow_untested_optimizer\": true,\n",
    "  \"wall_clock_breakdown\": false,\n",
    "  \"steps_per_print\": 9999999999\n",
    "}\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b218c5-2c51-49e3-a483-fe904b6b8276",
   "metadata": {},
   "source": [
    "Then, write the following code. The argument parser must include the options --local_rank and --deepspeed_config, and among them, --local_rank is automatically provided when the script is executed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9243ead3-fe8d-4af8-9566-270ca34b805a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "src/ch7_zero_redundancy_optimization/zero_dp_args.py\n",
    "\"\"\"\n",
    "from argparse import ArgumentParser\n",
    "from datasets import load_dataset\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import deepspeed\n",
    "import torch.distributed as dist\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "parser = ArgumentParser()\n",
    "parser.add_argument(\n",
    "    \"--deepspeed_config\", default=\"../src/zero_dp_config.json\", type=str\n",
    ")\n",
    "parser.add_argument(\"--local_rank\", default=0, type=int)\n",
    "args = parser.parse_args()\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=3e-5, weight_decay=3e-7)\n",
    "\n",
    "engine, optimizer, _, scheduler = deepspeed.initialize(\n",
    "    args=args,\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    ")\n",
    "\n",
    "datasets = load_dataset(\"squad\").data[\"train\"][\"context\"]\n",
    "datasets = [str(sample) for sample in datasets]\n",
    "data_loader = DataLoader(datasets, batch_size=8, num_workers=8)\n",
    "\n",
    "for i, data in enumerate(data_loader):\n",
    "    tokens = tokenizer(\n",
    "        data,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=1024,\n",
    "    )\n",
    "\n",
    "    loss = engine(\n",
    "        input_ids=tokens.input_ids.cuda(),\n",
    "        attention_mask=tokens.attention_mask.cuda(),\n",
    "        labels=tokens.input_ids.cuda(),\n",
    "    ).loss\n",
    "\n",
    "    engine.backward(loss)\n",
    "    engine.step()\n",
    "\n",
    "    if i % 10 == 0 and dist.get_rank() == 0:\n",
    "        print(f\"step:{i}, loss:{loss}\")\n",
    "\n",
    "    if i >= 300:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89bfa431-5100-4dd2-9db3-fcf9857908c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!deepspeed --num_gpus=4 ../src/zero_args.py --deepspeed_config=../src/ch7_zero_redundancy_optimization/zero_dp_config.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0c555a-9853-434d-b851-e58567197be0",
   "metadata": {},
   "source": [
    "Or, you can directly pass the configuration into deepspeed.initialize()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19868aa-cef3-4616-a9b9-8aa73e645bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "src/ch7_zero_redundancy_optimization/zero_dp_args.py\n",
    "\"\"\"\n",
    "from datasets import load_dataset\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import deepspeed\n",
    "import torch.distributed as dist\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "optimizer = Adam(model.parameters(), lr=3e-5, weight_decay=3e-7)\n",
    "\n",
    "engine, optimizer, _, scheduler = deepspeed.initialize(\n",
    "    optimizer=optimizer,\n",
    "    model=model,\n",
    "    config={\n",
    "        \"train_batch_size\": 16,\n",
    "        \"gradient_accumulation_steps\": 1,\n",
    "        \"scheduler\": {\n",
    "            \"type\": \"WarmupDecayLR\",\n",
    "            \"params\": {\n",
    "                \"total_num_steps\": 300,\n",
    "                \"warmup_min_lr\": 0,\n",
    "                \"warmup_max_lr\": 3e-5,\n",
    "                \"warmup_num_steps\": 30,\n",
    "            },\n",
    "        },\n",
    "        \"fp16\": {\n",
    "            \"enabled\": True,\n",
    "            \"initial_scale_power\": 32,\n",
    "            \"loss_scale_window\": 1000,\n",
    "            \"hysteresis\": 2,\n",
    "            \"min_loss_scale\": 1,\n",
    "        },\n",
    "        \"zero_optimization\": {\n",
    "            \"stage\": 1,\n",
    "            \"allgather_partitions\": True,\n",
    "            \"allgather_bucket_size\": 5e8,\n",
    "            \"overlap_comm\": False,\n",
    "            \"reduce_scatter\": True,\n",
    "            \"reduce_bucket_size\": 5e8,\n",
    "            \"contiguous_gradients\": True,\n",
    "        },\n",
    "        \"zero_allow_untested_optimizer\": True,\n",
    "        \"wall_clock_breakdown\": False,\n",
    "        \"steps_per_print\": 9999999999,\n",
    "    },\n",
    ")\n",
    "\n",
    "datasets = load_dataset(\"squad\").data[\"train\"][\"context\"]\n",
    "datasets = [str(sample) for sample in datasets]\n",
    "data_loader = DataLoader(datasets, batch_size=8, num_workers=8)\n",
    "\n",
    "for i, data in enumerate(data_loader):\n",
    "    tokens = tokenizer(\n",
    "        data,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=1024,\n",
    "    )\n",
    "\n",
    "    loss = engine(\n",
    "        input_ids=tokens.input_ids.cuda(),\n",
    "        attention_mask=tokens.attention_mask.cuda(),\n",
    "        labels=tokens.input_ids.cuda(),\n",
    "    ).loss\n",
    "\n",
    "    engine.backward(loss)\n",
    "    engine.step()\n",
    "\n",
    "    if i % 10 == 0 and dist.get_rank() == 0:\n",
    "        print(f\"step:{i}, loss:{loss}\")\n",
    "\n",
    "    if i >= 300:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220a27a5-2240-453a-baf7-21b9dbe02af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!deepspeed --num_gpus=4 ../src/ch7_zero_redundancy_optimization/zero_config.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa92f945-32b7-491a-9c79-fdf8ad291cad",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 4. Activation Checkpointing\n",
    "\n",
    "In addition to FP16 and FP32 model parameters, gradients, and optimizer states, another large memory region is **Activation Memory**.  \n",
    "Activations refer to the input tensors that are multiplied by the model weights.\n",
    "\n",
    "For example, consider the following neural network:\n",
    "\n",
    "$y = w_1 \\cdot (w_2 \\cdot x)$\n",
    "\n",
    "In this case:\n",
    "- The tensor `x`, which is multiplied by `w_2`\n",
    "- The tensor `w_2 · x`, which is multiplied by `w_1`\n",
    "\n",
    "are both stored as **Activation Memory**.\n",
    "\n",
    "These activation tensors must be kept during the forward pass because they are required later to compute gradients during the backward pass.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faec4cc5-e3d7-4cff-b3a5-dd3197e8272f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Reference: https://pytorch.org/tutorials/beginner/examples_autograd/two_layer_net_custom_function.html\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "class ReLU(torch.autograd.Function):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        ctx.save_for_backward(input)\n",
    "        # input 값을 저장하고 있음.\n",
    "        \n",
    "        return input.clamp(min=0)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input, = ctx.saved_tensors\n",
    "        grad_input = grad_output.clone()\n",
    "        grad_input[input < 0] = 0\n",
    "        return grad_input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b906595-5e6f-4959-844f-6bb19d535aef",
   "metadata": {},
   "source": [
    "![](../images/max_pooling.png)\n",
    "\n",
    "In the Pipeline Parallelism session, we mentioned that during the backward pass, activation tensors used in the forward pass must be stored.  \n",
    "As shown above, in order to compute gradients for a Max Pooling layer, the original positions of the pooled values are required. Therefore, the input tensor from the forward pass must be preserved.\n",
    "\n",
    "Likewise, if you look at the implementation of `ReLU`, you can see that the `input` tensor is saved using `ctx.save_for_backward`.\n",
    "\n",
    "![](../images/checkpoint_full_act.gif)\n",
    "\n",
    "**In other words, to perform the backward step, inputs from the forward step must be stored.**  \n",
    "The animation above illustrates this process.\n",
    "\n",
    "However, storing activations everywhere significantly increases memory consumption.\n",
    "\n",
    "![](../images/checkpoint_no_act.gif)\n",
    "\n",
    "If activation tensors are not stored, memory usage can be greatly reduced.  \n",
    "But in that case, as shown above, the forward computation must be executed again during the backward pass to recompute the activations.\n",
    "\n",
    "Activation Checkpointing combines the advantages of both approaches by storing activations only at selected points.\n",
    "\n",
    "![](../images/checkpoint_act.gif)\n",
    "\n",
    "By storing activations only at intermediate checkpoints, we avoid recomputing the forward pass from the very beginning.  \n",
    "Instead, the forward computation resumes from the nearest checkpoint, reducing computation time.  \n",
    "At the same time, since most activations are discarded, memory consumption is significantly reduced.\n",
    "\n",
    "This technique—**saving activations at intermediate points and recomputing forward passes from those checkpoints when needed**—is called **Activation Checkpointing**.\n",
    "\n",
    "PyTorch already provides built-in support for checkpointing.  \n",
    "Let’s try it out using PyTorch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657470d4-35ed-4937-8165-2ca1a1a0231d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "src/ch7_zero_redundancy_optimization/checkpointing.py\n",
    "\"\"\"\n",
    "from torch import nn\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "from transformers import BertTokenizer, BertLayer, BertConfig\n",
    "\n",
    "config = BertConfig.from_pretrained(\"bert-base-cased\")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "tokens = tokenizer(\"Hello I am Kevin\", return_tensors=\"pt\")\n",
    "\n",
    "embedding = nn.Embedding(tokenizer.vocab_size, config.hidden_size)\n",
    "layers = nn.ModuleList([BertLayer(config) for _ in range(6)])\n",
    "\n",
    "hidden_states = embedding(tokens.input_ids)\n",
    "attention_mask = tokens.attention_mask\n",
    "\n",
    "for i, layer_module in enumerate(layers):\n",
    "    layer_outputs = checkpoint(\n",
    "        layer_module,\n",
    "        hidden_states,\n",
    "        attention_mask,\n",
    "    )\n",
    "\n",
    "    hidden_states = layer_outputs[0]\n",
    "\n",
    "print(f\"output: {hidden_states}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1491290a-7b3a-4f37-880e-5de2789dfc29",
   "metadata": {},
   "source": [
    "Usage is very simple. As shown in the example above, you only need to **replace the existing call from `module(a, b, c)` to `checkpoint(module, a, b, c)`**, and that’s it.\n",
    "\n",
    "In addition, most models in Hugging Face `transformers`, which we frequently use, already include built-in support for Activation Checkpointing.  \n",
    "You can **enable or disable it simply by calling**:\n",
    "\n",
    "- `model.gradient_checkpointing_enable()`\n",
    "- `model.gradient_checkpointing_disable()`\n",
    "\n",
    "Isn’t that incredibly easy?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994bb9d3-dce3-409c-8818-f9374e43cb06",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 4. ZeRO-R\n",
    "\n",
    "ZeRO-R is a collection of techniques designed to highly optimize areas such as **Activation Memory** and **Communication Buckets**.\n",
    "\n",
    "![](../images/zero_r_1.png)\n",
    "\n",
    "In the previous chapter, we improved **model state memory** (FP16 & FP32 parameters, gradients, optimizer states) efficiently using **ZeRO-DP**.  \n",
    "ZeRO-R proposes the following three additional solutions:\n",
    "\n",
    "- **Activation Memory Partitioning**\n",
    "- **Constant Size Buffer**\n",
    "- **Memory Defragmentation**\n",
    "\n",
    "Let’s go through each one.\n",
    "\n",
    "<br>\n",
    "\n",
    "### 1) Activation Memory Partitioning\n",
    "\n",
    "<br>\n",
    "\n",
    "![](../images/zero_r_2.png)\n",
    "\n",
    "Although Activation Checkpointing can help with memory efficiency and speed, it can still cause significant memory issues when training very large models.  \n",
    "This is especially true when combined with model parallelism, where many copies of activation tensors are created in different places after the forward pass.\n",
    "\n",
    "**ZeRO-R addresses this by All-gathering activation tensors and then partitioning only the necessary ones across GPUs.**  \n",
    "Additionally, extremely large activations are checkpointed to **CPU RAM**, slightly sacrificing speed but significantly saving GPU memory.\n",
    "\n",
    "<br>\n",
    "\n",
    "### 2) Constant Memory Buffer\n",
    "\n",
    "Constant Memory Buffer refers to a technique that **keeps the size of communication buckets (used in All-reduce, All-gather, etc.) constant**.\n",
    "\n",
    "In general, as the model grows larger, it is beneficial for communication buckets to grow as well.  \n",
    "However, when models become extremely large, bucket sizes can grow too much and occupy a significant portion of GPU memory.\n",
    "\n",
    "To address this, **an upper limit is imposed on bucket sizes so they cannot grow beyond a fixed maximum value**.  \n",
    "Once the bucket size reaches a certain threshold, maintaining it at that size is sufficient to achieve good efficiency without further growth.\n",
    "\n",
    "<br>\n",
    "\n",
    "### 3) Memory Defragmentation (Contiguous Checkpointing)\n",
    "\n",
    "![](../images/zero_r_3.png)\n",
    "\n",
    "During training, tensors are frequently created and destroyed, which causes **GPU memory fragmentation** to occur very often.  \n",
    "In some cases, even when there is enough total GPU memory available, fragmentation prevents large contiguous tensors from being allocated.\n",
    "\n",
    "To solve this, ZeRO-R **pre-allocates contiguous empty memory regions** that can hold activations, gradients, and similar tensors.  \n",
    "When tensors of similar sizes are created, they are **moved into these reserved regions**, minimizing fragmentation as much as possible.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb84e12-e6c3-4cee-a641-62c6a689e7c9",
   "metadata": {},
   "source": [
    "Just like ZeRO-DP, you only need to write a simple configuration.\n",
    "\n",
    "- **Constant Buffer Size**\n",
    "  - The maximum bucket size is determined using `allgather_bucket_size` and `reduce_bucket_size`.\n",
    "- **Activation Memory**\n",
    "  - Activation memory is partitioned across GPUs using `partition_activations`.\n",
    "  - Extremely large activation tensors are offloaded to the CPU using `cpu_checkpointing`.\n",
    "- **Memory Defragmentation**\n",
    "  - Memory fragmentation is mitigated using `contiguous_memory_optimization`.\n",
    "\n",
    "In addition to these techniques, many more optimizations exist.  \n",
    "If you want to learn more about the available options in detail, please refer to the paper and the official documentation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af48f0e5-3722-405c-a1cd-85b2f2477069",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "```\n",
    "{\n",
    "  \"train_batch_size\": 16,\n",
    "  \"gradient_accumulation_steps\": 1,\n",
    "  \"scheduler\": {\n",
    "    \"type\": \"WarmupDecayLR\",\n",
    "    \"params\": {\n",
    "      \"total_num_steps\": 300,\n",
    "      \"warmup_min_lr\": 0,\n",
    "      \"warmup_max_lr\": 3e-5,\n",
    "      \"warmup_num_steps\": 30\n",
    "    }\n",
    "  },\n",
    "  \"fp16\": {\n",
    "    \"enabled\": true,\n",
    "    \"initial_scale_power\": 32,\n",
    "    \"loss_scale_window\": 1000,\n",
    "    \"hysteresis\": 2,\n",
    "    \"min_loss_scale\": 1\n",
    "  },\n",
    "  \"zero_optimization\": {\n",
    "    \"stage\": 1,\n",
    "    \"allgather_bucket_size\": 5e8,\n",
    "    \"reduce_bucket_size\": 5e8\n",
    "  },\n",
    "  \"activation_checkpointing\": {\n",
    "    \"partition_activations\": true,\n",
    "    \"cpu_checkpointing\": true,\n",
    "    \"contiguous_memory_optimization\": true,\n",
    "    \"number_checkpoints\": 4\n",
    "  },\n",
    "  \"zero_allow_untested_optimizer\": true,\n",
    "  \"wall_clock_breakdown\": false,\n",
    "  \"steps_per_print\": 9999999999\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e9e9fe-bd0d-4583-9aab-3c28d6d71d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "!deepspeed --num_gpus=4 ../src/zero_args.py --deepspeed_config=../src/ch7_zero_redundancy_optimization/zero_r_config.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219dcc96-7ff9-4a99-8433-97f39d1a1ad7",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 5. ZeRO Offload\n",
    "\n",
    "In the previous chapter, we saw that **Activation Memory Partitioning** included the ability to offload very large activation tensors to the CPU.  \n",
    "**ZeRO Offload**, which is a successor to ZeRO-R, goes one step further by **offloading parts of the model itself to CPU RAM**, effectively breaking the GPU memory capacity limit.\n",
    "\n",
    "The core idea of ZeRO Offload is as follows.\n",
    "\n",
    "![](../images/zero_off_1.png)\n",
    "\n",
    "#### GPU-side\n",
    "- FP16 parameters and gradients reside on the GPU.\n",
    "- Forward and backward passes are executed on the GPU (because they involve heavy computation).\n",
    "\n",
    "#### CPU-side\n",
    "- FP32 parameters, gradients, and optimizer states reside on the CPU.\n",
    "- Weight updates are performed on the CPU (because they are relatively lightweight operations).\n",
    "- In particular, a highly optimized **CPU Adam optimizer** is implemented to run very efficiently on the CPU.\n",
    "\n",
    "In general, CPU computation speed is tens of times slower than GPU computation speed.  \n",
    "Therefore, large-scale computations must be performed on the GPU.  \n",
    "For this reason, forward and backward passes are executed on the GPU.\n",
    "\n",
    "If we think about it, a large portion of GPU memory is occupied by FP32 parameters, gradients, and optimizer states, yet the operations they perform are limited to the relatively low-cost **weight update** step.\n",
    "\n",
    "![](../images/ddp_analysis_3.png)\n",
    "\n",
    "By offloading all FP32 components to the CPU, the GPU is left with only FP16 tensors that are strictly necessary for GPU computation, resulting in a much more relaxed GPU memory state.\n",
    "\n",
    "<br>\n",
    "\n",
    "### DPU: Delayed Parameter Update\n",
    "\n",
    "![](../images/zero_off_2.png)\n",
    "\n",
    "If data transfer to the CPU starts only after all forward and backward computations on the GPU are finished, the GPU must remain idle during communication.  \n",
    "ZeRO Offload introduces a technique called **Delayed Parameter Update (DPU)**, which—similar to Gradient Bucketing in DDP—**overlaps communication and computation to reduce overall execution time**.\n",
    "\n",
    "![](../images/zero_off_3.png)\n",
    "\n",
    "Experimental results show that applying DPU does not negatively impact model quality and can slightly improve training speed.\n",
    "\n",
    "<br>\n",
    "\n",
    "### ZeRO Offload + ZeRO-DP\n",
    "\n",
    "![](../images/zero_off_4.png)\n",
    "\n",
    "ZeRO Offload can be combined with **ZeRO-DP**.  \n",
    "If optimizer states and gradients are offloaded to the CPU while ZeRO-DP is enabled, the system takes the form shown above.\n",
    "\n",
    "Note that the combination of ZeRO-DP and Offload is supported starting from **ZeRO stage 2**, and to offload model parameters as well, the ZeRO stage must be set to **stage 3**.\n",
    "\n",
    "- **ZeRO stage 2**: Optimizer States Offload\n",
    "- **ZeRO stage 3**: Optimizer States + Parameter Offload\n",
    "\n",
    "<br>\n",
    "\n",
    "### CPU Adam\n",
    "\n",
    "The standard Adam optimizer is highly optimized for GPUs, so running it on the CPU is generally slower.  \n",
    "To address this, ZeRO Offload provides a **CPU Adam optimizer** that applies various optimization techniques to run very efficiently on the CPU.\n",
    "\n",
    "The implementation of CPU Adam lies closer to computer architecture and operating systems than to traditional machine learning or distributed systems, so it is not covered in detail here.  \n",
    "For more information, please refer to the paper.  \n",
    "(Honestly, I also skipped a deep dive into this part—if anyone has studied it in depth, please let me know by opening an issue.)\n",
    "\n",
    "![](../images/cpu_adam.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153ee9b2-d19d-4e5b-815c-b2a0017fe030",
   "metadata": {},
   "source": [
    "Let’s try **ZeRO Offload** in practice.  \n",
    "As before, we first modify the configuration.\n",
    "\n",
    "To offload both the **optimizer** and **parameters**, the ZeRO stage is set to **stage 3**, and the options `offload_param` and `offload_optimizer` are added.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc90bda-d7f6-40dd-b1bf-ef372a57470d",
   "metadata": {},
   "source": [
    "```\n",
    "{\n",
    "  \"train_batch_size\": 16,\n",
    "  \"gradient_accumulation_steps\": 1,\n",
    "  \"scheduler\": {\n",
    "    \"type\": \"WarmupDecayLR\",\n",
    "    \"params\": {\n",
    "      \"total_num_steps\": 300,\n",
    "      \"warmup_min_lr\": 0,\n",
    "      \"warmup_max_lr\": 3e-5,\n",
    "      \"warmup_num_steps\": 30\n",
    "    }\n",
    "  },\n",
    "  \"fp16\": {\n",
    "    \"enabled\": true,\n",
    "    \"initial_scale_power\": 32,\n",
    "    \"loss_scale_window\": 1000,\n",
    "    \"hysteresis\": 2,\n",
    "    \"min_loss_scale\": 1\n",
    "  },\n",
    "  \"zero_optimization\": {\n",
    "    \"stage\": 3,\n",
    "    \"allgather_bucket_size\": 5e8,\n",
    "    \"reduce_bucket_size\": 5e8,\n",
    "    \"offload_param\": {\n",
    "      \"device\": \"cpu\",\n",
    "      \"pin_memory\": true\n",
    "    },\n",
    "    \"offload_optimizer\": {\n",
    "      \"device\": \"cpu\",\n",
    "      \"pin_memory\": true\n",
    "    }\n",
    "  },\n",
    "  \"activation_checkpointing\": {\n",
    "    \"partition_activations\": true,\n",
    "    \"cpu_checkpointing\": true,\n",
    "    \"contiguous_memory_optimization\": true,\n",
    "    \"number_checkpoints\": 4\n",
    "  },\n",
    "  \"zero_allow_untested_optimizer\": true,\n",
    "  \"wall_clock_breakdown\": false,\n",
    "  \"steps_per_print\": 9999999999\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb39136-a33f-4424-954d-ebb2345db579",
   "metadata": {},
   "outputs": [],
   "source": [
    "!deepspeed --num_gpus=4 ../src/zero_args.py --deepspeed_config=../src/zero_off_config.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24204dea-c86d-4450-a764-52f082dc18b3",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 6. ZeRO Infinity\n",
    "\n",
    "ZeRO Infinity adopts a method of storing parameters in **NVMe (SSD) memory**.  \n",
    "Because NVMe has a much larger memory capacity than even CPU RAM, this approach is considered to push memory limits even further.  \n",
    "The ZeRO Infinity algorithm is also very complex, so let’s watch a video to understand it:\n",
    "\n",
    "https://www.microsoft.com/en-us/research/uploads/prod/2021/04/1400x788_deepspeed_nologo-1.mp4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e713e0c-5118-4e38-b586-d1e6c2d0381a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "HTML(\"\"\"\n",
    "<div align=\"middle\">\n",
    "<video width=\"80%\" controls>\n",
    "      <source src=\"../images/zero_infinity.mp4\" type=\"video/mp4\">\n",
    "</video></div>\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d6cbfd-5eea-402c-a3e5-d1d033990b77",
   "metadata": {},
   "source": [
    "### Core Idea of ZeRO Infinity\n",
    "\n",
    "ZeRO Infinity is an extension of **ZeRO Offload**.  \n",
    "Previously, ZeRO Offload operated CPU RAM and GPU VRAM as follows:\n",
    "\n",
    "- **GPU**: FP16 parameters and gradients reside on the GPU, performing **forward and backward passes**.\n",
    "- **CPU**: FP32 parameters, gradients, and optimizer states reside on the CPU, performing **weight updates**.\n",
    "\n",
    "ZeRO Infinity introduces **NVMe storage**, resulting in the use of three types of devices. The usage is as follows:\n",
    "\n",
    "- **NVMe**: By default, all parameters reside on NVMe when they are not in use.\n",
    "- **GPU**: FP16 parameters and gradients are loaded from NVMe to the GPU **only when forward and backward passes need to be performed**.\n",
    "- **CPU**: FP32 parameters, gradients, and optimizer states are loaded from NVMe to the CPU **only when weight updates need to be performed**.\n",
    "\n",
    "In short, **all tensors are stored on NVMe by default and are moved to CPU or GPU only when needed for computation**.\n",
    "\n",
    "<br>\n",
    "    \n",
    "### Comparison with Offload & ZeRO-DP\n",
    "\n",
    "![](../images/zero_infinity.png)\n",
    "\n",
    "Since ZeRO Infinity stores almost all tensors on NVMe, both CPU and GPU remain nearly empty.  \n",
    "As shown above, this enables training models that were completely impossible to train using previous techniques.\n",
    "\n",
    "According to experimental results, ZeRO Infinity was also **faster than ZeRO Offload**.\n",
    "\n",
    "ZeRO Infinity requires devices equipped with NVMe storage, so we will not run a hands-on experiment in this tutorial.  \n",
    "If you have an NVMe-enabled system, you can enable ZeRO Infinity by setting the device of `offload_param` and `offload_optimizer` to `nvme` and configuring the `nvme_path` appropriately, as shown below:\n",
    "\n",
    "```json\n",
    "\"offload_param\": {\n",
    "    \"device\": \"nvme\",\n",
    "    \"nvme_path\": \"/local_nvme\",\n",
    "    \"pin_memory\": true\n",
    "},\n",
    "\"offload_optimizer\": {\n",
    "    \"device\": \"nvme\",\n",
    "    \"nvme_path\": \"/local_nvme\",\n",
    "    \"pin_memory\": true\n",
    "}\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
