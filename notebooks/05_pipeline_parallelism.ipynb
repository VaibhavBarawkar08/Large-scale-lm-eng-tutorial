{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1084bd8c-731f-4abc-8279-e988279cd59f",
   "metadata": {},
   "source": [
    "# Pipeline Parallelism\n",
    "In this session, we will explore pipeline parallelism.\n",
    "\n",
    "## 1. Inter-layer Model Parallelism\n",
    "Pipeline parallelism is an improvement over inter-layer model parallelism. Inter-layer model parallelism is a method in which specific layers are assigned to specific GPUs, as shown below. In the figure, layers 1, 2, and 3 are assigned to GPU 1, while layers 4 and 5 are assigned to GPU 2. Each partitioned segment is called a **stage**. In this example, the model is split into two stages.\n",
    "\n",
    "![](../images/inter_layer.png)\n",
    "\n",
    "However, due to the nature of neural networks‚Äîwhere the output of a previous layer becomes the input to the next layer‚Äîcomputation on one GPU must finish before another GPU can begin its computation. In other words, as illustrated below, inter-layer model parallelism has a critical limitation: **only one GPU can be utilized at a time**.\n",
    "\n",
    "![](../images/inter_layer_2.png)\n",
    "![](../images/inter_layer_3.gif)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1f41cd-6f27-49be-9721-fc84d118610c",
   "metadata": {},
   "source": [
    "## 2. GPipe\n",
    "GPipe is a pipeline parallelism technique developed by Google. It was introduced to reduce GPU idle time in inter-layer model parallelism. GPipe works by further splitting a mini-batch into micro-batches and pipelining the training process.\n",
    "\n",
    "![](../images/gpipe_1.png)\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "![](../images/pipeline_parallelism2.png)\n",
    "\n",
    "<br>\n",
    "\n",
    "### Micro-batch\n",
    "- A **mini-batch** is a subset of samples obtained by dividing the entire dataset into *n* parts.\n",
    "- A **micro-batch** is a further subdivision of a mini-batch into *m* smaller subsets.\n",
    "\n",
    "![](../images/gpipe_2.png)\n",
    "\n",
    "<br`m\n",
    "\n",
    "### Pipelining\n",
    "GPipe splits a mini-batch into micro-batches and pipelines the computation. The red regions (where GPUs are idle) are referred to as **bubble time**. As the micro-batch size increases, the bubble time decreases.\n",
    "\n",
    "![](../images/gpipe_3.gif)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2824f08-8730-40d3-9f6f-037a9c90d4f3",
   "metadata": {},
   "source": [
    "### GPipe with PyTorch\n",
    "You can easily use GPipe by leveraging `torchgpipe`, which was released by KakaoBrain. However, only models wrapped with `nn.Sequential` are supported, and the input and output types of all modules are restricted to `torch.Tensor` or `Tuple[torch.Tensor]`. As a result, implementing models with this approach can be quite challenging.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e659778-eb34-4652-ba92-85709cc8a5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "src/ch5_pipeline_parallelism/gpipe.py\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from datasets import load_dataset\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from torchgpipe import GPipe\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from transformers.models.gpt2.modeling_gpt2 import GPT2Block as GPT2BlockBase\n",
    "\n",
    "\n",
    "class GPT2Preprocessing(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.embed_dim = config.hidden_size\n",
    "        self.wte = nn.Embedding(config.vocab_size, self.embed_dim)\n",
    "        self.wpe = nn.Embedding(config.max_position_embeddings, self.embed_dim)\n",
    "        self.drop = nn.Dropout(config.embd_pdrop)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        input_shape = input_ids.size()\n",
    "        input_ids = input_ids.view(-1, input_shape[-1])\n",
    "        position_ids = torch.arange(\n",
    "            0, input_shape[-1], dtype=torch.long, device=input_ids.device\n",
    "        )\n",
    "        position_ids = position_ids.unsqueeze(0).view(-1, input_shape[-1])\n",
    "        inputs_embeds = self.wte(input_ids)\n",
    "        position_embeds = self.wpe(position_ids)\n",
    "        hidden_states = inputs_embeds + position_embeds\n",
    "        hidden_states = self.drop(hidden_states)\n",
    "        return hidden_states\n",
    "\n",
    "\n",
    "class GPT2Block(GPT2BlockBase):\n",
    "    def forward(self, hidden_states):\n",
    "        hidden_states = super(GPT2Block, self).forward(\n",
    "            hidden_states=hidden_states,\n",
    "        )\n",
    "        return hidden_states[0]\n",
    "\n",
    "\n",
    "class GPT2Postprocessing(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_f = nn.LayerNorm(\n",
    "            config.hidden_size,\n",
    "            eps=config.layer_norm_epsilon,\n",
    "        )\n",
    "        self.lm_head = nn.Linear(\n",
    "            config.hidden_size,\n",
    "            config.vocab_size,\n",
    "            bias=False,\n",
    "        )\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        hidden_states = self.ln_f(hidden_states)\n",
    "        lm_logits = self.lm_head(hidden_states)\n",
    "        return lm_logits\n",
    "\n",
    "\n",
    "def create_model_from_pretrained(model_name):\n",
    "    pretrained = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "    preprocess = GPT2Preprocessing(pretrained.config)\n",
    "    preprocess.wte.weight = pretrained.transformer.wte.weight\n",
    "    preprocess.wpe.weight = pretrained.transformer.wpe.weight\n",
    "\n",
    "    blocks = pretrained.transformer.h\n",
    "    for i, block in enumerate(blocks):\n",
    "        block.__class__ = GPT2Block\n",
    "\n",
    "    postprocess = GPT2Postprocessing(pretrained.config)\n",
    "    postprocess.ln_f.weight = pretrained.transformer.ln_f.weight\n",
    "    postprocess.ln_f.bias = pretrained.transformer.ln_f.bias\n",
    "    postprocess.lm_head.weight.data = pretrained.lm_head.weight.data.clone()\n",
    "\n",
    "    return nn.Sequential(preprocess, *blocks, postprocess)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    world_size = 4\n",
    "\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    model = create_model_from_pretrained(model_name=\"gpt2\")\n",
    "    model = GPipe(\n",
    "        model,\n",
    "        balance=[4, 3, 3, 4],\n",
    "        devices=[0, 1, 2, 3],\n",
    "        chunks=world_size,\n",
    "    )\n",
    "\n",
    "    datasets = load_dataset(\"squad\").data[\"train\"][\"context\"]\n",
    "    datasets = [str(sample) for sample in datasets]\n",
    "    data_loader = DataLoader(datasets, batch_size=8, num_workers=8)\n",
    "\n",
    "    optimizer = Adam(model.parameters(), lr=3e-5)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    for i, data in enumerate(data_loader):\n",
    "        optimizer.zero_grad()\n",
    "        tokens = tokenizer(data, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "        input_ids = tokens.input_ids.to(0)\n",
    "        labels = tokens.input_ids.to(world_size - 1)\n",
    "\n",
    "        lm_logits = model(input_ids)\n",
    "        shift_logits = lm_logits[..., :-1, :].contiguous()\n",
    "        shift_labels = labels[..., 1:].contiguous()\n",
    "        loss = nn.CrossEntropyLoss()(\n",
    "            shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1)\n",
    "        )\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            print(f\"step: {i}, loss: {loss}\")\n",
    "        if i == 300:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b89d7a-90ca-4561-95e2-713fec60b73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m torch.distributed.launch --nproc_per_node=4 ../src/gpipe.py\n",
    "!python ../src/ch5_pipeline_parallelism/gpipe.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e9dc9f-d84c-4ff0-ad7e-64f814817e04",
   "metadata": {},
   "source": [
    "## 3. 1F1B Pipelining (PipeDream)\n",
    "\n",
    "`PipeDream`, released by Microsoft, performs pipelining in a slightly different way compared to `GPipe`. This approach is commonly referred to as **1F1B**. Unlike GPipe, which performs the backward pass only after all forward passes are completed, PipeDream alternates between **forward** and **backward** passes.\n",
    "\n",
    "<img src=\"../images/1f1b.png\" width=600>\n",
    "\n",
    "There are two main challenges in 1F1B pipelining:\n",
    "1. Weight version management  \n",
    "2. Work partitioning  \n",
    "\n",
    "<br>\n",
    "\n",
    "### 1) Weight Version Management\n",
    "In the case of GPipe, only a single version of the weights is maintained, but periodic **pipeline flushes** occur. A pipeline flush refers to the process of updating parameters using the computed gradients. During this flush phase, no forward or backward computations are performed, which reduces processing efficiency.\n",
    "\n",
    "<img src=\"../images/pipeline_flush.png\" width=600>\n",
    "\n",
    "PipeDream continuously updates parameters without requiring such flushes. As a result, idle time for both forward and backward passes is eliminated. However, this requires maintaining **multiple versions of the model parameters**. If only the latest version of the parameters were stored, a situation could arise where the next layer is updated while outputs from previous layers are still being propagated.\n",
    "\n",
    "<img src=\"../images/1f1b.gif\" width=800>\n",
    "\n",
    "To prevent this issue, multiple versions of the weights are stored and managed. This, however, consumes a significant amount of memory, leading to a trade-off:\n",
    "\n",
    "- **GPipe**: Memory-efficient, but less efficient in processing  \n",
    "- **PipeDream**: Memory-inefficient, but more efficient in processing  \n",
    "\n",
    "<br>\n",
    "\n",
    "### 2) Work Partitioning\n",
    "The second challenge concerns how to partition the neural network. Simply assigning the same number of layers to each partition is not always the best solution. The most important goal is to **minimize idle time**, which requires the execution time of each partition to be as balanced as possible. In addition, factors such as parameter size and activation memory must also be considered.\n",
    "\n",
    "<img src=\"../images/pipe_dream.png\" width=600>\n",
    "\n",
    "PipeDream determines an optimal partitioning strategy through **profiling and optimization**.\n",
    "\n",
    "<br><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ef5374-9fdc-4f03-8ee3-680a3e13e688",
   "metadata": {},
   "source": [
    "## 4. Variations of 1F1B Pipelining\n",
    "\n",
    "Here, we introduce two pipeline strategies that improve upon PipeDream‚Äôs 1F1B pipelining.\n",
    "\n",
    "<br>\n",
    "\n",
    "### 1) PipeDream 2BW (2-buffered weight update)\n",
    "PipeDream 2BW was introduced to address the memory inefficiency of the original PipeDream. The core idea is to perform **gradient accumulation** during pipelining. By accumulating multiple gradients and applying updates all at once, it mitigates memory inefficiency. Unlike the original approach, 2BW only needs to maintain **two versions of the weights**.\n",
    "\n",
    "![](../images/pipe_dream_2bw.png)\n",
    "\n",
    "<br>\n",
    "\n",
    "### 2) PipeDream Flush\n",
    "PipeDream Flush is a pipelining method that combines 1F1B with **pipeline flush**. Because flushes occur, the idle time is similar to that of GPipe, but the amount of **activation memory that must be maintained during the forward‚Äìbackward process is reduced**. Since PipeDream Flush performs flushes, there is no need to manage multiple versions of parameters. As a result, only a single set of weights needs to be maintained, making it even more memory-efficient than PipeDream 2BW. (Among the techniques introduced so far, this is the **most memory-efficient**.)\n",
    "\n",
    "![](../images/pipe_dream_flush.png)\n",
    "\n",
    "![](../images/pipe_dream_flush_2.png)\n",
    "\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3fec59-b10b-4873-9b1a-979cf7d37222",
   "metadata": {},
   "source": [
    "### Wait a moment‚Ä¶ what is activation memory?\n",
    "Most layers store the outputs produced during the forward pass until the backward pass is executed. Those who have used `torch.autograd.Function` may already be familiar with this behavior. The outputs of the forward layer are stored in the `ctx` variable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd10a61-eba0-4efd-bf7b-0ff0c6c4a307",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Note: https://pytorch.org/tutorials/beginner/examples_autograd/two_layer_net_custom_function.html\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "class ReLU(torch.autograd.Function):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        ctx.save_for_backward(input)\n",
    "        # input Í∞íÏùÑ Ï†ÄÏû•ÌïòÍ≥† ÏûàÏùå.\n",
    "        \n",
    "        return input.clamp(min=0)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input, = ctx.saved_tensors\n",
    "        grad_input = grad_output.clone()\n",
    "        grad_input[input < 0] = 0\n",
    "        return grad_input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a68f0d-f8d8-4f4f-ba82-08335ecaf375",
   "metadata": {},
   "source": [
    "This is because the values used during the forward pass are required to compute gradients during differentiation. Let‚Äôs look at the following example.\n",
    "\n",
    "![](../images/max_pooling.png)\n",
    "\n",
    "The figure above shows a max pooling operation and the corresponding gradient computation. During the backward pass, a (2, 2) tensor such as `[[0.8, 1.2], [0.9, 0.5]]` is given as input. Using this input, we must compute the gradient matrix shown on the right. To do this, the original (4, 4) tensor from the forward pass is required.\n",
    "\n",
    "For this reason, the tensor from the forward pass must be stored in memory. The memory required to store tensors that were used during the forward pass in order to perform the backward pass is called **activation memory**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9f780f-f78a-4788-a9d8-b560e6decbc4",
   "metadata": {},
   "source": [
    "Now that we understand what activation memory is, shall we try practicing PipeDream? **PipeDream Flush is implemented in Microsoft‚Äôs distributed training library, DeepSpeed.**  \n",
    "(Reference: https://github.com/microsoft/DeepSpeed/issues/1110)  \n",
    "So, let‚Äôs use DeepSpeed.\n",
    "\n",
    "### How to Use DeepSpeed Commands\n",
    "Before that, let‚Äôs first look at a very convenient feature provided by `deepspeed`. Previously, we used  \n",
    "`python -m torch.distributed.launch --nproc_per_node=n OOO.py`  \n",
    "for distributed training, but it was long and inconvenient. DeepSpeed provides much simpler commands such as `deepspeed` or `ds`.\n",
    "\n",
    "- `ds --num_gpus=n OOO.py`\n",
    "- `deepspeed --num_gpus=n OOO.py`\n",
    "\n",
    "Running these commands behaves exactly the same as `torch.distributed.launch`. From now on, we will use DeepSpeed commands for all distributed programs. (Honestly, `torch.distributed.launch` is just too long üò≠)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05fa1a3-4127-4ee0-ad82-8d7d90f93c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "src/ch5_pipeline_parallelism/pipe_dream.py\n",
    "\"\"\"\n",
    "import deepspeed\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from datasets import load_dataset\n",
    "from deepspeed import PipelineModule\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from transformers.models.gpt2.modeling_gpt2 import GPT2Block as GPT2BlockBase\n",
    "import torch.distributed as dist\n",
    "\n",
    "\n",
    "class GPT2Preprocessing(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.embed_dim = config.hidden_size\n",
    "        self.wte = nn.Embedding(config.vocab_size, self.embed_dim)\n",
    "        self.wpe = nn.Embedding(config.max_position_embeddings, self.embed_dim)\n",
    "        self.drop = nn.Dropout(config.embd_pdrop)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        input_shape = input_ids.size()\n",
    "        input_ids = input_ids.view(-1, input_shape[-1])\n",
    "        position_ids = torch.arange(\n",
    "            0, input_shape[-1], dtype=torch.long, device=input_ids.device\n",
    "        )\n",
    "        position_ids = position_ids.unsqueeze(0).view(-1, input_shape[-1])\n",
    "        inputs_embeds = self.wte(input_ids)\n",
    "        position_embeds = self.wpe(position_ids)\n",
    "        hidden_states = inputs_embeds + position_embeds\n",
    "        hidden_states = self.drop(hidden_states)\n",
    "        return hidden_states\n",
    "\n",
    "\n",
    "class GPT2Block(GPT2BlockBase):\n",
    "    def forward(self, hidden_states):\n",
    "        hidden_states = super(GPT2Block, self).forward(\n",
    "            hidden_states=hidden_states,\n",
    "        )\n",
    "        return hidden_states[0]\n",
    "\n",
    "\n",
    "class GPT2Postprocessing(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_f = nn.LayerNorm(\n",
    "            config.hidden_size,\n",
    "            eps=config.layer_norm_epsilon,\n",
    "        )\n",
    "        self.lm_head = nn.Linear(\n",
    "            config.hidden_size,\n",
    "            config.vocab_size,\n",
    "            bias=False,\n",
    "        )\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        hidden_states = self.ln_f(hidden_states)\n",
    "        lm_logits = self.lm_head(hidden_states)\n",
    "        return lm_logits\n",
    "\n",
    "\n",
    "def create_model_from_pretrained(model_name):\n",
    "    pretrained = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "    preprocess = GPT2Preprocessing(pretrained.config)\n",
    "    preprocess.wte.weight = pretrained.transformer.wte.weight\n",
    "    preprocess.wpe.weight = pretrained.transformer.wpe.weight\n",
    "\n",
    "    blocks = pretrained.transformer.h\n",
    "    for i, block in enumerate(blocks):\n",
    "        block.__class__ = GPT2Block\n",
    "\n",
    "    postprocess = GPT2Postprocessing(pretrained.config)\n",
    "    postprocess.ln_f.weight = pretrained.transformer.ln_f.weight\n",
    "    postprocess.ln_f.bias = pretrained.transformer.ln_f.bias\n",
    "    postprocess.lm_head.weight.data = pretrained.lm_head.weight.data.clone()\n",
    "\n",
    "    return nn.Sequential(preprocess, *blocks, postprocess)\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    batch_encoding = tokenizer.pad(\n",
    "        {\"input_ids\": batch}, padding=\"max_length\", max_length=1024\n",
    "    )\n",
    "    return batch_encoding.input_ids\n",
    "\n",
    "\n",
    "def batch_fn(data):\n",
    "    input_ids = data\n",
    "    labels = data\n",
    "    return input_ids, labels\n",
    "\n",
    "\n",
    "def loss_fn(logits, labels):\n",
    "    logits = logits[..., :-1, :].contiguous()\n",
    "    labels = labels[..., 1:].contiguous()\n",
    "\n",
    "    return nn.CrossEntropyLoss()(\n",
    "        logits.view(-1, logits.size(-1)),\n",
    "        labels.view(-1),\n",
    "    )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    dist.init_process_group(\"nccl\")\n",
    "    world_size, rank = dist.get_world_size(), dist.get_rank()\n",
    "    batch_size, train_steps = 16, 300\n",
    "    train_samples = batch_size * train_steps\n",
    "\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    model = PipelineModule(\n",
    "        create_model_from_pretrained(model_name=\"gpt2\"),\n",
    "        loss_fn=loss_fn,\n",
    "        num_stages=world_size,\n",
    "        partition_method=\"type:GPT2Block\"\n",
    "        # You can choose which layers to parallelize using partition_method.\n",
    "    )\n",
    "    engine, optimizer, _, _ = deepspeed.initialize(\n",
    "        model=model,\n",
    "        optimizer=Adam(model.parameters(), lr=3e-5),\n",
    "        config={\n",
    "            \"train_batch_size\": batch_size,\n",
    "            \"steps_per_print\": 9999999,\n",
    "            # Turn off logging: https://github.com/microsoft/DeepSpeed/issues/1119\n",
    "        },\n",
    "    )\n",
    "    engine.set_batch_fn(batch_fn)\n",
    "\n",
    "    datasets = load_dataset(\"squad\").data[\"train\"][\"context\"]\n",
    "    datasets = [str(sample) for i, sample in enumerate(datasets) if i < train_samples]\n",
    "    datasets = [\n",
    "        tokenizer(data, return_tensors=\"pt\", max_length=1024).input_ids[0]\n",
    "        for data in tqdm(datasets)\n",
    "    ]\n",
    "    data_loader = iter(\n",
    "        DataLoader(\n",
    "            sorted(datasets, key=len, reverse=True),\n",
    "            # Uniform-length batching\n",
    "            # https://mccormickml.com/2020/07/29/smart-batching-tutorial/\n",
    "            batch_size=batch_size,\n",
    "            num_workers=8,\n",
    "            collate_fn=collate_fn,\n",
    "            shuffle=False,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    for i in range(train_steps):\n",
    "        loss = engine.train_batch(data_loader)\n",
    "\n",
    "        if i % 10 == 0 and rank == 0:\n",
    "            print(f\"step: {i}, loss: {loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba96ecf-5410-4b02-b42c-7aac23687302",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ds --num_gpus=4 ../src/ch5_pipeline_parallelism/pipe_dream.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26345239-1e5e-478e-8c46-46b3da8dc40b",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "## 5. Interleaved Scheduling\n",
    "Previously, each stage (a contiguous set of layers) was computed sequentially to produce outputs. For example, if there are 8 layers and 2 devices, layers 1‚Äì4 are typically assigned to device 1, and layers 5‚Äì8 to device 2. In this setup, device 1 executes layers 1‚Äì4 sequentially and then outputs the result. (Both GPipe and 1F1B operate in this manner.)\n",
    "\n",
    "![](../images/interleaved_1.png)\n",
    "\n",
    "However, **interleaved scheduling overlaps the execution within a single stage to drastically reduce bubble time**. For instance, if device 1 is assigned layers 1‚Äì4, it can execute layers 1‚Äì2 and layers 3‚Äì4 concurrently. This approach reduces bubble time, but it increases communication overhead, so careful tuning is required. (There is a trade-off.)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
