{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2165068b-fe7e-4fcf-86d1-c58c2babae7c",
   "metadata": {},
   "source": [
    "# Multi-dimensional Parallelism\n",
    "\n",
    "In this session, we will go through several concepts and hands-on practices used for **Multi-dimensional Parallelism**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177f8ef6-f73e-4f1a-bef3-1c3399cbe40a",
   "metadata": {},
   "source": [
    "## 1. Multi-dimensional Parallelism\n",
    "\n",
    "Multi-dimensional Parallelism refers to using multiple parallelization techniques together.  \n",
    "For example, if you have 8 GPUs numbered from 0 to 7, you could apply Data Parallelism to 2 GPUs, Pipeline Parallelism to another 2 GPUs, and Tensor Parallelism to another 2 GPUs.\n",
    "\n",
    "In this case, the model is parallelized across multiple dimensions, and depending on how many dimensions are used, it is referred to as **N-dimensional parallelism**.  \n",
    "The example above corresponds to a **3-dimensional parallelism** of (2 × 2 × 2).\n",
    "\n",
    "![](../images/parallelism.png)\n",
    "\n",
    "Applying multiple parallelization techniques simultaneously requires highly advanced engineering.  \n",
    "In this session, we will learn how to manage and use these parallelization techniques together.\n",
    "\n",
    "<br>\n",
    "\n",
    "## 2. MPU (Model Parallel Unit)\n",
    "\n",
    "https://github.com/NVIDIA/Megatron-LM/blob/main/megatron/mpu/initialize.py#L57\n",
    "\n",
    "MPU is a concept proposed in Megatron-LM that provides various modules related to model parallelism.  \n",
    "In particular, **MPU automatically creates and manages process groups, making multi-dimensional parallelism much easier to use**.\n",
    "\n",
    "Let’s look at an example of process groups for multi-dimensional parallelism.\n",
    "\n",
    "Assume we have **16 GPUs**, and we parallelize the model with the following dimensions:\n",
    "\n",
    "- Data Parallelism: 2  \n",
    "- Tensor Parallelism: 2  \n",
    "- Pipeline Parallelism: 4  \n",
    "\n",
    "This configuration results in:\n",
    "- 8 Tensor Parallelism groups\n",
    "- 8 Data Parallelism groups\n",
    "- 4 Pipeline Parallelism groups\n",
    "\n",
    "In other words, the number of process groups is determined by dividing the total number of GPUs by the size of each parallel dimension.\n",
    "\n",
    "- **Tensor Parallelism groups** can be created as follows:\n",
    "  - `[0, 1], [2, 3], [4, 5], [6, 7], [8, 9], [10, 11], [12, 13], [14, 15]`\n",
    "  - That is, GPU 0 communicates with GPU 1 for tensor-parallel operations, GPU 2 with GPU 3, and so on.\n",
    "\n",
    "<br>\n",
    "\n",
    "- **Data Parallelism groups** can be created as follows:\n",
    "  - `[0, 2], [1, 3], [4, 6], [5, 7], [8, 10], [9, 11], [12, 14], [13, 15]`\n",
    "  - That is, GPU 0 communicates with GPU 2 for data-parallel operations, GPU 1 with GPU 3, and so on.\n",
    "\n",
    "<br>\n",
    "\n",
    "- **Pipeline Parallelism groups** can be created as follows:\n",
    "  - `[0, 4, 8, 12], [1, 5, 9, 13], [2, 6, 10, 14], [3, 7, 11, 15]`\n",
    "  - During the forward pass, communication flows in the order `0 → 4 → 8 → 12`, and during the backward pass, communication flows in the reverse order `12 → 8 → 4 → 0`.\n",
    "\n",
    "<br>\n",
    "\n",
    "This may look quite complex, but there is no need to memorize the process creation order.  \n",
    "The MPU object automatically handles all of this for you.  \n",
    "The structure can be visualized as follows.\n",
    "\n",
    "<br>\n",
    "\n",
    "```\n",
    "              +---------+  +---------+  +---------+  +---------+\n",
    "      tensor  |   g00   |  |   g04   |  |   g08   |  |   g12   |\n",
    "data          +---------+  +---------+  +---------+  +---------+ ===> forward\n",
    "      tensor  |   g01   |  |   g05   |  |   g09   |  |   g13   |\n",
    "              +---------+  +---------+  +---------+  +---------+\n",
    "               pipeline     pipeline     pipeline     pipeline\n",
    "\n",
    "              +---------+  +---------+  +---------+  +---------+\n",
    "      tensor  |   g02   |  |   g06   |  |   g10   |  |   g14   |\n",
    "data          +---------+  +---------+  +---------+  +---------+ ===> forward\n",
    "      tensor  |   g03   |  |   g07   |  |   g11   |  |   g15   |\n",
    "              +---------+  +---------+  +---------+  +---------+\n",
    "                pipeline     pipeline     pipeline     pipeline\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "Since the model is split across three dimensions, it can also be represented as a 3D cuboid, as shown below.\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "```\n",
    "                        [g02, g06, g10, g14]\n",
    "                      /  |              /  |\n",
    "                     [g00, g04, g08, g12]  |\n",
    "                     |   |             |   |\n",
    "        3D parallel  |  [g03, g07, g11, g15]\n",
    "                     |  /              |  /\n",
    "                     [g01, g05, g09, g13]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2ceae7-f096-41a9-a783-ac5e5056124c",
   "metadata": {},
   "source": [
    "We will now implement and use **MPU** directly.  \n",
    "This implementation is based on the Megatron-LM code, with modifications made by me.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2841f68d-82fa-4bae-a3c3-a23f4c9fe4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "from torch import Tensor\n",
    "from torch.autograd.function import Function\n",
    "\n",
    "\n",
    "class MPU(object):\n",
    "    \"\"\"\n",
    "    MPU: Model Parallel Unit\n",
    "\n",
    "    Notes:\n",
    "        Let's say we have a total of 16 GPUs denoted g0 ... g15 and we use 2 GPUs to parallelize the model tensor,\n",
    "        and 4 GPUs to parallelize the model pipeline. The present method will create 8 tensor model-parallel group,\n",
    "        4 pipeline model parallel groups and 8 data parallel groups as:\n",
    "\n",
    "        - width: 4 pipeline parallel group\n",
    "            [g0, g4, g8, g12], [g1, g5, g9, g13], [g2, g6, g10, g14], [g3, g7, g11, g15]\n",
    "        - height: 8 tensor parallel group\n",
    "            [g0, g1], [g2, g3], [g4, g5], [g6, g7], [g8, g9], [g10, g11], [g12, g13], [g14, g15]\n",
    "        - depth: 8 data parallel group\n",
    "            [g0, g2], [g1, g3], [g4, g6], [g5, g7], [g8, g10], [g9, g11], [g12, g14], [g13, g15]\n",
    "\n",
    "                        [g02, g06, g10, g14]\n",
    "                      /  |              /  |\n",
    "                     [g00, g04, g08, g12]  |\n",
    "                     |   |             |   |\n",
    "        3D parallel  |  [g03, g07, g11, g15]\n",
    "                     |  /              |  /\n",
    "                     [g01, g05, g09, g13]\n",
    "\n",
    "                      +---------+  +---------+  +---------+  +---------+\n",
    "              tensor  |   g00   |  |   g04   |  |   g08   |  |   g14   |\n",
    "        data          +---------+  +---------+  +---------+  +---------+ ===> forward\n",
    "              tensor  |   g01   |  |   g05   |  |   g09   |  |   g13   |\n",
    "                      +---------+  +---------+  +---------+  +---------+\n",
    "                        pipeline     pipeline     pipeline     pipeline\n",
    "\n",
    "                      +---------+  +---------+  +---------+  +---------+\n",
    "              tensor  |   g02   |  |   g06   |  |   g10   |  |   g12   |\n",
    "        data          +---------+  +---------+  +---------+  +---------+ ===> forward\n",
    "              tensor  |   g03   |  |   g07   |  |   g11   |  |   g15   |\n",
    "                      +---------+  +---------+  +---------+  +---------+\n",
    "                        pipeline     pipeline     pipeline     pipeline\n",
    "\n",
    "    References:\n",
    "        Original MPU implementation of Megatron-LM.\n",
    "        https://github.com/NVIDIA/Megatron-LM/blob/main/megatron/mpu/initialize.py\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    _tensor_model_parallel_group = None\n",
    "    _pipeline_model_parallel_group = None\n",
    "    _data_parallel_group = None\n",
    "\n",
    "    _tensor_model_parallel_world_size = None\n",
    "    _pipeline_model_parallel_world_size = None\n",
    "    _data_parallel_world_size = None\n",
    "\n",
    "    _tensor_model_parallel_rank = None\n",
    "    _pipeline_model_parallel_rank = None\n",
    "    _pipeline_global_ranks = None\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        tensor_model_parallel_size: int,\n",
    "        pipeline_model_parallel_size: int,\n",
    "        backend: str,\n",
    "        master_port: int,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Initialize MPU object. All process groups are initialized in this method.\n",
    "\n",
    "        Args:\n",
    "            tensor_model_parallel_size (int): tensor model parallel world size\n",
    "            pipeline_model_parallel_size (int): pipeline model parallel world size\n",
    "        \"\"\"\n",
    "\n",
    "        if not dist.is_initialized():\n",
    "            self.initialize_distributed(backend, master_port)\n",
    "\n",
    "        current_rank = dist.get_rank()\n",
    "        global_world_size = dist.get_world_size()\n",
    "\n",
    "        assert (\n",
    "            global_world_size >= tensor_model_parallel_size\n",
    "        ), \"param `tensor_model_parallel_size` must be smaller than global world size.\"\n",
    "\n",
    "        assert (\n",
    "            global_world_size >= pipeline_model_parallel_size\n",
    "        ), \"param `pipeline_model_parallel_size` must be smaller than global world size.\"\n",
    "\n",
    "        total_model_parallel_size = (\n",
    "            tensor_model_parallel_size * pipeline_model_parallel_size\n",
    "        )\n",
    "\n",
    "        assert (\n",
    "            global_world_size % total_model_parallel_size == 0\n",
    "        ), \"global world sizes must be divisible by model parallel world sizes (tp * pp)\"\n",
    "\n",
    "        num_tensor_model_parallel_groups = (\n",
    "            global_world_size // tensor_model_parallel_size\n",
    "        )\n",
    "\n",
    "        num_pipeline_model_parallel_groups = (\n",
    "            global_world_size // pipeline_model_parallel_size\n",
    "        )\n",
    "\n",
    "        # 1. initialize data parallel group\n",
    "        self._initialize_data_parallel_group(\n",
    "            current_rank=current_rank,\n",
    "            tensor_model_parallel_size=tensor_model_parallel_size,\n",
    "            pipeline_model_parallel_size=pipeline_model_parallel_size,\n",
    "            num_pipeline_model_parallel_groups=num_pipeline_model_parallel_groups,\n",
    "        )\n",
    "\n",
    "        # 2. initialize tensor model parallel group\n",
    "        self._initialize_tensor_model_parallel_group(\n",
    "            current_rank=current_rank,\n",
    "            tensor_model_parallel_size=tensor_model_parallel_size,\n",
    "            num_tensor_model_parallel_groups=num_tensor_model_parallel_groups,\n",
    "        )\n",
    "\n",
    "        # 3. initialize pipeline model parallel group\n",
    "        self._initialize_pipeline_model_parallel_group(\n",
    "            current_rank=current_rank,\n",
    "            global_world_size=global_world_size,\n",
    "            num_pipeline_model_parallel_groups=num_pipeline_model_parallel_groups,\n",
    "        )\n",
    "\n",
    "        # 4. create distributed functions\n",
    "        functions = self._initialize_functions()\n",
    "        self._broadcast_fn = functions[\"broadcast\"]\n",
    "        self._reduce_fn = functions[\"reduce\"]\n",
    "        self._scatter_fn = functions[\"scatter\"]\n",
    "        self._gather_fn = functions[\"gather\"]\n",
    "\n",
    "    def _initialize_data_parallel_group(\n",
    "        self,\n",
    "        current_rank: int,\n",
    "        tensor_model_parallel_size: int,\n",
    "        pipeline_model_parallel_size: int,\n",
    "        num_pipeline_model_parallel_groups: int,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Initialize data parallel group\n",
    "\n",
    "        Args:\n",
    "            current_rank (int): current rank\n",
    "            tensor_model_parallel_size (int): tensor model parallel world size\n",
    "            pipeline_model_parallel_size (int): pipeline model parallel world size\n",
    "            num_pipeline_model_parallel_groups (int): the number of pipeline model parallel groups\n",
    "        \"\"\"\n",
    "        assert (\n",
    "            self._data_parallel_group is None\n",
    "        ), \"data parallel group is already initialized.\"\n",
    "\n",
    "        for i in range(pipeline_model_parallel_size):\n",
    "            start_rank = i * num_pipeline_model_parallel_groups\n",
    "            end_rank = (i + 1) * num_pipeline_model_parallel_groups\n",
    "\n",
    "            for j in range(tensor_model_parallel_size):\n",
    "                ranks = list(\n",
    "                    range(start_rank + j, end_rank, tensor_model_parallel_size)\n",
    "                )\n",
    "\n",
    "                group = dist.new_group(ranks)\n",
    "                if current_rank in ranks:\n",
    "                    self._data_parallel_group = group\n",
    "\n",
    "    def _initialize_tensor_model_parallel_group(\n",
    "        self,\n",
    "        current_rank: int,\n",
    "        tensor_model_parallel_size: int,\n",
    "        num_tensor_model_parallel_groups: int,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Initialize tensor model parallel group\n",
    "\n",
    "        Args:\n",
    "            current_rank (int): current rank\n",
    "            tensor_model_parallel_size (int): tensor model parallel world size\n",
    "            num_tensor_model_parallel_groups (int): the number of tensor model parallel groups\n",
    "        \"\"\"\n",
    "        assert (\n",
    "            self._tensor_model_parallel_group is None\n",
    "        ), \"tensor model parallel group is already initialized.\"\n",
    "\n",
    "        for i in range(num_tensor_model_parallel_groups):\n",
    "            start_rank = i * tensor_model_parallel_size\n",
    "            end_rank = (i + 1) * tensor_model_parallel_size\n",
    "\n",
    "            ranks = list(range(start_rank, end_rank))\n",
    "            group = dist.new_group(ranks)\n",
    "\n",
    "            if current_rank in ranks:\n",
    "                self._tensor_model_parallel_group = group\n",
    "\n",
    "    def _initialize_pipeline_model_parallel_group(\n",
    "        self,\n",
    "        current_rank: int,\n",
    "        global_world_size: int,\n",
    "        num_pipeline_model_parallel_groups: int,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Initialize pipeline model parallel group\n",
    "\n",
    "        Args:\n",
    "            current_rank (int): current rank\n",
    "            global_world_size (int): global world size\n",
    "            num_pipeline_model_parallel_groups (int): the number of model parallel groups\n",
    "        \"\"\"\n",
    "        assert (\n",
    "            self._pipeline_model_parallel_group is None\n",
    "        ), \"pipeline model parallel group is already initialized.\"\n",
    "\n",
    "        for i in range(num_pipeline_model_parallel_groups):\n",
    "            ranks = list(\n",
    "                range(i, global_world_size, num_pipeline_model_parallel_groups)\n",
    "            )\n",
    "\n",
    "            group = dist.new_group(ranks)\n",
    "\n",
    "            if current_rank in ranks:\n",
    "                self._pipeline_model_parallel_group = group\n",
    "                self._pipeline_global_ranks = ranks\n",
    "\n",
    "    def model_parallel_is_initialized(self) -> bool:\n",
    "        \"\"\"\n",
    "        Check if model and data parallel groups are initialized.\n",
    "\n",
    "        Returns:\n",
    "            bool: whether MPU is initialized\n",
    "        \"\"\"\n",
    "        if (\n",
    "            self._tensor_model_parallel_group is None\n",
    "            or self._pipeline_model_parallel_group is None\n",
    "            or self._data_parallel_group is None\n",
    "        ):\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    def get_model_parallel_group(self):\n",
    "        \"\"\"\n",
    "        Get the tensor model parallel group.\n",
    "\n",
    "        Notes:\n",
    "            This method existed in the old version of Megatron-LM. It is the same as `get_tensor_model_parallel_group()`,\n",
    "            But we must support backward compatibility because this method is invoked by libraries such as DeepSpeed.\n",
    "\n",
    "        Returns:\n",
    "            ProcessGroup: tensor model parallel group\n",
    "        \"\"\"\n",
    "        return self.get_tensor_model_parallel_group()\n",
    "\n",
    "    def get_model_parallel_world_size(self) -> int:\n",
    "        \"\"\"\n",
    "        Get the tensor model parallel world size\n",
    "\n",
    "        Notes:\n",
    "            This method existed in the old version of Megatron-LM. It is the same as `get_tensor_model_parallel_world_size()`,\n",
    "            But we must support backward compatibility because this method is invoked by libraries such as DeepSpeed.\n",
    "\n",
    "        Returns:\n",
    "            int: tensor model parallel world size\n",
    "        \"\"\"\n",
    "        return self.get_tensor_model_parallel_world_size()\n",
    "\n",
    "    def get_model_parallel_rank(self) -> int:\n",
    "        \"\"\"\n",
    "        Get the tensor model parallel rank\n",
    "\n",
    "        Notes:\n",
    "            This method existed in the old version of Megatron-LM. It is the same as `get_tensor_model_parallel_rank()`,\n",
    "            But we must support backward compatibility because this method is invoked by libraries such as DeepSpeed.\n",
    "\n",
    "        Returns:\n",
    "            int: tensor model parallel world size\n",
    "        \"\"\"\n",
    "        return self.get_tensor_model_parallel_rank()\n",
    "\n",
    "    def get_tensor_model_parallel_group(self):\n",
    "        \"\"\"\n",
    "        Get tensor model parallel group\n",
    "\n",
    "        Returns:\n",
    "            ProcessGroup: tensor model parallel group\n",
    "        \"\"\"\n",
    "\n",
    "        assert (\n",
    "            self._tensor_model_parallel_group is not None\n",
    "        ), \"tensor model parallel group is not initialized.\"\n",
    "\n",
    "        return self._tensor_model_parallel_group\n",
    "\n",
    "    def get_pipeline_model_parallel_group(self):\n",
    "        \"\"\"\n",
    "        Get pipeline model parallel group\n",
    "\n",
    "        Returns:\n",
    "            ProcessGroup: pipeline model parallel group\n",
    "        \"\"\"\n",
    "        assert (\n",
    "            self._pipeline_model_parallel_group is not None\n",
    "        ), \"pipeline model parallel group is not initialized.\"\n",
    "\n",
    "        return self._pipeline_model_parallel_group\n",
    "\n",
    "    def get_data_parallel_group(self):\n",
    "        assert (\n",
    "            self._data_parallel_group is not None\n",
    "        ), \"data parallel group is not initialized.\"\n",
    "\n",
    "        return self._data_parallel_group\n",
    "\n",
    "    def get_tensor_model_parallel_world_size(self) -> int:\n",
    "        \"\"\"\n",
    "        Get tensor model parallel world size\n",
    "\n",
    "        Returns:\n",
    "            int: tensor model parallel world size\n",
    "        \"\"\"\n",
    "        if self._tensor_model_parallel_world_size is not None:\n",
    "            return self._tensor_model_parallel_world_size\n",
    "\n",
    "        return dist.get_world_size(self.get_tensor_model_parallel_group())\n",
    "\n",
    "    def set_tensor_model_parallel_world_size(self, world_size: int) -> None:\n",
    "        \"\"\"\n",
    "        Set tensor model parallel world size\n",
    "\n",
    "        Args:\n",
    "            world_size (int): tensor model parallel world size\n",
    "        \"\"\"\n",
    "        self._tensor_model_parallel_world_size = world_size\n",
    "\n",
    "    def get_pipeline_model_parallel_world_size(self) -> int:\n",
    "        \"\"\"\n",
    "        Get pipeline model parallel world size\n",
    "\n",
    "        Returns:\n",
    "            int: pipeline model parallel world size\n",
    "        \"\"\"\n",
    "        if self._pipeline_model_parallel_world_size is not None:\n",
    "            return self._pipeline_model_parallel_world_size\n",
    "\n",
    "        return dist.get_world_size(self.get_pipeline_model_parallel_group())\n",
    "\n",
    "    def set_pipeline_model_parallel_world_size(self, world_size: int) -> None:\n",
    "        \"\"\"\n",
    "        Set pipeline model parallel world size\n",
    "\n",
    "        Args:\n",
    "            world_size (int): pipeline model parallel world size\n",
    "        \"\"\"\n",
    "        self._pipeline_model_parallel_world_size = world_size\n",
    "\n",
    "    def get_tensor_model_parallel_rank(self) -> int:\n",
    "        \"\"\"\n",
    "        Get tensor model parallel rank\n",
    "\n",
    "        Returns:\n",
    "            int: tensor model parallel rank\n",
    "        \"\"\"\n",
    "        if self._tensor_model_parallel_rank is not None:\n",
    "            return self._tensor_model_parallel_rank\n",
    "\n",
    "        return dist.get_rank(self.get_tensor_model_parallel_group())\n",
    "\n",
    "    def set_tensor_model_parallel_rank(self, rank: int) -> None:\n",
    "        \"\"\"\n",
    "        Set tensor model parallel rank\n",
    "\n",
    "        Args:\n",
    "            rank (int): tensor model parallel rank\n",
    "        \"\"\"\n",
    "\n",
    "        self._tensor_model_parallel_rank = rank\n",
    "\n",
    "    def get_pipeline_model_parallel_rank(self) -> int:\n",
    "        \"\"\"\n",
    "        Get pipeline model parallel rank\n",
    "\n",
    "        Returns:\n",
    "            int: pipeline model parallel rank\n",
    "        \"\"\"\n",
    "        if self._pipeline_model_parallel_rank is not None:\n",
    "            return self._pipeline_model_parallel_rank\n",
    "\n",
    "        return dist.get_rank(self.get_pipeline_model_parallel_group())\n",
    "\n",
    "    def set_pipeline_model_parallel_rank(self, rank: int) -> None:\n",
    "        \"\"\"\n",
    "        Set pipeline model parallel rank\n",
    "\n",
    "        Args:\n",
    "            rank (int): pipeline model parallel rank\n",
    "        \"\"\"\n",
    "\n",
    "        self._pipeline_model_parallel_rank = rank\n",
    "\n",
    "    def is_pipeline_fist_stage(self) -> bool:\n",
    "        \"\"\"\n",
    "        Return `True` if in the first pipeline model parallel stage, `False` otherwise\n",
    "\n",
    "        Returns:\n",
    "            bool: whether current pipeline model parallel stage is first\n",
    "        \"\"\"\n",
    "        return self.get_pipeline_model_parallel_rank() == 0\n",
    "\n",
    "    def is_pipeline_last_stage(self) -> bool:\n",
    "        \"\"\"\n",
    "        Return `True` if in the last pipeline model parallel stage, `False` otherwise\n",
    "\n",
    "        Returns:\n",
    "            bool: whether current pipeline model parallel stage is last\n",
    "        \"\"\"\n",
    "        return self.get_pipeline_model_parallel_rank() == (\n",
    "            self.get_pipeline_model_parallel_world_size() - 1\n",
    "        )\n",
    "\n",
    "    def get_tensor_model_parallel_src_rank(self) -> int:\n",
    "        \"\"\"\n",
    "        Calculate the global rank corresponding to the first local rank in the tensor model parallel group.\n",
    "\n",
    "        Returns:\n",
    "            int: tensor model parallel source rank\n",
    "        \"\"\"\n",
    "\n",
    "        global_rank = dist.get_rank()\n",
    "        local_world_size = self.get_tensor_model_parallel_world_size()\n",
    "        return (global_rank // local_world_size) * local_world_size\n",
    "\n",
    "    def get_pipeline_model_parallel_fist_rank(self):\n",
    "        \"\"\"\n",
    "        Get the first pipeline model parallel rank\n",
    "\n",
    "        Returns:\n",
    "            int: the first pipeline model parallel rank\n",
    "        \"\"\"\n",
    "        return self._pipeline_global_ranks[0]\n",
    "\n",
    "    def get_pipeline_model_parallel_last_rank(self):\n",
    "        \"\"\"\n",
    "        Get the last pipeline model parallel rank\n",
    "\n",
    "        Returns:\n",
    "            int: the last pipeline model parallel rank\n",
    "        \"\"\"\n",
    "        return self._pipeline_global_ranks[\n",
    "            self.get_pipeline_model_parallel_world_size() - 1\n",
    "        ]\n",
    "\n",
    "    def get_pipeline_model_parallel_next_rank(self) -> int:\n",
    "        \"\"\"\n",
    "        Get the next pipeline model parallel rank comparison with current stage.\n",
    "\n",
    "        Returns:\n",
    "            int: the next pipeline model parallel rank\n",
    "        \"\"\"\n",
    "        assert (\n",
    "            self._pipeline_global_ranks is not None\n",
    "        ), \"pipeline model parallel group is not initialized.\"\n",
    "\n",
    "        rank_in_pipe = self.get_pipeline_model_parallel_rank()\n",
    "        world_size = self.get_pipeline_model_parallel_world_size()\n",
    "        return self._pipeline_global_ranks[(rank_in_pipe + 1) % world_size]\n",
    "\n",
    "    def get_pipeline_model_parallel_prev_rank(self) -> int:\n",
    "        \"\"\"\n",
    "        Get the previous pipeline model parallel rank comparison with current stage.\n",
    "\n",
    "        Returns:\n",
    "            int: the previous pipeline model parallel rank\n",
    "        \"\"\"\n",
    "        assert (\n",
    "            self._pipeline_global_ranks is not None\n",
    "        ), \"pipeline model parallel group is not initialized.\"\n",
    "\n",
    "        rank_in_pipe = self.get_pipeline_model_parallel_rank()\n",
    "        world_size = self.get_pipeline_model_parallel_world_size()\n",
    "        return self._pipeline_global_ranks[(rank_in_pipe - 1) % world_size]\n",
    "\n",
    "    def get_data_parallel_world_size(self) -> int:\n",
    "        \"\"\"\n",
    "        Get data parallel world size\n",
    "\n",
    "        Returns:\n",
    "            int: data parallel world size\n",
    "        \"\"\"\n",
    "\n",
    "        return dist.get_world_size(self.get_data_parallel_group())\n",
    "\n",
    "    def get_data_parallel_rank(self) -> int:\n",
    "        \"\"\"\n",
    "        Get data parallel rank\n",
    "\n",
    "        Returns:\n",
    "            int: data parallel rank\n",
    "        \"\"\"\n",
    "        return dist.get_rank(self.get_data_parallel_group())\n",
    "\n",
    "    def destroy_model_parallel(self) -> None:\n",
    "        \"\"\"\n",
    "        Destroy all the model parallel groups\n",
    "        \"\"\"\n",
    "\n",
    "        self._tensor_model_parallel_group = None\n",
    "        self._pipeline_model_parallel_group = None\n",
    "        self._data_parallel_group = None\n",
    "\n",
    "    def _broadcast(self, inputs: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Pass the input to the model parallel region.\n",
    "\n",
    "        Args:\n",
    "            inputs (Tensor): input tensor\n",
    "\n",
    "        Returns:\n",
    "            Tensor: broadcast tensor\n",
    "        \"\"\"\n",
    "        return inputs.clone()\n",
    "\n",
    "    def _reduce(self, inputs: Tensor):\n",
    "        \"\"\"\n",
    "        All-reduce the input tensor across tensor model parallel group.\n",
    "\n",
    "        Args:\n",
    "            inputs (Tensor): input tensor\n",
    "\n",
    "        Returns:\n",
    "            Tensor: all-reduced tensor\n",
    "        \"\"\"\n",
    "        if self.get_tensor_model_parallel_world_size() == 1:\n",
    "            return inputs\n",
    "\n",
    "        dist.all_reduce(inputs, group=self.get_tensor_model_parallel_group())\n",
    "        return inputs\n",
    "\n",
    "    def _scatter(self, inputs: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Split the tensor along its last dimension and keep the corresponding slice.\n",
    "\n",
    "        Args:\n",
    "            inputs (Tensor): input tensor\n",
    "\n",
    "        Returns:\n",
    "            Tensor: scattered tensor\n",
    "        \"\"\"\n",
    "        world_size = self.get_tensor_model_parallel_world_size()\n",
    "\n",
    "        if world_size == 1:\n",
    "            return inputs\n",
    "\n",
    "        last_dim = inputs.dim() - 1\n",
    "        last_dim_size = inputs.size()[last_dim] // world_size\n",
    "\n",
    "        inputs_list = torch.split(\n",
    "            tensor=inputs,\n",
    "            split_size_or_sections=last_dim_size,\n",
    "            dim=last_dim,\n",
    "        )\n",
    "\n",
    "        rank = self.get_tensor_model_parallel_rank()\n",
    "        outputs = inputs_list[rank].contiguous()\n",
    "        return outputs\n",
    "\n",
    "    def _gather(self, inputs: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Gather tensors and concatenate along the last dimension\n",
    "\n",
    "        Args:\n",
    "            inputs (Tensor): input tensor\n",
    "\n",
    "        Returns:\n",
    "            Tensor: gathered tensor\n",
    "        \"\"\"\n",
    "        world_size = self.get_tensor_model_parallel_world_size()\n",
    "\n",
    "        if world_size == 1:\n",
    "            return inputs\n",
    "\n",
    "        last_dim = inputs.dim() - 1\n",
    "        rank = self.get_tensor_model_parallel_rank()\n",
    "\n",
    "        tensor_list = [torch.empty_like(inputs) for _ in range(world_size)]\n",
    "        tensor_list[rank] = inputs\n",
    "        torch.distributed.all_gather(\n",
    "            tensor_list, inputs, group=self.get_tensor_model_parallel_group()\n",
    "        )\n",
    "        outputs = torch.cat(tensor_list, dim=last_dim).contiguous()\n",
    "        return outputs\n",
    "\n",
    "    def broadcast(self, inputs: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Pass the input to the model parallel region.\n",
    "\n",
    "        Args:\n",
    "            inputs (Tensor):\n",
    "\n",
    "        Returns:\n",
    "            Tensor: broadcast tensor\n",
    "        \"\"\"\n",
    "\n",
    "        if self._enable_grad(inputs):\n",
    "            outputs = self._broadcast_fn.apply(inputs)\n",
    "        else:\n",
    "            outputs = self._broadcast(inputs)\n",
    "        return outputs\n",
    "\n",
    "    def reduce(self, inputs: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        All-reduce the input tensor across tensor model parallel group.\n",
    "\n",
    "        Args:\n",
    "            inputs (Tensor): input tensor\n",
    "\n",
    "        Returns:\n",
    "            Tensor: all-reduced tensor\n",
    "        \"\"\"\n",
    "\n",
    "        if self._enable_grad(inputs):\n",
    "            outputs = self._reduce_fn.apply(inputs)\n",
    "        else:\n",
    "            outputs = self._reduce(inputs)\n",
    "        return outputs\n",
    "\n",
    "    def scatter(self, inputs: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Split the tensor along its last dimension and keep the corresponding slice.\n",
    "\n",
    "        Args:\n",
    "            inputs (Tensor): input tensor\n",
    "\n",
    "        Returns:\n",
    "            Tensor: scattered tensor\n",
    "        \"\"\"\n",
    "\n",
    "        if self._enable_grad(inputs):\n",
    "            outputs = self._scatter_fn.apply(inputs)\n",
    "        else:\n",
    "            outputs = self._scatter(inputs)\n",
    "        return outputs\n",
    "\n",
    "    def gather(self, inputs: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Gather tensors and concatenate along the last dimension\n",
    "\n",
    "        Args:\n",
    "            inputs (Tensor): input tensor\n",
    "\n",
    "        Returns:\n",
    "            Tensor: gathered tensor\n",
    "        \"\"\"\n",
    "\n",
    "        if self._enable_grad(inputs):\n",
    "            outputs = self._gather_fn.apply(inputs)\n",
    "        else:\n",
    "            outputs = self._gather(inputs)\n",
    "        return outputs\n",
    "\n",
    "    @staticmethod\n",
    "    def _enable_grad(inputs: Tensor) -> bool:\n",
    "        \"\"\"\n",
    "        Check current tensor is enabled to pass gradient.\n",
    "\n",
    "        Args:\n",
    "            inputs (Tensor): input tensor\n",
    "\n",
    "        Returns:\n",
    "            bool: whether gradient can be passed or not\n",
    "        \"\"\"\n",
    "        return torch.is_grad_enabled() and inputs.requires_grad\n",
    "\n",
    "    def _initialize_functions(self):\n",
    "        class Broadcast(Function):\n",
    "            @staticmethod\n",
    "            def forward(ctx, inputs):\n",
    "                return self._broadcast(inputs)\n",
    "\n",
    "            @staticmethod\n",
    "            def backward(ctx, inputs):\n",
    "                return self._reduce(inputs)\n",
    "\n",
    "        class Reduce(Function):\n",
    "            @staticmethod\n",
    "            def forward(ctx, inputs):\n",
    "                return self._reduce(inputs)\n",
    "\n",
    "            @staticmethod\n",
    "            def backward(ctx, inputs):\n",
    "                return self._broadcast(inputs)\n",
    "\n",
    "        class Scatter(Function):\n",
    "            @staticmethod\n",
    "            def forward(ctx, inputs):\n",
    "                return self._scatter(inputs)\n",
    "\n",
    "            @staticmethod\n",
    "            def backward(ctx, inputs):\n",
    "                return self._gather(inputs)\n",
    "\n",
    "        class Gather(Function):\n",
    "            @staticmethod\n",
    "            def forward(ctx, inputs):\n",
    "                return self._gather(inputs)\n",
    "\n",
    "            @staticmethod\n",
    "            def backward(ctx, inputs):\n",
    "                return self._scatter(inputs)\n",
    "\n",
    "        return {\n",
    "            \"broadcast\": Broadcast,\n",
    "            \"reduce\": Reduce,\n",
    "            \"scatter\": Scatter,\n",
    "            \"gather\": Gather,\n",
    "        }\n",
    "\n",
    "    @staticmethod\n",
    "    def initialize_distributed(backend, master_port):\n",
    "        \"\"\"Initialize torch.distributed and mpu.\"\"\"\n",
    "        if not torch.distributed.is_initialized():\n",
    "            rank = int(os.getenv(\"RANK\", 0))\n",
    "            world_size = int(os.getenv(\"WORLD_SIZE\", 1))\n",
    "            os.environ[\"MASTER_PORT\"] = str(master_port)\n",
    "            device_count = torch.cuda.device_count()\n",
    "\n",
    "            if device_count > 0:\n",
    "                device = rank % device_count\n",
    "                torch.cuda.set_device(device)\n",
    "\n",
    "            torch.distributed.init_process_group(\n",
    "                backend=backend,\n",
    "                world_size=world_size,\n",
    "                rank=rank,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713e9594-8068-4b67-a4b3-abad6ab5b5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "src/test_mpu.py\n",
    "\"\"\"\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "from mpu import MPU\n",
    "\n",
    "mpu = MPU(\n",
    "    tensor_model_parallel_size=2,\n",
    "    pipeline_model_parallel_size=2,\n",
    "    backend=\"nccl\",\n",
    "    master_port=5678,\n",
    ")\n",
    "\n",
    "# 1. MPU automatically creates process groups like the following\n",
    "#    and provides methods to access them.\n",
    "print(f\"TP group: {mpu.get_tensor_model_parallel_group()}\")\n",
    "print(f\"TP wsz: {mpu.get_tensor_model_parallel_world_size()}\")\n",
    "print(f\"TP rank: {mpu.get_tensor_model_parallel_rank()}\")\n",
    "dist.barrier()\n",
    "print(\"\\n\")\n",
    "\n",
    "print(f\"PP group: {mpu.get_pipeline_model_parallel_group()}\")\n",
    "print(f\"PP wsz: {mpu.get_pipeline_model_parallel_world_size()}\")\n",
    "print(f\"PP rank: {mpu.get_pipeline_model_parallel_rank()}\")\n",
    "dist.barrier()\n",
    "print(\"\\n\")\n",
    "\n",
    "# 2. The data parallel size is automatically determined\n",
    "#    based on the TP and PP sizes.\n",
    "#    For example, if TP=4 and PP=1 on 16 GPUs,\n",
    "#    then DP size becomes 16 / (4 * 1) = 4.\n",
    "print(f\"DP group: {mpu.get_data_parallel_group()}\")\n",
    "print(f\"DP wsz: {mpu.get_data_parallel_world_size()}\")\n",
    "print(f\"DP rank: {mpu.get_data_parallel_rank()}\")\n",
    "dist.barrier()\n",
    "print(\"\\n\")\n",
    "\n",
    "# 3. MPU supports operations such as reduce, scatter, gather, and broadcast.\n",
    "#    Since these operations are mostly used within the tensor parallel group,\n",
    "#    the tensor parallel group is set as the default.\n",
    "a = torch.tensor([2, 3, 4, 5]).cuda() * dist.get_rank()\n",
    "a = mpu.reduce(a)\n",
    "print(a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98754b37-cbe3-44c4-9091-5a7c0825fddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!deepspeed --num_gpus=4 ../src/ch8_multi_dimensional_parallelism/test_mpu.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bbb68db-e5c9-4eff-bf2f-45349814186d",
   "metadata": {},
   "source": [
    "## 3. Introduction to Large-scale Projects\n",
    "\n",
    "Currently, almost all large-model–related projects are based on **Megatron-LM**. Since we already practiced Megatron-LM in the **Tensor Parallelism** session, we will not run additional hands-on experiments in this section.\n",
    "\n",
    "All three projects introduced below are **fork repositories of Megatron-LM**, so their usage is largely the same, with most differences appearing only in arguments and configurations. Please read through each project description and clone the repository that best fits your needs.\n",
    "\n",
    "<br>\n",
    "\n",
    "### 1) Megatron-DeepSpeed\n",
    "\n",
    "- Maintainer: Shaden, Jeff, … DeepSpeed Team  \n",
    "- [Latest Megatron + DeepSpeed](https://github.com/microsoft/Megatron-DeepSpeed): Upstream maintained  \n",
    "- [3D parallelism (Megatron 1.1.5) + ZeRO-1](https://github.com/microsoft/DeepSpeedExamples/tree/c1b206c137bb028fc8124fd7d434c2da10efc033/Megatron-LM-v1.1.5-3D_parallelism/megatron): Upstream not maintained  \n",
    "- [2D parallelism (Megatron 1.1.5) + ZeRO-3](https://github.com/microsoft/DeepSpeedExamples/tree/c1b206c137bb028fc8124fd7d434c2da10efc033/Megatron-LM-v1.1.5-ZeRO3): Upstream not maintained  \n",
    "\n",
    "This project adds **DeepSpeed ZeRO** to Megatron-LM. It is believed to be the codebase used for **Megatron-Turing 530B**, reportedly co-developed by NVIDIA and Microsoft.\n",
    "\n",
    "DeepSpeed **ZeRO is also treated as a dimension of multi-dimensional parallelism**, and **ZeRO-DP can be applied instead of standard DP**. However, **ZeRO stage 2 and 3 are not compatible with pipeline parallelism**. Therefore, if you want to use **ZeRO-2 or ZeRO-3**, pipeline parallelism must be disabled.\n",
    "\n",
    "As a result, you must choose one of the following strategies:\n",
    "\n",
    "- **(ZeRO-2 or ZeRO-3 + Tensor Parallelism)**  \n",
    "- **(ZeRO-1 + Tensor Parallelism + Pipeline Parallelism)**  \n",
    "\n",
    "If the model size is extremely large, ZeRO-3 may be a good choice. However, so far, **3D Parallelism + ZeRO-1** is known to provide better efficiency in many cases. The reason is that **tensor parallel communication is fast within a node but becomes significantly slower across nodes**.\n",
    "\n",
    "![](../images/megatron_3d.png)\n",
    "\n",
    "<br>\n",
    "\n",
    "In most cases, as shown in the figure above, **tensor parallelism is used within a node**, **pipeline parallelism is used across nodes**, and **ZeRO-1 is additionally applied**.\n",
    "\n",
    "However, as mentioned in the ZeRO paper, **ZeRO allows much larger models to be trained with the same resources** compared to other methods. Therefore, you should carefully choose your parallelization strategy based on your available resources, target model size, and server configuration.\n",
    "\n",
    "<br>\n",
    "\n",
    "### 2) GPT-NeoX\n",
    "\n",
    "![](../images/eleuther_ai.png)\n",
    "\n",
    "- Maintainer: Stella, … EleutherAI Team  \n",
    "- https://github.com/EleutherAI/gpt-neox: Upstream not maintained  \n",
    "- https://github.com/EleutherAI/deeperspeed: Upstream not maintained  \n",
    "\n",
    "GPT-NeoX is a GPU-based codebase developed by the **EleutherAI** team, well known for GPT-Neo. GPT-NeoX builds on top of  \n",
    "[3D parallelism (Megatron 1.1.5) + ZeRO-1](https://github.com/microsoft/DeepSpeedExamples/tree/c1b206c137bb028fc8124fd7d434c2da10efc033/Megatron-LM-v1.1.5-3D_parallelism/megatron)  \n",
    "and adds various modeling components.\n",
    "\n",
    "Examples include:\n",
    "\n",
    "- ScaleNorm  \n",
    "- RMSNorm  \n",
    "- Rotary Embedding  \n",
    "- Alibi Embedding  \n",
    "- Shampoo Optimizer  \n",
    "- SM3 Optimizer  \n",
    "\n",
    "It also supports **DeepSpeed Sparse Attention**. GPT-NeoX focuses on **English-language models** and **implements only GPT-style architectures**; other architectures such as BERT or T5 have been removed.\n",
    "\n",
    "One important point is that **upstream synchronization with Megatron-LM is not maintained**, and the two codebases have diverged significantly. Since Megatron-LM continues to evolve, **upstream maintenance is an important consideration**.\n",
    "\n",
    "Additionally, GPT-NeoX uses **DeeperSpeed**, a fork of DeepSpeed, which is also **not upstream maintained**.\n",
    "\n",
    "<br>\n",
    "\n",
    "### 3) Big-Science\n",
    "\n",
    "![](../images/big_science.png)\n",
    "\n",
    "- Maintainer: Stas, Wang, … Hugging Face Team  \n",
    "- https://github.com/bigscience-workshop/Megatron-DeepSpeed: Upstream maintained  \n",
    "\n",
    "The **BigScience Workshop** is a one-year initiative led by **Hugging Face**, using computing resources provided by the French government, with the goal of developing **large multilingual models**.\n",
    "\n",
    "This project is a fork of  \n",
    "[Latest Megatron + DeepSpeed](https://github.com/microsoft/Megatron-DeepSpeed).  \n",
    "Although it started most recently, it is being developed at the fastest pace due to the large number of contributors.\n",
    "\n",
    "BigScience continuously adds **recently proven high-performance components**, such as:\n",
    "\n",
    "- ScaleNorm  \n",
    "- RMSNorm  \n",
    "- Rotary Embedding  \n",
    "- Alibi Embedding  \n",
    "\n",
    "Unlike some other projects, BigScience supports **GPT, BERT, T5, Prefix-LM**, and more.  \n",
    "Another key characteristic is that **BigScience maintains upstream compatibility with Megatron-LM**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec6dee6-d871-4da0-bdb9-86a074808fb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af644c93-64a2-4827-839b-b6bbc2259b5f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
